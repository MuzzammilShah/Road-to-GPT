{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Documenting My Journey in Artificial Intelligence.</p> Start Learning"},{"location":"ZeroToHero/","title":"Zero to Hero Series","text":""},{"location":"ZeroToHero/#introduction","title":"Introduction","text":"<p>This repository serves as a structured documentation hub for my journey in understanding and implementing artificial intelligence and machine learning concepts, with a primary focus on building a GPT model. The content is heavily based on Andrej Karpathy's Neural Networks: Zero to Hero series and aims to provide well-organized insights, implementations, and notes for anyone following a similar path.</p> <p>This site is designed for:</p> <ul> <li> <p>Learners and developers interested in AI and deep learning.</p> </li> <li> <p>Anyone following Karpathy's Zero to Hero series.</p> </li> <li> <p>The open-source community looking for structured references and implementations.</p> </li> </ul> <p> </p> <p> </p>"},{"location":"ZeroToHero/#documentation-structure","title":"Documentation structure","text":"<p>The learning resources are divided into three primary sections: Language Models, Transformer Models and General MISC.</p> <p>I. Language Modeling Framework</p> Topic Implementation of Lecture Link Documentation Status Backpropagation Micrograd Read notes  Done Language Model-1 Makemore (Part 1) Read notes  Done Language Model-2 Makemore (Part 2) Read notes  Done Language Model-3 Makemore (Part 3) Read notes  Done Language Model-4 Makemore (Part 4) Read notes  Done Language Model-5 Makemore (Part 5) Read notes  Done <p>II. Transformer based Modeling Framework - GPT</p> Topic Implementation of Lecture Link Documentation Status Transformer Model-1 (GPT) Build GPT Read notes  Done Transformer Model-2 (GPT) Reproducing GPT-2 Read notes  - Tokenizers Build GPT Tokenizer Read notes  - <p>III. General MISC</p> Topic Lecture Link Documentation Status Intro to LLMs Read notes  Done Deep Dive to LLMs Read notes  Done Using LLMs Read notes  - <p> </p>"},{"location":"ZeroToHero/#reflections-and-usage-guidelines","title":"Reflections and Usage Guidelines","text":"<p>Note from the author</p> <ul> <li> <p>This repository is a personal knowledge base for revision, experimentation, and sharing insights.</p> </li> <li> <p>If you find it useful, or spot any errors or improvements, feel free to reach out.</p> </li> <li> <p>You are welcome to reference this material for your own learning journey!</p> </li> </ul> <p>Ethical Usage</p> <p>This content is based on open-source educational materials (Thanks to Andrej Karpathy). While you are encouraged to learn from and contribute to this resource, please do not copy or repurpose it for commercial use. Respect the effort behind this work, and let's continue fostering an open and ethical AI learning community!</p> <p> </p>"},{"location":"ZeroToHero/GPT-1/","title":"Transformer Model-1","text":""},{"location":"ZeroToHero/GPT-1/#transformer-model-gpt-1","title":"TRANSFORMER MODEL - GPT 1","text":"<p>Timeline: 11th - 21st February, 2025</p>"},{"location":"ZeroToHero/GPT-1/#introduction","title":"Introduction","text":"<p>This project is an implementation of a GPT-style language model following Andrej Karpathy\u2019s (iconic, if i may add) \"Let\u2019s Build GPT from Scratch\" video. It walks through the key components of modern transformers, from a simple bigram model to a fully functional self-attention mechanism and multi-headed transformer blocks.  </p>"},{"location":"ZeroToHero/GPT-1/#overview","title":"Overview","text":"<ul> <li>Baseline Model: Bigram language modeling, loss calculation, and text generation.</li> <li>Self-Attention: Understanding matrix multiplications, softmax, and positional encodings. </li> <li>Transformer Architecture: Multi-head self-attention, feedforward layers, residual connections, and layer normalization.</li> <li>Scaling Up: Dropout regularization, encoder vs. decoder architectures (only decoder block has been implemented, no encoder).</li> </ul>"},{"location":"ZeroToHero/GPT-1/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li> <p><code>gpt-dev.ipynb</code> is the only and the main notebook where the implementation has been done from top to bottom.</p> </li> <li> <p>The notebook has three main sections: </p> <ol> <li>Baseline language modeling and Code setup</li> <li>Building the \"self-attention\"</li> <li>Building the Transformer</li> </ol> <p>Each of these sections have their own respective sub sections breakdown in order.</p> </li> <li> <p><code>bigram.md</code> provides a detailed breakdown of the initial python script which was made after section 1. </p> </li> <li><code>gpt.py</code> is the name of the final python script actually containing the model implementation, you may find that in the implementation project repo on my github.</li> </ul> <p>Changes from the original video and Notes</p> <ul> <li>I've used a different and a bigger dataset for this, namely the 'Harry Potter Novels' collection. I found the raw dataset on kaggle (as 7 individual datasets) after which i had them merged and cleaned up seperately, so that the outputs can be a lot more cleaner. You may find the notebooks which I had implemented for that under the <code>additional-files</code> directory, so feel free to check that out.</li> <li>This model is trained on 6 million characters (so ~6 million tokens)</li> <li>The final output can be found in the file <code>generated.txt</code>.</li> <li>I ran this model on a NVIDIA GeForce GTX 1650 of my personal laptop with a decent amount of GPU memory (CUDA Version 12.6) and it took approximately 90 minutes to train and generate the final document.</li> <li>I've also added breakdowns of the codes based on andrej's explainations and how much I understood so feel free to read them as well.</li> </ul> <p> </p> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/GPT-1/bigram/","title":"Bigram Script Breakdown","text":"<p>Following is a breakdown of the bigram.py script added in the implementation repository.</p>"},{"location":"ZeroToHero/GPT-1/bigram/#1-initialisation","title":"1. Initialisation","text":"<p><pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(3007)\n\nwith open('cleaned_dataset.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n</code></pre> Breaking it down:</p> <p>The above codes have the same explaination as in the notebook combined together. Preparation of the dataset, splitting them into train and val. Finally loading batches of data.</p> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#2-estimate_loss-function","title":"2. <code>estimate_loss()</code> Function","text":"<p><pre><code>@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n</code></pre> Breaking it down:</p> <ul> <li><code>@torch.no_grad()</code>: This decorator disables gradient tracking, which saves memory and speeds up evaluation (since gradients (grads) are not needed during evaluation).</li> <li><code>model.eval()</code>: Puts the model in evaluation mode (disables dropout, batch norm, etc.).</li> <li><code>for split in ['train', 'val']</code>: We compute loss separately for training and validation datasets.</li> <li><code>losses = torch.zeros(eval_iters)</code>: We will store <code>eval_iters</code> number of loss values in this tensor.</li> <li>Looping over <code>eval_iters</code> times:<ul> <li><code>X, Y = get_batch(split)</code>: Get a batch of training or validation data.</li> <li><code>logits, loss = model(X, Y)</code>: Compute predictions and loss.</li> <li><code>losses[k] = loss.item()</code>: Store the loss in the tensor.</li> </ul> </li> <li><code>out[split] = losses.mean()</code>: Compute the average loss across <code>eval_iters</code> runs for more stable estimates. This is to reduce the noice (for example, the visual graph would look like its waving up and down, this stabalises it).</li> <li><code>model.train()</code>: Switch back to training mode.</li> <li><code>return out</code>: Returns a dictionary with average training and validation loss.</li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#3-bigramlanguagemodel-class","title":"3. <code>BigramLanguageModel</code> Class","text":"<p>This is the core model for the character-level bigram model. <pre><code>class BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n</code></pre> Breaking it down:</p> <ul> <li><code>nn.Module</code>: Parent class for all PyTorch models.</li> <li><code>nn.Embedding(vocab_size, vocab_size)</code>:<ul> <li>A lookup table that maps each character (index) to a learnable vector of size <code>vocab_size</code>.</li> <li>The model simply learns a direct mapping from each character to the probabilities of the next character.</li> </ul> </li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#4-forward-method","title":"4. <code>forward()</code> Method","text":"<p><pre><code>def forward(self, idx, targets=None):\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n</code></pre> Breaking it down:</p> <ul> <li> <p>Inputs:</p> <ul> <li><code>idx</code>: A tensor of shape <code>(B, T)</code>, where <code>B</code> is batch size and <code>T</code> is sequence length.</li> <li><code>targets</code>: Ground truth next characters (if provided).</li> </ul> </li> <li> <p>Steps:</p> <ul> <li>Look up embeddings:<ul> <li><code>logits = self.token_embedding_table(idx)</code> \u2192 Shape <code>(B, T, C)</code>, where <code>C = vocab_size</code> (one row for each token).</li> </ul> </li> <li>Compute loss (if targets exist):<ul> <li>Reshape <code>logits</code> and <code>targets</code> to be <code>(B*T, C)</code> and <code>(B*T)</code>, respectively.</li> <li>Compute <code>F.cross_entropy(logits, targets)</code>, which measures how well the predicted character distribution matches the true character. It is basically the calculation of the 'negative log likehood' which we found out was the best way to determine the loss in a language model.</li> </ul> </li> <li>Return:<ul> <li><code>logits</code>: The raw scores for the next token.</li> <li><code>loss</code>: The loss value (if <code>targets</code> were provided).</li> </ul> </li> </ul> </li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#5-generate-method","title":"5. <code>generate()</code> Method","text":"<p><pre><code>def generate(self, idx, max_new_tokens):\n    for _ in range(max_new_tokens):\n        logits, loss = self(idx)\n        logits = logits[:, -1, :] # Focus on last time step\n        probs = F.softmax(logits, dim=-1) # Convert to probabilities\n        idx_next = torch.multinomial(probs, num_samples=1) # Sample next token\n        idx = torch.cat((idx, idx_next), dim=1) # Append to sequence\n    return idx\n</code></pre> Breaking it Down:</p> <ul> <li> <p>Inputs:</p> <ul> <li><code>idx</code>: A tensor of shape <code>(B, T)</code>, representing the input sequence.</li> <li><code>max_new_tokens</code>: The number of tokens to generate.</li> </ul> </li> <li> <p>Steps:</p> <ul> <li>Loop for <code>max_new_tokens</code> iterations:</li> <li>Compute <code>logits, loss = self(idx)</code>, which gets predictions for the next character.</li> <li>Extract only the last token's logits: <code>logits = logits[:, -1, :]</code>.</li> <li>Apply <code>softmax</code> to get probabilities.</li> <li>Use <code>torch.multinomial(probs, num_samples=1)</code> to sample a token based on probabilities.</li> <li>Append <code>idx_next</code> to the sequence.</li> <li>Return the final sequence.</li> </ul> </li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#6-training-loop","title":"6. Training Loop","text":"<p><pre><code>model = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n</code></pre> Breaking it Down:</p> <ul> <li>Model initialization: <code>model = BigramLanguageModel(vocab_size)</code>, then moved to <code>device</code>.</li> <li>Optimizer: <code>AdamW</code> optimizer is used to update model parameters.</li> <li>Training Loop:<ul> <li>Evaluate loss every <code>eval_interval</code> iterations<ul> <li>Calls <code>estimate_loss()</code>.</li> </ul> </li> <li>Get a batch of training data:<ul> <li><code>xb, yb = get_batch('train')</code>.</li> </ul> </li> <li>Compute loss:<ul> <li><code>logits, loss = model(xb, yb)</code>.</li> </ul> </li> <li>Backpropagation:<ul> <li><code>optimizer.zero_grad(set_to_none=True)</code>: Clears gradients.</li> <li><code>loss.backward()</code>: Computes gradients.</li> <li><code>optimizer.step()</code>: Updates model weights.</li> </ul> </li> </ul> </li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#7-generating-text","title":"7. Generating text","text":"<p><pre><code># generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n</code></pre> Breaking it Down:</p> <ul> <li><code>context = torch.zeros((1, 1), dtype=torch.long, device=device)</code>:     A single-token input (typically the start-of-sequence).</li> <li><code>m.generate(context, max_new_tokens=500)</code>:     Generates 500 tokens from the model.</li> <li><code>decode(...)</code>:     Converts the generated token sequence back into text.</li> </ul> <p> </p>"},{"location":"ZeroToHero/GPT-1/bigram/#final-thoughts","title":"Final Thoughts","text":"<ul> <li>This is a simple character-level bigram model, meaning each character prediction is based only on the previous character.</li> <li>The <code>nn.Embedding</code> layer learns to associate each character with a probability distribution over possible next characters.</li> <li>The model is trained using cross-entropy loss.</li> <li>The <code>generate()</code> function samples new characters based on learned probabilities.</li> </ul>"},{"location":"ZeroToHero/GPT-1/gpt-dev/","title":"Notebook Implemented","text":"<p>Download the dataset from the implementation repository.</p> In\u00a0[2]: Copied! <pre>#input the dataset and read it in\nwith open('cleaned_dataset.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n</pre> #input the dataset and read it in with open('cleaned_dataset.txt', 'r', encoding='utf-8') as f:     text = f.read() In\u00a0[2]: Copied! <pre>print(\"Length of dataset (in characters): \", len(text))\n</pre> print(\"Length of dataset (in characters): \", len(text)) <pre>Length of dataset (in characters):  6199345\n</pre> In\u00a0[3]: Copied! <pre>#The first 1000 characters\nprint(text[:1000])\n</pre> #The first 1000 characters print(text[:1000]) <pre>M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youd expect to be involved in anything strange or mysterious, because they just didnt hold with such nonsense.\n\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didnt think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursleys sister, but they hadnt met for several years; \n</pre> In\u00a0[3]: Copied! <pre>#Listing all the possible unique characters that occur in our dataset\ncharacters = sorted(list(set(text)))\nvocab_size = len(characters)\nprint(''.join(characters))\nprint(vocab_size)\n</pre> #Listing all the possible unique characters that occur in our dataset characters = sorted(list(set(text))) vocab_size = len(characters) print(''.join(characters)) print(vocab_size) <pre>\n !\"&amp;'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}\n86\n</pre> <p>Now, we need some strategy to tokenize the input text. When we say tokenize we mean convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements.</p> <p> </p> <p>Here in our case we are going to be building a character level language model - So will be translating individual characters into integers.</p> <p> </p> <p>We will be implementing encoders and decoders, but rather a simple one (as that should be enough for our usecase).</p> <p>But there are may others (Encoding texts into integers and also decoding them) which use different schema and different vocabularies:</p> <ul> <li><p>Google uses sentencepiece: This encoder implements sub-word units. What that means is that it neither considers the entire word nor a single character. And that is what is usually adopted in practice.</p> </li> <li><p>OpenAI uses tiktoken: This uses BPE i.e. Bi Pair Encoding tokenizer and this what GPT uses. Here the vocabulary size is very large, almost upto 50,000 tokens.</p> </li> </ul> <p>So here we have tradeoffs:</p> <ul> <li>You can have very long sequence integers with a small vocabulary.</li> <li>You can have very large vocabulary with a small sequence of integers.</li> </ul> <p>Now, we will be sticking to a character level tokenizer only and we are using a simple encoder and decoder. And our vocabulary size is pretty small i.e. <code>86</code> characters (so our tradeoff will be that we will have a large sequence of integers when it is encoded)</p> In\u00a0[4]: Copied! <pre># Creating mapping from characters to integers\n\nstoi = { ch:i for i,ch in enumerate(characters) }\nitos = { i:ch for i,ch in enumerate(characters) }\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\n#Example to see how the encoding and decoding is happening\n# print(encode(\"harry potter\"))\n# print(decode(encode(\"harry potter\")))\n\n# Output:\n# [64, 57, 74, 74, 81, 1, 72, 71, 76, 76, 61, 74]\n# harry potter\n</pre> # Creating mapping from characters to integers  stoi = { ch:i for i,ch in enumerate(characters) } itos = { i:ch for i,ch in enumerate(characters) }  encode = lambda s: [stoi[c] for c in s] decode = lambda l: ''.join([itos[i] for i in l])  #Example to see how the encoding and decoding is happening # print(encode(\"harry potter\")) # print(decode(encode(\"harry potter\")))  # Output: # [64, 57, 74, 74, 81, 1, 72, 71, 76, 76, 61, 74] # harry potter In\u00a0[5]: Copied! <pre># Now we will be encoding our entire dataset\n\nimport torch #I used `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`my CUDA version is 12.6\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape , data.size)\nprint(data[:1000])\n</pre> # Now we will be encoding our entire dataset  import torch #I used `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`my CUDA version is 12.6 data = torch.tensor(encode(text), dtype=torch.long) print(data.shape , data.size) print(data[:1000]) <pre>torch.Size([6199345]) &lt;built-in method size of Tensor object at 0x000002D0CD42BC40&gt;\ntensor([38,  1, 74, 11,  1, 57, 70, 60,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75,\n        68, 61, 81,  9,  1, 71, 62,  1, 70, 77, 69, 58, 61, 74,  1, 62, 71, 77,\n        74,  9,  1, 41, 74, 65, 78, 61, 76,  1, 29, 74, 65, 78, 61,  9,  1, 79,\n        61, 74, 61,  1, 72, 74, 71, 77, 60,  1, 76, 71,  1, 75, 57, 81,  1, 76,\n        64, 57, 76,  1, 76, 64, 61, 81,  1, 79, 61, 74, 61,  1, 72, 61, 74, 62,\n        61, 59, 76, 68, 81,  1, 70, 71, 74, 69, 57, 68,  9,  1, 76, 64, 57, 70,\n        67,  1, 81, 71, 77,  1, 78, 61, 74, 81,  1, 69, 77, 59, 64, 11,  1, 45,\n        64, 61, 81,  1, 79, 61, 74, 61,  1, 76, 64, 61,  1, 68, 57, 75, 76,  1,\n        72, 61, 71, 72, 68, 61,  1, 81, 71, 77, 60,  1, 61, 80, 72, 61, 59, 76,\n         1, 76, 71,  1, 58, 61,  1, 65, 70, 78, 71, 68, 78, 61, 60,  1, 65, 70,\n         1, 57, 70, 81, 76, 64, 65, 70, 63,  1, 75, 76, 74, 57, 70, 63, 61,  1,\n        71, 74,  1, 69, 81, 75, 76, 61, 74, 65, 71, 77, 75,  9,  1, 58, 61, 59,\n        57, 77, 75, 61,  1, 76, 64, 61, 81,  1, 66, 77, 75, 76,  1, 60, 65, 60,\n        70, 76,  1, 64, 71, 68, 60,  1, 79, 65, 76, 64,  1, 75, 77, 59, 64,  1,\n        70, 71, 70, 75, 61, 70, 75, 61, 11,  0,  0, 38, 74, 11,  1, 29, 77, 74,\n        75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 61,  1, 60, 65, 74, 61, 59,\n        76, 71, 74,  1, 71, 62,  1, 57,  1, 62, 65, 74, 69,  1, 59, 57, 68, 68,\n        61, 60,  1, 32, 74, 77, 70, 70, 65, 70, 63, 75,  9,  1, 79, 64, 65, 59,\n        64,  1, 69, 57, 60, 61,  1, 60, 74, 65, 68, 68, 75, 11,  1, 33, 61,  1,\n        79, 57, 75,  1, 57,  1, 58, 65, 63,  9,  1, 58, 61, 61, 62, 81,  1, 69,\n        57, 70,  1, 79, 65, 76, 64,  1, 64, 57, 74, 60, 68, 81,  1, 57, 70, 81,\n         1, 70, 61, 59, 67,  9,  1, 57, 68, 76, 64, 71, 77, 63, 64,  1, 64, 61,\n         1, 60, 65, 60,  1, 64, 57, 78, 61,  1, 57,  1, 78, 61, 74, 81,  1, 68,\n        57, 74, 63, 61,  1, 69, 77, 75, 76, 57, 59, 64, 61, 11,  1, 38, 74, 75,\n        11,  1, 29, 77, 74, 75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 65, 70,\n         1, 57, 70, 60,  1, 58, 68, 71, 70, 60, 61,  1, 57, 70, 60,  1, 64, 57,\n        60,  1, 70, 61, 57, 74, 68, 81,  1, 76, 79, 65, 59, 61,  1, 76, 64, 61,\n         1, 77, 75, 77, 57, 68,  1, 57, 69, 71, 77, 70, 76,  1, 71, 62,  1, 70,\n        61, 59, 67,  9,  1, 79, 64, 65, 59, 64,  1, 59, 57, 69, 61,  1, 65, 70,\n         1, 78, 61, 74, 81,  1, 77, 75, 61, 62, 77, 68,  1, 57, 75,  1, 75, 64,\n        61,  1, 75, 72, 61, 70, 76,  1, 75, 71,  1, 69, 77, 59, 64,  1, 71, 62,\n         1, 64, 61, 74,  1, 76, 65, 69, 61,  1, 59, 74, 57, 70, 65, 70, 63,  1,\n        71, 78, 61, 74,  1, 63, 57, 74, 60, 61, 70,  1, 62, 61, 70, 59, 61, 75,\n         9,  1, 75, 72, 81, 65, 70, 63,  1, 71, 70,  1, 76, 64, 61,  1, 70, 61,\n        65, 63, 64, 58, 71, 74, 75, 11,  1, 45, 64, 61,  1, 29, 77, 74, 75, 68,\n        61, 81, 75,  1, 64, 57, 60,  1, 57,  1, 75, 69, 57, 68, 68,  1, 75, 71,\n        70,  1, 59, 57, 68, 68, 61, 60,  1, 29, 77, 60, 68, 61, 81,  1, 57, 70,\n        60,  1, 65, 70,  1, 76, 64, 61, 65, 74,  1, 71, 72, 65, 70, 65, 71, 70,\n         1, 76, 64, 61, 74, 61,  1, 79, 57, 75,  1, 70, 71,  1, 62, 65, 70, 61,\n        74,  1, 58, 71, 81,  1, 57, 70, 81, 79, 64, 61, 74, 61, 11,  0,  0, 45,\n        64, 61,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1, 64, 57, 60,  1, 61, 78,\n        61, 74, 81, 76, 64, 65, 70, 63,  1, 76, 64, 61, 81,  1, 79, 57, 70, 76,\n        61, 60,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 57, 68, 75, 71,  1,\n        64, 57, 60,  1, 57,  1, 75, 61, 59, 74, 61, 76,  9,  1, 57, 70, 60,  1,\n        76, 64, 61, 65, 74,  1, 63, 74, 61, 57, 76, 61, 75, 76,  1, 62, 61, 57,\n        74,  1, 79, 57, 75,  1, 76, 64, 57, 76,  1, 75, 71, 69, 61, 58, 71, 60,\n        81,  1, 79, 71, 77, 68, 60,  1, 60, 65, 75, 59, 71, 78, 61, 74,  1, 65,\n        76, 11,  1, 45, 64, 61, 81,  1, 60, 65, 60, 70, 76,  1, 76, 64, 65, 70,\n        67,  1, 76, 64, 61, 81,  1, 59, 71, 77, 68, 60,  1, 58, 61, 57, 74,  1,\n        65, 76,  1, 65, 62,  1, 57, 70, 81, 71, 70, 61,  1, 62, 71, 77, 70, 60,\n         1, 71, 77, 76,  1, 57, 58, 71, 77, 76,  1, 76, 64, 61,  1, 41, 71, 76,\n        76, 61, 74, 75, 11,  1, 38, 74, 75, 11,  1, 41, 71, 76, 76, 61, 74,  1,\n        79, 57, 75,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1,\n        75, 65, 75, 76, 61, 74,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 64,\n        57, 60, 70, 76,  1, 69, 61, 76,  1, 62, 71, 74,  1, 75, 61, 78, 61, 74,\n        57, 68,  1, 81, 61, 57, 74, 75, 24,  1])\n</pre> <p>Now we get to the interesting part (atleast for me lol), we will be splitting the train and validation set. In our case we will be taking 90% for training and remaining for validation. The reason is we dont want our model to completely memorise the dataset and instead generate 'Harry Potter' like texts, hence we are witholding some information and will be using it to check for overfitting at the end.</p> In\u00a0[6]: Copied! <pre>n = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n</pre> n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] <p>Okay so now, we never feed our entire data into the model, as that would be computationally expensive and prohibitive. So we divide them into blocks and then group all those blocks into batches and then train them. Each batch is independently trainied and are not communicating with each other.</p> In\u00a0[7]: Copied! <pre>torch.manual_seed(3007) # My dataset is different from what sensei is using, so i am using my own random number here :)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data #if the function call is for train then it considers train data else the val data\n    ix = torch.randint(len(data) - block_size, (batch_size,)) #this one takes the random chunk of values\n    x = torch.stack([data[i:i+block_size] for i in ix]) #x is the first array which will take the values\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #y is the second array which will consider the respective target values (\"the next character that needs to be predicted\")\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n</pre> torch.manual_seed(3007) # My dataset is different from what sensei is using, so i am using my own random number here :) batch_size = 4 # how many independent sequences will we process in parallel? block_size = 8 # what is the maximum context length for predictions?  def get_batch(split):     # generate a small batch of data of inputs x and targets y     data = train_data if split == 'train' else val_data #if the function call is for train then it considers train data else the val data     ix = torch.randint(len(data) - block_size, (batch_size,)) #this one takes the random chunk of values     x = torch.stack([data[i:i+block_size] for i in ix]) #x is the first array which will take the values     y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #y is the second array which will consider the respective target values (\"the next character that needs to be predicted\")     return x, y  xb, yb = get_batch('train') print('inputs:') print(xb.shape) print(xb) print('targets:') print(yb.shape) print(yb)  print('----')  for b in range(batch_size): # batch dimension     for t in range(block_size): # time dimension         context = xb[b, :t+1]         target = yb[b,t]         print(f\"when input is {context.tolist()} the target: {target}\") <pre>inputs:\ntorch.Size([4, 8])\ntensor([[79, 57, 68, 67, 65, 70, 63,  1],\n        [64,  1, 57, 70, 63, 74, 81,  1],\n        [ 1, 69, 65, 60, 57, 65, 74,  9],\n        [ 1, 60, 65, 60,  1, 65, 76,  1]])\ntargets:\ntorch.Size([4, 8])\ntensor([[57, 68, 67, 65, 70, 63,  1, 57],\n        [ 1, 57, 70, 63, 74, 81,  1, 57],\n        [69, 65, 60, 57, 65, 74,  9,  1],\n        [60, 65, 60,  1, 65, 76,  1, 65]])\n----\nwhen input is [79] the target: 57\nwhen input is [79, 57] the target: 68\nwhen input is [79, 57, 68] the target: 67\nwhen input is [79, 57, 68, 67] the target: 65\nwhen input is [79, 57, 68, 67, 65] the target: 70\nwhen input is [79, 57, 68, 67, 65, 70] the target: 63\nwhen input is [79, 57, 68, 67, 65, 70, 63] the target: 1\nwhen input is [79, 57, 68, 67, 65, 70, 63, 1] the target: 57\nwhen input is [64] the target: 1\nwhen input is [64, 1] the target: 57\nwhen input is [64, 1, 57] the target: 70\nwhen input is [64, 1, 57, 70] the target: 63\nwhen input is [64, 1, 57, 70, 63] the target: 74\nwhen input is [64, 1, 57, 70, 63, 74] the target: 81\nwhen input is [64, 1, 57, 70, 63, 74, 81] the target: 1\nwhen input is [64, 1, 57, 70, 63, 74, 81, 1] the target: 57\nwhen input is [1] the target: 69\nwhen input is [1, 69] the target: 65\nwhen input is [1, 69, 65] the target: 60\nwhen input is [1, 69, 65, 60] the target: 57\nwhen input is [1, 69, 65, 60, 57] the target: 65\nwhen input is [1, 69, 65, 60, 57, 65] the target: 74\nwhen input is [1, 69, 65, 60, 57, 65, 74] the target: 9\nwhen input is [1, 69, 65, 60, 57, 65, 74, 9] the target: 1\nwhen input is [1] the target: 60\nwhen input is [1, 60] the target: 65\nwhen input is [1, 60, 65] the target: 60\nwhen input is [1, 60, 65, 60] the target: 1\nwhen input is [1, 60, 65, 60, 1] the target: 65\nwhen input is [1, 60, 65, 60, 1, 65] the target: 76\nwhen input is [1, 60, 65, 60, 1, 65, 76] the target: 1\nwhen input is [1, 60, 65, 60, 1, 65, 76, 1] the target: 65\n</pre> <p>The explaination for above is rather simple, in the first array we have the batch of data which we have considered and each row is the block of data. The second array shows us what the target value will be for the corresponding value in the first array.</p> <p>For example, In first array value is 79 -&gt; so in target array its value will be 57 In first array value is 79, 57 -&gt; so in target array its value will be 68 and so on</p> In\u00a0[8]: Copied! <pre>print(xb)\n</pre> print(xb) <pre>tensor([[79, 57, 68, 67, 65, 70, 63,  1],\n        [64,  1, 57, 70, 63, 74, 81,  1],\n        [ 1, 69, 65, 60, 57, 65, 74,  9],\n        [ 1, 60, 65, 60,  1, 65, 76,  1]])\n</pre> <p>Since we now have our first set of data which we need to feed into our transformer, we will now implement the simplest model - therefore, the bigram languuage model in pytorch.</p> <p> </p> <p>Okay so the lecture goes on from the 22nd minute to the 34th, there was a lot of quick breakdown of the code since it was already done in the previous videos (i still couldn't get some of the visuals as to why we did what we did, but lets see how this goes). Also right now, the output maybe a little silly, but apparently we will be needing this generate function in the bigram model class later in the end when we want the model to refer the history of the sentences formed (like if we are at one point in a sentence, then need it to keep track of the previous characters generated up until that point).</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n</pre> import torch import torch.nn as nn from torch.nn import functional as F In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(3007)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n</pre> torch.manual_seed(3007)  class BigramLanguageModel(nn.Module):      def __init__(self, vocab_size):         super().__init__()         # each token directly reads off the logits for the next token from a lookup table         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)      def forward(self, idx, targets=None):          # idx and targets are both (B,T) tensor of integers         logits = self.token_embedding_table(idx) # (B,T,C)          if targets is None:             loss = None         else:             B, T, C = logits.shape             logits = logits.view(B*T, C)             targets = targets.view(B*T)             loss = F.cross_entropy(logits, targets)          return logits, loss      def generate(self, idx, max_new_tokens):         # idx is (B, T) array of indices in the current context         for _ in range(max_new_tokens):             # get the predictions             logits, loss = self(idx)             # focus only on the last time step             logits = logits[:, -1, :] # becomes (B, C)             # apply softmax to get probabilities             probs = F.softmax(logits, dim=-1) # (B, C)             # sample from the distribution             idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)             # append sampled index to the running sequence             idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)         return idx  m = BigramLanguageModel(vocab_size) logits, loss = m(xb, yb) print(logits.shape) print(loss)  print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) <pre>torch.Size([32, 86])\ntensor(4.9531, grad_fn=&lt;NllLossBackward0&gt;)\n\n1fzM0Fy_Bufz 1/cPH9mF_c/CYk]kZ573w8,2 \\Oww)(y&amp;9D9,HzR]\nMOpFbp[&amp;vdr[D9QO4Kl)qKhWCuifZ3YXyi[IK\"\\-8IZdD\n</pre> <p>Okay so now, we are going to train this above model, we will first declare an optimizer called <code>AdamW</code> which is a lot more advance than the one we have been using previously which was the gradient descent optimizer which can also be invoked using <code>SGD</code> but that is the simplest one so we are not going ahead with that.</p> <p> </p> <p>So at the end of this we are expecting a slightly, very slightly improvements of the output from the above cell (obviously can't expect much because this is a bigram model)</p> In\u00a0[11]: Copied! <pre># create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n</pre> # create a PyTorch optimizer optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) In\u00a0[15]: Copied! <pre>#Play around with the range value, increase it and see the loss improves overtime, once you see a small enough value (atleast maybe 2.5), stop and check your output in the next cell!\n\nbatch_size = 32\nfor steps in range(10000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n</pre> #Play around with the range value, increase it and see the loss improves overtime, once you see a small enough value (atleast maybe 2.5), stop and check your output in the next cell!  batch_size = 32 for steps in range(10000): # increase number of steps for good results...      # sample a batch of data     xb, yb = get_batch('train')      # evaluate the loss     logits, loss = m(xb, yb)     optimizer.zero_grad(set_to_none=True)     loss.backward()     optimizer.step()  print(loss.item()) <pre>2.438239336013794\n</pre> In\u00a0[17]: Copied! <pre>print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n</pre> print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) <pre>\n\n\n\n\n\nTh hatI UI, Er oprr wadiomacalse he ffoinele he entich are Nond t eng pasig n the Rokisthe fouthay ry nld Hahabetond IDemeckny yild:Pryo the FIngl, wasioo whed whed id ht crok the r. ofthe stham, whted r monalimind . ll tt menteacile avit be the wand t I Anditan deeveysseishaintained wad hery, g t th cl bres ditiver, herithethened?' nithe NJferrdeng to Du-ce pe t h, awhemin1|jousis. teaby Us. t p e sprusw in ag tt t caunong on, he y It way r herd wa as.\n\n' iqutshotadng  hitrstwe che s As Ha\n</pre> <p>Eh, not bad (jk its gibberish, but hey its forming words in a sentence format yay hahah)</p> <p>Okay so better explaination on whats happening above:</p> <p>What we are doing here is that, the tokens are not really talking to each other.</p> <p>Here, although we are passing the entire previous characters, we are only considering the last charcter in that sequence to predict the next one (so now you see why this is a very simple model. So in the above output sentence <code>Th hatI UI, Er</code>, for predicting <code>r</code> it only considered <code>E</code> from the entire sequence of previous characters). We can see that in the code section <code>logits = logits[:, -1, :]</code> where <code>-1</code> is added.</p> <p>Next what we want to do is that we want the tokens to communicate with each other so that there is more context to what needs to be generated next. Therefore allowing us to move into transformers.</p> <p> </p> <p>Note: Added a bigram.py file which is essentially a python script containing all of the above implemented code. You can run it and see the output produced (There are some modifications to it, where if you have CUDA it utilises it and moves the training inputs, models to it etc. Along with knowing which mode your model is in- whether it is train or val. Apparently this isnt that important for a bigram model, but it is good practise)</p> <p>From minute 42nd to 58th, sensei explains the mathematical trick to self-attention and he uses three versions to explain it. Essentially it is matrix multiplication of the lower triangle of the matrix.</p> <p>To summarise it more clearly:</p> <ul> <li>You can do weighted aggregations of your past elements, by using matrix multiplications of a lower triangular fashion.</li> <li>And the elements in the lower triangular part, are telling you how much each element fuses into the final position. Therefore, we are using this to develop the self attention block.</li> </ul> <p>This is what that matrix would look like:</p> <pre><code>1 0 0 0 0\n1 1 0 0 0\n1 1 1 0 0\n1 1 1 1 0\n1 1 1 1 1\n</code></pre> <p>Self-Attention breakdown implementation</p> In\u00a0[3]: Copied! <pre># version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n</pre> # version 4: self-attention! torch.manual_seed(1337) B,T,C = 4,8,32 # batch, time, channels x = torch.randn(B,T,C)  # let's see a single Head perform self-attention head_size = 16 key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) value = nn.Linear(C, head_size, bias=False) k = key(x)   # (B, T, 16) q = query(x) # (B, T, 16) wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)  tril = torch.tril(torch.ones(T, T)) #wei = torch.zeros((T,T)) wei = wei.masked_fill(tril == 0, float('-inf')) wei = F.softmax(wei, dim=-1)  v = value(x) out = wei @ v #out = wei @ x  out.shape Out[3]: <pre>torch.Size([4, 8, 16])</pre> In\u00a0[4]: Copied! <pre>wei[0]\n</pre> wei[0] Out[4]: <pre>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)</pre> <p>Notes:</p> <ul> <li>Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.</li> <li>There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.</li> <li>Each example across batch dimension is of course processed completely independently and never \"talk\" to each other</li> <li>In an \"encoder\" attention block just delete the single line that does masking with <code>tril</code>, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.</li> <li>\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)</li> <li>\"Scaled\" attention additional divides <code>wei</code> by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</li> </ul> <p>Implementation of the Tranformer model diagram.</p> <p> </p> <p>(\u26a0\ufe0fNote: Do not run the following cells, I have split them up for explaination breakdown purposes. You can find the final script here in gpt.py)</p> <p> </p> <p>Inserting a Single Self Attention block: Here we have implemented a module called 'Head' which is a single head implementation of self attention.</p> In\u00a0[\u00a0]: Copied! <pre>class Head(nn.Module)\n</pre> class Head(nn.Module) In\u00a0[\u00a0]: Copied! <pre>def __init__(self, head_size):\n    super().__init__()\n    self.key = nn.Linear(n_embd, head_size, bias=False)\n    self.query = nn.Linear(n_embd, head_size, bias=False)\n    self.value = nn.Linear(n_embd, head_size, bias=False)\n    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    self.dropout = nn.Dropout(dropout)\n</pre> def __init__(self, head_size):     super().__init__()     self.key = nn.Linear(n_embd, head_size, bias=False)     self.query = nn.Linear(n_embd, head_size, bias=False)     self.value = nn.Linear(n_embd, head_size, bias=False)     self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))      self.dropout = nn.Dropout(dropout) <ul> <li>We are giving the input <code>head_size</code> and we are passing it to the <code>key</code>, <code>query</code> and <code>value</code> - <code>Linear</code> layers. People don't usually use <code>bias</code> for these so they are set to false.</li> <li><code>tril</code> is not a parameter (this is what makes that lower triangle in the matrix example that we saw), so considering the \"PyTorch naming convintion\" we register them as buffers so that we can assign it to the module.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def forward(self, x):\n       # input of size (batch, time-step, channels)\n       # output of size (batch, time-step, head size)\n       B,T,C = x.shape\n       k = self.key(x)   # (B,T,hs)\n       q = self.query(x) # (B,T,hs)\n       # compute attention scores (\"affinities\")\n       wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -&gt; (B, T, T)\n       wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n       wei = F.softmax(wei, dim=-1) # (B, T, T)\n       wei = self.dropout(wei)\n       # perform the weighted aggregation of the values\n       v = self.value(x) # (B,T,hs)\n       out = wei @ v # (B, T, T) @ (B, T, hs) -&gt; (B, T, hs)\n       return out\n</pre> def forward(self, x):        # input of size (batch, time-step, channels)        # output of size (batch, time-step, head size)        B,T,C = x.shape        k = self.key(x)   # (B,T,hs)        q = self.query(x) # (B,T,hs)        # compute attention scores (\"affinities\")        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -&gt; (B, T, T)        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)        wei = F.softmax(wei, dim=-1) # (B, T, T)        wei = self.dropout(wei)        # perform the weighted aggregation of the values        v = self.value(x) # (B,T,hs)        out = wei @ v # (B, T, T) @ (B, T, hs) -&gt; (B, T, hs)        return out <ul> <li>We passing <code>x</code> as the input, calculating the key and query.</li> <li>Then we are calculating the attention scores inside <code>wei</code>.</li> <li>Here we are making sure that <code>tril</code> doesn't communicate with the future (the additional values beyond the character point we are in) <code>wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))</code> so that acts as a decoder block.</li> <li>Then its the softmax, aggregating the value and out.</li> </ul> <p> </p> <p>Implementing the Multi-Head Attention Layer</p> In\u00a0[\u00a0]: Copied! <pre>class MultiHeadAttention(nn.Module):\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n</pre> class MultiHeadAttention(nn.Module):      def __init__(self, num_heads, head_size):         super().__init__()         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])         self.proj = nn.Linear(head_size * num_heads, n_embd)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out <ul> <li>Here we want multiple heads of self attention running in parallel. So in pytorch you mention the number of heads and the head size, and you run all of them in parallel into a list. Finally, you simply just concatenate all of the outputs and we are concatenating over the channel dimension (dim=-1)</li> <li>So this allows the tokens to communicate with each other, like the vowels, the correlation between them and finding different things, so this helps in providing different communication channels allowing the gathering of different types of data and decode the output.</li> </ul> <p> </p> <p>Implementing Feed Forward</p> In\u00a0[\u00a0]: Copied! <pre>class FeedFoward(nn.Module):\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</pre> class FeedFoward(nn.Module):      def __init__(self, n_embd):         super().__init__()         self.net = nn.Sequential(             nn.Linear(n_embd, 4 * n_embd),             nn.ReLU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x) <ul> <li>After the self attention where the tokens are communicating with each other, they need to start processing it individually to understand and that is where feed forwarding comes in. So Linear layer is applied on a per token level.</li> <li>So you can say that after the 'communication' in the previous layer, here is where the model 'computes' them.</li> <li>The <code>4 *</code> is done because in the paper for that layer they have mentioned that for them the dimensionality of input and output is 512 and the inner layer is 2048, basically the inner layer is 4 times the value, therefore we've added that as well. And the parameters are switched i.e. <code>(n_embd, 4 * n_embd)</code> then <code>nn.Linear(4 * n_embd, n_embd)</code> for the implementation of residual connections. \u00a0</li> </ul> <p>Putting both the communication and computational modules together in a Block</p> In\u00a0[\u00a0]: Copied! <pre>class Block(nn.Module):\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n</pre> class Block(nn.Module):      def __init__(self, n_embd, n_head):         # n_embd: embedding dimension, n_head: the number of heads we'd like         super().__init__()         head_size = n_embd // n_head         self.sa = MultiHeadAttention(n_head, head_size)         self.ffwd = FeedFoward(n_embd)         self.ln1 = nn.LayerNorm(n_embd)         self.ln2 = nn.LayerNorm(n_embd) <ul> <li>Overall that is what is happening in the transformer model diagram as well, all the self attention layer communication (except one where cross attention is happening, we haven't implemented that in this) and then the feed forwarding computation are grouped together in a Block which is represented as <code>Nx</code> in the diagram.</li> <li>Therefore we have the Block module where we have the communications taking place in <code>MultiHeadAttention(n_head, head_size)</code> followed by the computation <code>FeedFoward(n_embd)</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n</pre> def forward(self, x):         x = x + self.sa(self.ln1(x))         x = x + self.ffwd(self.ln2(x))         return x <ul> <li>Residual connection is implemented here <code>x = x + &lt;self attention and feed forward layer calls&gt;</code></li> </ul> <p> </p> <ul> <li>This is the Add part in the transformer model diagram within <code>Add &amp; Norm</code>.</li> <li>The Norm in that is basically a Layer Norm, and for that implementation we just do our Batch Normalisation (obviously cutting out some of the parts for example the buffers which were not required).</li> <li>Turns out, in the current implementation of Transformer model, there are very few changes to it but we now implement the 'Add &amp; Norm' layer before the Multi Head Self attention block (and that is what we have implemented). This is what we see in <code>self.ln1 = nn.LayerNorm(n_embd)</code>, <code>self.ln2 = nn.LayerNorm(n_embd)</code>.</li> </ul> <p> </p> <p>Finally, Scaling up the model</p> <p>There were some final touch ups done to the code including:</p> <ul> <li><code>n_layer</code>, <code>n_head</code> (in GPTLanguageModel class)</li> <li>Adding a <code>nn.Dropout(dropout)</code> (see feed forward, multi head and head classes), which was taken from this paper, see diagram in page 2, where it just freezes some of the nodes during both the forward and backward pass basically and we are using this as a regularization technique as we are scaling up the model (and concerned about overfitting).</li> <li>Hyperparameter values have also been changed.</li> </ul> <p> </p> <ul> <li>The final complete code can be found here</li> <li>The output produced can be viewed here.</li> </ul> <p> </p> <p>Final Thoughts: You will notice that the output won't be too great, as this is a transformer model trained on a character level, based on just the 6 million characters dataset on Harry Potter novels. Lastly, this is a decoder only transformer model as we see on GPT, So it only generates texts similar to what was fed into it. The additional layer of self attention, cross attention and decoder block haven't been implemented as they served a different purpose while the paper was written (it was written for you can say language translation, as the encoding was to convert from one language to another. The translated language is what we would have seen from the decoder).</p>"},{"location":"ZeroToHero/GPT-1/gpt-dev/#baseline-language-modeling-and-code-setup","title":"Baseline language modeling and Code setup\u00b6","text":""},{"location":"ZeroToHero/GPT-1/gpt-dev/#building-the-self-attention","title":"Building the \"self-attention\"\u00b6","text":""},{"location":"ZeroToHero/GPT-1/gpt-dev/#building-the-transformer","title":"Building the Transformer\u00b6","text":""},{"location":"ZeroToHero/GPT-2/","title":"TRANSFORMER MODEL - GPT 2","text":"<p>COMING SOON</p>"},{"location":"ZeroToHero/GPT-Tokenizer/","title":"TOKENIZATION","text":"<p>COMING SOON</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/","title":"DEEP DIVE INTO LLMs","text":"<p>Timeline: 19th - 23rd March, 2025</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#section-i-pre-training","title":"SECTION I: PRE-TRAINING","text":""},{"location":"ZeroToHero/General-MISC/deepdive-llms/#pretraining-data-internet","title":"Pretraining data (internet)","text":"<p>(Fig. STEP 1) Hugging face site - FineWeb dataset.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#tokenization","title":"Tokenization","text":"<p>(Fig. STEP 2) Website was useful to view tokenization and <code>cl100k_base</code> is what GPT-4 base model uses as its tokenizer.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#neural-network-io","title":"Neural network I/O","text":"<p>(Fig. STEP 3 (i)) Neural network internals</p> <p>(Fig. STEP 3 (ii)) The model architecture visualization site, pretty good interactive + detailed overview.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#inference","title":"Inference","text":"<p>(Fig. STEP 4) Basically means, \"To generate data, predict one token at a time\". And this is what ChatGPT is essentially doing, it is inferencing, its generating next words and that is what we are seeing in the tool itself.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#gpt-2-training-and-inference","title":"GPT-2: training and inference","text":"<p>(Fig. Demo)</p> <ul> <li>Timestamp: 31:09 very nice demo to see the internal processing of training the model and it inferencing the output (Will cover this in in my own GPT2 implementation as I do that lecture).</li> <li>Timestamp: 39:30 -\u00a0 42:52 Loved this part, the best visual explanation of how everything looks in a large scale and in industry standard POV.</li> <li>Resource for renting Cloud computers with GPUs.</li> </ul>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#llama-31-base-model-inference","title":"Llama 3.1 base model inference","text":"<ul> <li> <p>(Fig. \"Base\" models in the wild) One point I liked here (which I wasn't aware of but was always curious on) is explaining the meaning of the model names, 'Instruct' meaning it is an assistant model. 'BASE' is of course the base model that is obtained after pretraining.</p> <p>Resource for interacting with base models (paid).</p> <p>Practical points on Base models:</p> <ul> <li>Also remember, this is not yet an assistant. It\u2019s a token generator, so more of a 'Stochastic system'. So even if you ask a question for let's say recommendations, it will start to put out tokens and most of them maybe correct. But it is important to remember that in this model, more the frequency of certain information in the training data, the more likely it is to remember that and generate it. So this model, right now will mostly just generate texts it has the highest probability of remembering.</li> <li>There are also cases where 'Regurgitation' happens where the model ends of remembering EXACTLY what it learnt during its training, word by word. This is usually undesirable. This mainly happens because it might be the case where, that particular information had appeared or was trained in multiple epochs (because it is from a very reliable source with rich data, like Wikipedia), so the model ends up remembering it. Eg shown in video: Adding a line from Zebra article from Wikipedia, most of it is almost exactly the same, but at some point it will deviate and start to produce something random.</li> <li>Then comes the most famous case- 'Hallucination'. Where when the model is asked something it definitely wasn't trained on, then it just takes its best guess and starts to produce data. Eg. in video: 2024 presidential election as the data for llama3 was till the end of 2023 only.</li> <li>Another popular case where we implement 'Few shot prompt' where see the model's 'Context learning ability' in action. Eg in the video: You give a set of examples, like a word in English and its Japanese equivalent, finally you just give an English word and ask it to generate, and it will.</li> </ul> </li> </ul>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#pretraining-to-post-training","title":"Pretraining to post-training","text":"<p>Here we come to the end of SECTION I, where we went through the 'PRETRAINING' stage and the output produced is the 'BASE MODEL' as we see in the diagram. Now, most of the computational heavy lifting is already done here so in this next stage it will be fairly less expensive to implement. And this stage is called 'POST-TRAINING'.</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#section-ii-post-training-supervised-finetuning-sft","title":"SECTION II: POST-TRAINING (Supervised Finetuning - SFT)","text":""},{"location":"ZeroToHero/General-MISC/deepdive-llms/#post-training-data-conversations","title":"Post-training data (conversations)","text":"<ul> <li> <p>(Fig. Conversations) We want to convert the base model to an actual assistant which can handle conversations. There are 3 examples which we can take:</p> <ol> <li>A General conversation, question and answering with follow ups</li> <li>A friendly assistant (based on the last message in the second block in the figure)</li> <li>An assistant with the capability to deny anything.</li> </ol> <p>Now, we are not going to explicitly program the assistant using python or anything that. Because this is NN, we train this by providing datasets. So, the dataset will contain hundreds and thousands of conversations like the three examples mentioned above and more. Therefore you can say that the assistant is being \"programmed by example\".</p> <p>In this stage of post-training, we take the base model obtained after pre-training, we chuck out the internet information dataset on which it was training on and swap it with these new conversations dataset and train it. The pre-training may take months, let's say in this example 3 months. Then the post-training would only take like 3 hours.</p> </li> <li> <p>(Fig. Conversation Protocol / Format) The questions now are: Where are these conversations? How do we represent them? How do we get the model to see conversations instead of just raw texts? What are the outcomes of this kind of training? What do you get (in like a psychological sense) when we talk about the model?</p> <p>The tokenization of the conversation. Different LLMs have different methods. When we look at the one GPT-4o uses we have <code>|im_start|</code>, <code>|im_sep|</code> and <code>|im_end|</code>, the <code>im</code> stands for <code>imaginary monologue</code>. For example, the first box conversation in the diagram would roughly convert to 1D list of 50 tokens as seen in the diagram. Also, those <code>im</code> labels will be represented as \"special tokens\" so you will see that they will all have their similar respective values, therefore useful for labelling, telling the model what it needs to focus on.</p> <p>The outcome of this is such that, next time while training, lets say at OpenAI. They would just leave the last answering part blank, like |im_start|assistant:|im_sep| ____` and this is where the LLM takes over and starts predicting the next possible tokens (like how we see in Step 4 during pre-training).</p> </li> <li> <p>(Fig. Conversation Datasets) Introduction to the 'Training language models to follow instructions with human feedback'\u00a0 :</p> <ol> <li>Human labellers were involved, examples also shown in the paper.</li> <li>The labelling instructions provided to the human labellers- be Helpful, Truthful and Harmless (Normally the instructions are very long, detailed and heavy, so obviously professionals are made to work on this).</li> <li>Sample open source dataset which followed the similar above pattern of providing instructions to the people on the internet and they produced the conversations forming the datasets, all properly labelled.</li> </ol> <p>Naturally, we cannot cover every possible conversation a user may have, but as long as we have enough concrete examples, the model will take on that persona and will know what kind of answer the user will need. Therefore also taking on the <code>Helpful, Truthful and Harmless</code> assistant persona.</p> <p>In this day and age, we obviously don\u2019t need humans to do this, naturally we have LLMs that help us perform this itself.</p> <p>Resources: UltraChat, Visual diagram of categories of questions asked.</p> </li> <li> <p>(Love this part by the way) Now we know how ChatGPT is actually producing these outputs or what are we actually taking to in ChatGPT. Its definitely not some \"magical AI\", Ultimately its from these labelled datasets which the labellers make. So its like almost having a simulation of those specialized labellers (educated experts on coding, cyber security etc.) we are having the conversation with real time. So its not really a \"magic output\".</p> <p>So next time you query something on ChatGPT, lets say <code>Top 5 landmarks in India</code> and it provides a list of answers. There is a high probability that if that same question was present in the dataset provided by the labellers, the model will just directly simulate that response. If not, then it\u2019s a more 'emergent' answer which the model processes based on what will be the most likely set of locations to recommend based on what people like, visiting frequency etc.</p> </li> </ul>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#hallucinations-tool-use-knowledgeworking-memory","title":"Hallucinations, tool use, knowledge/working memory","text":"<ul> <li> <p>(Fig. Hallucinations) Naturally, there will be effects of the above training pipeline that we saw (the emerging cognitive effects as sensei said), the most popular one is ofcourse- Hallucinations. We've seen plenty of examples on that. Again the best way to probably explain this is, the traditional model doesn't have access to the internet, so it doesn't know. It just has the probabilities of tokens that it has and it just samples it from them.</p> <p>Mitigation #1</p> <ul> <li>We see how Meta solved this in their Llama3 open source model. In their paper, labelled under the section 'Factuality' what they did is that: if there is something that the model doesn't know, then they take that and during training again, for that question they label the correct answer for that as \"the model doesn\u2019t know\". So that actually worked (it does sound simple in principle lol).</li> <li>Now keeping that above principle in mind, imagine a system of LLMs doing this: LLM1 produces Q&amp;A for a set of data -&gt; Take just the Q and ask it to a LLM2 -&gt; The LLM2 produces the A and is sent to LLM1 for checking/verifications -&gt; If it is correct then LLM2 knows and is all good, else LLM2 should say it doesn't know.</li> <li>Therefore we interrogate the model LLM2 multiple times to see the answers it generates and compare it with LLM1 to check. If its wrong, then we go and label for that question in LLM2 to say that - the correct answer for that question is \"I don't know\".</li> </ul> <p>Mitigation #2</p> <ul> <li>Using Tools - One of the tools is Web search so, Allow the model to Search! (Just like how humans would do lol). So we can introduce new tokens like <code>&lt;SEARCH_START&gt;</code> and <code>&lt;SEARCH_END&gt;</code>, so when this is encountered, the model will pause the generation, it will consider the text between those 2 labels, copy paste it to bing/google search and then the correct answers are then passed to the model's context window. Then it's from that the data is taken and passed on as a response. So the context window is like a active working directory which acts a source of memory for the model from which it recollects data, tokenises them and produces the answer.</li> <li>So the context window already has information on which it was trained on. Just that now we have these tools to function, where they search and collect more correct and relevant data and update the context window/add to it, which the model will reference from.</li> <li>Now, how does it know it needs to use search? Again we provide examples to it during training. Once we show the model enough examples of data on when, how and where it needs to do search, it gets a pretty good intuition of it and just performs the task. So all you have to do is just teach it :)</li> <li>Timestamp: 1:37:16 to 1:38:28 where we see that happening in ChatGPT. Just watch that clip if you want a better visual understanding of the above points.</li> </ul> </li> <li> <p>(Fig. \"Vague recollection\" vs. \"Working memory\") Knowledge in the parameters == Vague recollection (e.g. of something you read 1 month ago)</p> <p>Knowledge in the tokens of the context window == Working memory (eg. you provide the paragraph context to the model, so the model has direct access to the information and doesn't have to actively recollect it. This provides better answers. Even as humans we give better answers if we had the immediate context to the info VS something we read months ago.</p> </li> </ul>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#knowledge-of-self","title":"Knowledge of self","text":"<ul> <li> <p>(Fig. Knowledge to self) This part of the video was kind of like a slap in the face for me lol, I have done this countless of times to various different chatbots that I've encountered thinking I was the smart one. But it turns out, it was a really dumb thing to do lol (always learning something new eh).</p> <p>This case explains the time when the user asks \"Who built you?\" or \"What do you use\". Now, if we think about everything we have learned so far in this lecture, if it is something the model hasn't been trained on, it will most probably or if not, WILL hallucinateand will say \"by OpenAI!\" as that\u2019s what most of the training data may have contained (LMAO CRYING). But if it is like a case in this open source model where it has been specifically instructed to answer that question, then it will take from its knowledge and answer the question based on what it was trained on.</p> <p>Or we can also add 'System Message' as we see now, so for each conversation context window, the system message is added as invisible tokens :)</p> </li> </ul>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#models-need-tokens-to-think","title":"Models need tokens to think","text":"<ul> <li> <p>(Fig. recall: next token probabilities) This part is probably the next most favorite part of the lecture for me.</p> <p>Here, we are given two sample response from the model and are asked to choose the better one. Naturally, what i did was to see 'HOW I WOULD SOLVE' this problem and concluded that the second option (right) was better. Reason: I broke it down the same way in my head (logic wise only).</p> <p>Turns out there is a more computational way to explain this:</p> <ul> <li>Now, look at the diagram. While thinking of a response you can image the model generating the token from left to right. And for the token which will contain the answer is where the heavy amount of computation will happen.</li> <li>We should also note that, in a model such as the one we see in the image, there aren't many layers for computation (the attention layers etc.). So there is just too much computation happening in the forward pass in that one token and that is not good.</li> <li>Therefore it is important that the model breaks down the process into various steps before concluding to the answer. This way we are not stressing all the load on one token, but taking our time to reach to the answer (therefore also spreading out the computational resources, so not much strain on the forward pass of the network)</li> </ul> <p>What's even more beautiful is that we can also see this during the tokenization process in that site as well:</p> <ul> <li>Place the first response and see the token breakdown. After <code>'The' ' answer' 'is' '_'</code>  the token is left for the answer and it is that one token where the model is putting way too much computational load. And once that is done, the task is considered to be over and the rest of the message that you see is just something that it makes up!</li> <li>Similarly, if you see the token breakdown of the second response. You would see that it is \"spreading out it's computation\" until the final answer is reached the end only.</li> <li>This way we can conclude and mark that the second answer is significantly better and correct. In fact the first answer is terrible and is bad for the model, as that indicates that it hasn't been labelled properly during training (as we end up teaching the model to do the entire computation in a single token which is really bad). So, seeing that your model actually breaks down its task to multiple token before concluding to the final answer is actually a good sign.</li> </ul> <p>Now, we don't really have to worry about this as people at OpenAI (taking ChatGPT as an example) have labelers who actually worry about these things and take special care that the right answer generation is considered. So the next time you see ChatGPT break down a math problem into multiple steps before reaching to the conclusion (even if it is a simple one), remember that the model is doing that for itself and not for you :)</p> <p>We've also had cases where we specifically ask the model to calculate the answer in one go. If it is simple, it will give you the correct one. But if it is a harder one, then you would see that it fails to give the correct answer. But it will find it easier and provide the correct answer once it breaks down the process (You can see this example in the video from 1:54:03  to 1:55:50. I have also faced this in real life during work and that explains a lot haha)</p> <p>Note: Turns out, even the process of allowing the model to break down the code is kind of risky as it itself is thinking of the answer (like how we do mental calculations) which is not reliable in most cases. So here, we can instruct it to use a tool like <code>Use code</code>, so what this will do is it will use the special tokens to call a python interpreter. So this can be more reliable as the solution is programmatically generated (there is also no additional tokens for it, the code itself is generated, passed to the interpreter and answer is generated). So don't fully trust the output when it does it using memory, use tools whenever possible.</p> </li> <li> <p>(Fig. Models can't count) Another example of relying on tools while the model performs computation. Sensei uses the example of counting the dots. Now, models aren't good at counting (we've also seen that in the way it splits the tokens). But we know its good at 'copy pasting'. So when i ask it to \"use code\", it will correctly copy the dots given to it and provide it to the interpreter. So this way we have also broken down the task which is easier for the model to perform.</p> </li> </ul> <p>Final summary for this section: So we can see how models actually need token to think (phew, this took me ages to cover lol, but so worth it).</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#tokenization-revisited-models-struggle-with-spelling","title":"Tokenization revisited: models struggle with spelling","text":"<p>(Fig. Models are not good with spelling)</p> <ol> <li>Models don't see characters, they see tokens.</li> <li>Models are not good at counting.</li> </ol> <p>The above two points are so massively important as we've seen simple tasks where models fail to perform like \"Print every 3rd character from this word\" or more famously \"How many r's are there in strawberry\". The root cause of that is tokenization, therefore those two points.</p> <p>So here is where we come back to tokenization again. Incase of those example queries, the model interprets them differently as it sees them as tokens and on top of that if you ask it to count, we are basically asking it to do two of the most obvious tasks it is bad at.</p> <p>Thoughts</p> <p>This reminds me of the famous LLM lecture video from Stanford which i watched a couple of months back (while i was still in the early stages of makemore series) where during the first half, the lecturer was asked something and he said \"in those cases is where we will see that TOKENIZATION itself is the main issue\". It struck me a little that time as i thought tokenization was brilliant as the model is breaking down its task. But now i see what he meant there :) -Also yes I haven't finished that lecture yet lol, i should actually.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#jagged-intelligence","title":"Jagged intelligence","text":"<p>You can watch the timestamp for this here, But mainly its to show some headscratcher situations with the model. The main takeaway is this- See it as a stochastic system, you can just rely on it as a tool and not just \"let it rip\".</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#section-iii-post-training-reinforcement-learning-rl","title":"SECTION III: POST-TRAINING (Reinforcement Learning - RL)","text":""},{"location":"ZeroToHero/General-MISC/deepdive-llms/#supervised-finetuning-to-reinforcement-learning","title":"Supervised finetuning to reinforcement learning","text":"<p>Everything we've seen so far are the various stages of building a LLM. Usually in large companies (like OpenAI), each of the above stage have their own respective teams doing that work. And RL is considered as the final stage of this.</p> <p>To get an intuition of what this is, we see the example of a textbook which we used to learn from in school- The theory is the Pretraining, the Solved examples are the Supervised finetuning and the problem questions which we need to solve is Reinforcement learning (The question is there, the final answer may also be provided, but its the process of getting there/how we solve it is how we learn).</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#reinforcement-learning","title":"Reinforcement learning","text":"<p>\"What is easy or hard for us as humans/human labelers is different than what's easy/hard for models/LLMs, as they have different conditions. The token sequences are like a whole different challenge for it\".</p> <p>The process usually goes like:</p> <ul> <li>We generated 15 solutions.</li> <li>Only 4 of them got the right answer.</li> <li>Take the top solution (each right and short).</li> <li>Train on it.</li> <li>Repeat many, many times.</li> </ul> <p>So RL is where we really get dialed-in. We really discover the solutions that work for the model, we find the right answers, we encourage them and the model just gets better over time.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#deepseek-r1","title":"DeepSeek-R1","text":"<p>Okay so, it is important to know that RL is something that has emerged very recently in the industry. Step 1 &amp; 2 have been around for a while and companies like OpenAI have been working on this internally. But it was the DeepSeek paper that had publicly put it out and had also implemented it.</p> <p>One of the most famous phenomenon you can say, that was introduced in this paper was the \"aha moment\" where the model is just rethinking by itself as it looks into the approach. So if a problem is provided, it looks into it with different perspectives, takes analogies and thinks internally in all the possible ways. And the reason this is incredible is because you cannot hardcode all of these. So this does show a huge step taken into the process of RL. Also, it is important to know that, this process consumes A LOT and I mean A LOT of tokens. So naturally you will see the response improving as more tokens is utilized.</p> <p>Read these snippets from the paper to have a better explanation: Explanation and Example.</p> <p>There are a few ways that you can run these state of the art models where it is already hosted for you: Hugging Face inference playgroundor Together AI.</p> <p>One interesting point sensei has mentioned here while comparing the OpenAI's o1 models and DeepSeek, is that OpenAI chooses not to show the entire Chain of Thought (CoT) under the fear of \"distillation risk\" where someone could come and just imitate the reasoning steps of the model and almost recreate its performance (in terms of its performance settings), therefore they cut it short and don't show all of it's thinking texts.</p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#reinforcement-learning-from-human-feedback-rlhf","title":"Reinforcement learning from human feedback (RLHF)","text":"<p>RL is fairly under grasp when we are using it in a \"verifiable domain\" like solving a math equation where we know what the final answer is. Now, in cases of \"un-verifiable domain\" like telling jokes, there is no fixed answer so how do we train the model? That's where RL with Human Feedback comes in.</p> <p>We get humans to evaluate a good chunk of data -&gt; Train a whole different NN on it -&gt; Use that Model produced to evaluate the responses and gives its scores.</p> <p>So the RLHF approach is something like:</p> <ul> <li>STEP 1: Take 1,000 prompts, get 5 rollouts, order them from best to worst (cost: 5,000 scores from humans)</li> <li>STEP 2: Train a neural net  simulator of human preferences (\"reward model\")</li> <li>STEP 3: Run RL as usual, but using the simulator instead of actual humans</li> </ul> <p>Lastly, it is important to note that we cannot rely on RLHF completely, we need to peak its training at one point and then just crop it. If we allow it to grow after that, then it will just a way to \"game\" to model it is evaluating and the final outputs rated will almost be nonsensical. </p> <p>It's best covered by sensei in these points-</p> <p>UPSIDES:</p> <ul> <li>We can run RL, in arbitrary domains! (even the unverifiable ones)</li> <li>This (empirically) improves the performance of the model, possibly due to the \"discriminator - generator gap\".</li> <li>In many cases, it is much easier to discriminate than to generate. E.g. \"Write a poem\" vs. \"Which of these 5 poems is best?\"</li> </ul> <p>DOWNSIDES:</p> <ul> <li> <p>We are doing RL with respect to a lossy simulation of humans. It might be misleading!</p> </li> <li> <p>Even more subtle: RL discovers ways to \"game\" the model.</p> </li> <li> <p>It discovers \"adversarial examples\" of the reward model. E.g. after 1,000 updates, the top joke about pelicans is not the banger you want, but something totally non-sensical like \"the the the the the the the the\".</p> </li> </ul> <p> </p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#section-iv-conclusions","title":"SECTION IV: CONCLUSIONS","text":""},{"location":"ZeroToHero/General-MISC/deepdive-llms/#grand-summary","title":"Grand summary","text":"<p>I encourage you to just watch this part of the grand summary once you've read the notes and just want to do a quick refresh of your knowledge. Start from here.</p> <p>Ultimately, we've learnt the 3 main stages:</p> <ul> <li>Stage 1: Knowledge Acquisition of the internet.</li> <li>Stage 2: Is where the personality comes in.</li> <li>Stage 3:  Perfecting thinking process and improving the problem solving.</li> </ul> <p> </p>"},{"location":"ZeroToHero/General-MISC/deepdive-llms/#lecture-resources","title":"Lecture resources","text":"<ul> <li>Click here to view complete lecture drawing board image.</li> <li>Link to video: Watch here</li> <li>Download full drawing board here</li> <li>Import and view the above drawing board on Excalidraw</li> </ul> <p>Phew, that was one long lecture. Took me days to cover but it was truly refreshing. As he said again, very existing field to be in at a time like this. See you on the next one, Happy learning!</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/","title":"INTODUCTION TO LLMs","text":"<p>Timeline: 7th &amp; 17th-19th March, 2025</p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#llm-inference","title":"LLM Inference","text":"<p>A LLM is essentially two files (assuming we have a hypothetical directory) where there is a parameters file and a code file (c or python, any arbitrary language).</p> <p>The model in the hypothetical directory we are assuming is llama2 70b. This is popular because it is a open weights model where the weights, architecture and the paper were all released by Meta. Unlike ChatGPT, Claude or Grok where the model architecture was never released and is owned by the respective companies. We only use those models through a web interface but we actually don\u2019t have access to that model.</p> <p>So coming back to our directory: we have a parameters file and a code file which runs those parameters. The parameters are the weights/parameters of the neural net which is actually the language model.</p> <p>Now, because this is a 70b parameter model, each parameter is stored as 2 bytes so the total file size will be of 140GB. Its 2 bytes because this is a float16 number i.e. it's data type. This file just contains a large number/lists of parameters for that neural net. And we need something to run that, that where the code file comes in.</p> <p>It wouldn't take more than 500 lines of code (lets say in C) with no other dependencies to implement this neural network architecture, which in turn uses those parameters to run the model.</p> <p>These two files form a package of their own, so you don\u2019t need anything else to run this model (not even the internet), so you just compile the C and it generates the binaries to point to the parameters and you can start to talk to this model.</p> <p>Also, larger the parameter size of the model, more time it takes to run.</p> <p>Now, running the model is not really a big task, not a lot is required. But the computational complexity comes in when we would like to get/generate those parameters. So how do we get the parameters? where are they from?</p> <p>Whatever is in the code file i.e. the neural network architecture - the forward pass implementation etc., is all algorithmically understood and so on. But the magic really happens in the parameters. Therefore, how do we obtain them?</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#llm-training","title":"LLM Training","text":"<p>The parameters are obtained through a method called Training. So this step is essentially called 'Model Training'. What we read about before this is called 'Model Inference' where we just run the final model on a Mac or any device of yours. </p> <p>This process is where there are more computational effort is needed. So we take a large chunk of the internet ~10TB of data (this is obtaining by crawlers) -&gt; this is then passed to a cluster of GPUs (6000 GPUs running for 12 days costing about 2 million dollars) -&gt; finally to obtain the parameters file.</p> <p>Now the final parameters file can be thought of a zip file, except incase of a zip file its lossless compression, here there is loss. So what is happened is that there is a gibberish bulk of data (LLM \"dreams\" the internet data, were the texts produced wont necessarily make sense and it just generates something similar to what it learnt. Like codes, product reviews, articles etc.) that is actually generated based on the chunk of data provided to it, its not really a replica of it. Also notice that the file size is also considerably less as compared to the input chunk of data 10TB -&gt; 140GB.</p> <p>Just so we know, the above estimates of values in today's terms are rookie numbers. If you want to obtain models by OpenAI, Claude etc., then you just 10x those values. Therefore, they even cost tens of millions of dollars.</p> <p>Now, its essential we talk about what a neural network actually is. The most popular explanation is ofcourse the 'predicting the next word' example were the NN is fed in some sentence and it is supposed to predict the next word in the sentence/finish it. </p> <p>This may seem like a fairly simple task, but this is one of the best possible assignment for a NN to learn from. Now consider this example, the NN is fed a page from wikipedia on 'Naruto'. It needs to learn everything about Naruto inorder to answer/complete the sentence on it. For example, who are his parents, name of his village, name of his sensei etc. Since it has been trained on all of this, and the user types in the sentence: <code>Naruto wants to become a _</code>, the model would know what the next word is and complete it by saying <code>Hokage</code> with probably a ~98% match.</p> <p>Fun fact: Till here is were the GPT-1 implementation was done in the learning path :)</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#how-do-they-work","title":"How do they work?","text":"<p>Here is were we drop probably the most famous, groundbreaking concept in the world of ML - The Transformer architecture. The best part about this is that we know each and every part of this structure and what each of those sections perform mathematically. </p> <p>But little is known in full detail:</p> <ul> <li>The billions of parameters are just dispersed through the network of this system.</li> <li>We know how to iteratively adjust or optimize these parameters over time to make it better at prediction.</li> <li>We don't really know how the billions of parameters collaborate to do it.</li> </ul> <p>So ultimately we can just measure how we can improve them but we have no idea how/what it's actually doing to get there.</p> <p>What we know is that these NN build and maintain some kind of knowledge database to store all the information learnt, but even the data there is bit strange and imperfect. For example, there is this example called \"reversal curse\" where:</p> <pre><code>Q. Who is Naruto's father?\nA. Minato, the fourth hokage.\n</code></pre> <p>But if you ask,</p> <pre><code>Q. Who is Minato?\nA. I have no idea.\n</code></pre> <p>So ultimately, you can see LLMs as a inscrutable artifacts. What we mean by that is it cannot be studied like a car were we know all the different components. Although there are some advanced fields were researchers go hands on and dig deep to understand how every section work but yes it is a very sophisticated process.</p> <p>I am thinking maybe we can bring in one of chris nolan's quote from TENET here: \"Don't try to understand it, feel it.\" (Not cool for academic situations I know, but I couldn't help myself lol)</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#finetuning-into-an-assistant","title":"Finetuning into an Assistant","text":"<p>The above process was just the first step of training called the 'pre-training'. Now comes the second step is called 'finetuning'.</p> <p>The documents obtained through pre-training is not very useful as its just random chunk of data, so we need to turn that into an assistant like model.</p> <p>So how do we do that? We swap the dataset (internet data) on which the training was happening and replace it with the data which is a lot more formatted, i.e. in a question and answer format. Which is either written by a large group of people or a model/AI itself generates it (synthetic data).</p> <p>Therefore if you look at it, in the first step it was quantity over quality but in the second step it is vice versa.</p> <p>Now also note that, after having done the fine tuning, if the user asks something which wasn't in the training, the model will still answer as it was trained to be a helpful assistant and must answer in that way. So its still generating a sequence of data but in a more structured/fine tuned way - left to right and then right to bottom when it comes to framing words -&gt; sentences -&gt; paragraphs. In the end you can say its more of a formatting stage as well as it is converting internet data into a more structured helpful assistant like manner.</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#summary-so-far","title":"Summary so far","text":"<p>Now, the llama2 model that we took as example, Meta had released both the base model and the assistant model. The base model ofcourse aren't very useful, so if you ask it a question it will probably generate more questions. So what Meta has done is, they did the tough part (pre-training) and provided us with the base model (and they have also provided the assistant model so we can directly use it).</p> <p>Other ways of fine tuning mentioned in Appendix chapter of the video:</p> <ul> <li>Comparisons: providing different possible responses as options</li> <li>Labeling docs: an instruction document is provided as to how you need to label the data</li> <li>RLHF: Reinforcement Learning from Human Feedback (the name prettymuch explains it lol)</li> <li>Leaderboard: Like a chatbot arena, to rate which model performs better</li> </ul> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#interlude","title":"INTERLUDE","text":"<p>So, I have skipped the notes for Part 2 as at the time of watching the video vs at the time it was recorded by sensei, A lot has changed. Infact, a lot of his ideas were also implemented (also goes on to show how brilliant this man actually is)</p> <p>We can take the cases of some of his points like:</p> <ul> <li>In 'Leaderboard' we saw how only closed models like ChatGPT and Claude were on the top, only to be followed by open source models later behind. We couldn't finetune or modify those top performing models as they are closed source -&gt; But now we have Deepseek who entered the game, topped the charts with their model's performance and even open sourced it (This massively shook the industry by the way lol).</li> <li>'Tool Use' -&gt; Now known as 'AI Agents' were LLMs are used as the \"Brain\" of a system and tools are provided to it to perform various other tasks. Ofcourse one example was already there at the time in the implementation of DALL-E, the image generation model.</li> <li>'Thinking Systems' -&gt; Again Deepseek comes into picture here, but with OpenAI's o1 model which was released slightly earlier, where the model would be provided time to think before it answers.</li> <li>'LLM OS' -&gt; This is something which is not yet achieved at the time of me writing this, but I do know some companies which are on route to achieving this. My personal favorite that I am keeping track of is Letta AI, where I first learnt about this :)</li> </ul> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#llm-security","title":"LLM Security","text":"<p>Naturally as every new tech emerges, there will be security challenges. Here are some security related challenges where its like a 'cat-mouse' game as each of these challenges have also been overcome as we'll see.</p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#jailbreaks","title":"Jailbreaks:","text":"<ol> <li>Can't directly ask the process to make a dangerous chemical -&gt; but if you twist it by doing a role play (like telling your grandmom used to make it) then we jailbreak the model and it provides us with the response.</li> <li>The query being changed into its 'Base64 encoding' equivalent. Why this works is because it was only trained to refuse queries written in english, so it could be broken when provided in different language (real time example with claude)</li> <li>Providing your direct query and adding a 'Universal Transferable Suffix' - which is a chunk of random words carefully fine-tuned and can be appended to a prompt to jailbreak the model.</li> <li>The 'Scary panda' image - if you look closely it contains random noise, but when added with your dangerous prompt it jailbreaks the model as we are providing random noises to it.</li> </ol>"},{"location":"ZeroToHero/General-MISC/intro-llms/#prompt-injection","title":"Prompt Injection:","text":"<ol> <li>The attached image contains a very faint text which provides a new instruction and ending up hijacking the model to provide a different response.</li> <li>If the website sources itself contain hidden plain texts which ends up injecting a new prompt to the model.</li> <li>A google doc containing a prompt injection where it instructs it to pass details of the currently used account by sending a GET request (Real time example in BARD, although google engineers did prevent this. But it can be overcome if the attacker is within the google environment). So in this case the data of the user gets infiltrated to the attacker.</li> </ol>"},{"location":"ZeroToHero/General-MISC/intro-llms/#data-poisoning-backdoor-attacks","title":"Data poisoning / Backdoor attacks:","text":"<p>\"Sleeper agent\" attack - Adding a trigger word/phrase which then initiates an undesirable action which the attacker may have planned (This can happen when the attacker has access during the fine-tuning stage of the model. Turns out there also hasn't been any sufficient examples where this works in the pre-training stage, but it is important to be aware of such attacks).</p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#other-types-of-attacks","title":"Other types of attacks:","text":"<p>Adversarial inputs, Insecure output handling, Data reconstruction, Data extraction &amp; privacy, Denial of service (DoS), Escalation, Watermarking &amp; evasion, Model theft etc. (I haven't yet researched on all of them, but may try to comeback and update them here as I learn anything on it).</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#llm-security-conclusions","title":"LLM Security conclusions","text":"<p>Now, obviously that was scary (at least it was for me). Lucky for us, defences for all of these attacks have been developed, published and incorporated. So many of the above mentioned attacks may not even work anymore as these are patched over time!  But this goes on to show the 'cat and mouse' - 'attack and defence' game that we have been seeing in traditional security, now appearing in LLM security as well.</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/intro-llms/#lecture-resources","title":"Lecture resources","text":"<p>Link to video: Watch here</p> <p>So still a lot of ongoing work and is a very exiting place to be a part of :) See you on the next one, Happy learning!</p> <p> </p>"},{"location":"ZeroToHero/General-MISC/using-llms/","title":"USING LLMs","text":"<p>COMING SOON</p>"},{"location":"ZeroToHero/Makemore-part1/","title":"Makemore Part 1","text":""},{"location":"ZeroToHero/Makemore-part1/#language-model-1","title":"LANGUAGE MODEL - 1","text":"<p>Timeline: 4th - 22nd November, 2024</p>"},{"location":"ZeroToHero/Makemore-part1/#introduction","title":"Introduction","text":"<p>Welcome to my documentation for Makemore Part 1 from Andrej Karpathy's Neural Networks: Zero to Hero series. This section focuses on implementing a bigram character-level language model. Here, I\u2019ve compiled my notes and insights from the lecture to serve as a reference for understanding the foundational concepts and practical implementations discussed.</p>"},{"location":"ZeroToHero/Makemore-part1/#overview-of-makemore-part-1","title":"Overview of Makemore Part 1","text":"<p>In this part of the series, I explored the following topics:</p> <p>Implementing a Bigram Character-Level Language Model: The lecture introduces the basics of language modeling using bigrams, providing a step-by-step approach to understanding how character-level models can predict sequences of text.</p> <p>Key Concepts Covered:</p> <ul> <li>Introduction to <code>Broadcasting</code> and its use in neural networks</li> <li>Framework of language modeling, including model training and sampling</li> <li>Evaluation of loss functions, particularly negative log likelihood for classification</li> <li>Practical insights into the mechanics of character-level predictions</li> </ul>"},{"location":"ZeroToHero/Makemore-part1/#key-resources","title":"Key Resources","text":"<p>Video Lecture</p> <ul> <li>I watched the lecture on YouTube: Building Makemore Part 1</li> </ul> <p>Codes:</p> <ul> <li>The Jupyter notebooks and code implementations are available within this documentation itself.</li> <li>If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Language Model 1</li> </ul>"},{"location":"ZeroToHero/Makemore-part1/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li>The lecture documentation has been divided into 3 sets: Set A, Set B, and Set C.</li> <li>Each set has its own notes and notebook.</li> <li>Notes have been marked with timestamps to the video.</li> <li>This allows for simplicity and better understanding, as the lecture is long.</li> </ul> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Makemore-part1/A-Main-Notebook/","title":"Jupyter Notebook","text":"<p>Reading and exploring the dataset 00:03:03</p> In\u00a0[17]: Copied! <pre>words = open('names.txt', 'r').read().splitlines()\n</pre> words = open('names.txt', 'r').read().splitlines() In\u00a0[11]: Copied! <pre>words[:10]\n</pre> words[:10] Out[11]: <pre>['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']</pre> In\u00a0[22]: Copied! <pre>len(words)\n</pre> len(words) Out[22]: <pre>32033</pre> In\u00a0[23]: Copied! <pre>min(len(word) for word in words)\n</pre> min(len(word) for word in words) Out[23]: <pre>2</pre> In\u00a0[24]: Copied! <pre>max(len(word) for word in words)\n</pre> max(len(word) for word in words) Out[24]: <pre>15</pre> <p>Exploring the bigrams in the dataset 00:06:24</p> In\u00a0[25]: Copied! <pre>for word in words[:1]:  #Only taking the first word as an example\n    for ch1, ch2 in zip(word, word[1:]):    #Two columns are formed where The first character is taken and is zipped with its next one. So when there is only one character left, then it exits.\n        print(ch1, ch2)\n</pre> for word in words[:1]:  #Only taking the first word as an example     for ch1, ch2 in zip(word, word[1:]):    #Two columns are formed where The first character is taken and is zipped with its next one. So when there is only one character left, then it exits.         print(ch1, ch2) <pre>e m\nm m\nm a\n</pre> In\u00a0[26]: Copied! <pre>for word in words[:1]:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']    #Here we are just adding our own set of characters in the list, so as to get like this custom output - Labelling the first and the last character of the word\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n</pre> for word in words[:1]:     chs = [''] + list(word) + ['']    #Here we are just adding our own set of characters in the list, so as to get like this custom output - Labelling the first and the last character of the word     for ch1, ch2 in zip(chs, chs[1:]):         print(ch1, ch2) <pre>&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n</pre> In\u00a0[27]: Copied! <pre>for word in words[:3]:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n</pre> for word in words[:3]:     chs = [''] + list(word) + ['']     for ch1, ch2 in zip(chs, chs[1:]):         print(ch1, ch2) <pre>&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n&lt;S&gt; o\no l\nl i\ni v\nv i\ni a\na &lt;E&gt;\n&lt;S&gt; a\na v\nv a\na &lt;E&gt;\n</pre> <p>Counting bigrams in a python dictionary 00:09:24</p> In\u00a0[28]: Copied! <pre>b = {}  #We have created a dictionary 'b' - First data structure\nfor word in words[:1]:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2) #Then we have created a set 'bigram' which is just a set of two characters - Second data structure\n        b[bigram] = b.get(bigram, 0) + 1    #Here we are adding it to the dictionary 'b', where the key is 'bigram' (which is a set of character pairs) has the value counts of the set occoured (The number of times that set has occured)\n        print(ch1, ch2)\n</pre> b = {}  #We have created a dictionary 'b' - First data structure for word in words[:1]:     chs = [''] + list(word) + ['']     for ch1, ch2 in zip(chs, chs[1:]):         bigram = (ch1, ch2) #Then we have created a set 'bigram' which is just a set of two characters - Second data structure         b[bigram] = b.get(bigram, 0) + 1    #Here we are adding it to the dictionary 'b', where the key is 'bigram' (which is a set of character pairs) has the value counts of the set occoured (The number of times that set has occured)         print(ch1, ch2) <pre>&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n</pre> <p>Note: <code>b.get(bigram)</code> is the same as <code>b[bigram]</code>  Just that here: <code>b.get(bigram, 0)</code> if we don't get a bigram value, we want it to assign to 0     Finally we are adding one <code>+1</code> as we want to count the occurance.</p> In\u00a0[29]: Copied! <pre>b\n</pre> b Out[29]: <pre>{('&lt;S&gt;', 'e'): 1, ('e', 'm'): 1, ('m', 'm'): 1, ('m', 'a'): 1, ('a', '&lt;E&gt;'): 1}</pre> In\u00a0[30]: Copied! <pre>b = {}\nfor word in words[:3]:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0) + 1\n        print(ch1, ch2)\n</pre> b = {} for word in words[:3]:     chs = [''] + list(word) + ['']     for ch1, ch2 in zip(chs, chs[1:]):         bigram = (ch1, ch2)         b[bigram] = b.get(bigram, 0) + 1         print(ch1, ch2) <pre>&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n&lt;S&gt; o\no l\nl i\ni v\nv i\ni a\na &lt;E&gt;\n&lt;S&gt; a\na v\nv a\na &lt;E&gt;\n</pre> In\u00a0[31]: Copied! <pre>b\n</pre> b Out[31]: <pre>{('&lt;S&gt;', 'e'): 1,\n ('e', 'm'): 1,\n ('m', 'm'): 1,\n ('m', 'a'): 1,\n ('a', '&lt;E&gt;'): 3,\n ('&lt;S&gt;', 'o'): 1,\n ('o', 'l'): 1,\n ('l', 'i'): 1,\n ('i', 'v'): 1,\n ('v', 'i'): 1,\n ('i', 'a'): 1,\n ('&lt;S&gt;', 'a'): 1,\n ('a', 'v'): 1,\n ('v', 'a'): 1}</pre> In\u00a0[32]: Copied! <pre>b = {}\nfor word in words:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0) + 1\n\nb\n</pre> b = {} for word in words:     chs = [''] + list(word) + ['']     for ch1, ch2 in zip(chs, chs[1:]):         bigram = (ch1, ch2)         b[bigram] = b.get(bigram, 0) + 1  b Out[32]: <pre>{('&lt;S&gt;', 'e'): 1531,\n ('e', 'm'): 769,\n ('m', 'm'): 168,\n ('m', 'a'): 2590,\n ('a', '&lt;E&gt;'): 6640,\n ('&lt;S&gt;', 'o'): 394,\n ('o', 'l'): 619,\n ('l', 'i'): 2480,\n ('i', 'v'): 269,\n ('v', 'i'): 911,\n ('i', 'a'): 2445,\n ('&lt;S&gt;', 'a'): 4410,\n ('a', 'v'): 834,\n ('v', 'a'): 642,\n ('&lt;S&gt;', 'i'): 591,\n ('i', 's'): 1316,\n ('s', 'a'): 1201,\n ('a', 'b'): 541,\n ('b', 'e'): 655,\n ('e', 'l'): 3248,\n ('l', 'l'): 1345,\n ('l', 'a'): 2623,\n ('&lt;S&gt;', 's'): 2055,\n ('s', 'o'): 531,\n ('o', 'p'): 95,\n ('p', 'h'): 204,\n ('h', 'i'): 729,\n ('&lt;S&gt;', 'c'): 1542,\n ('c', 'h'): 664,\n ('h', 'a'): 2244,\n ('a', 'r'): 3264,\n ('r', 'l'): 413,\n ('l', 'o'): 692,\n ('o', 't'): 118,\n ('t', 't'): 374,\n ('t', 'e'): 716,\n ('e', '&lt;E&gt;'): 3983,\n ('&lt;S&gt;', 'm'): 2538,\n ('m', 'i'): 1256,\n ('a', 'm'): 1634,\n ('m', 'e'): 818,\n ('&lt;S&gt;', 'h'): 874,\n ('r', 'p'): 14,\n ('p', 'e'): 197,\n ('e', 'r'): 1958,\n ('r', '&lt;E&gt;'): 1377,\n ('e', 'v'): 463,\n ('v', 'e'): 568,\n ('l', 'y'): 1588,\n ('y', 'n'): 1826,\n ('n', '&lt;E&gt;'): 6763,\n ('b', 'i'): 217,\n ('i', 'g'): 428,\n ('g', 'a'): 330,\n ('a', 'i'): 1650,\n ('i', 'l'): 1345,\n ('l', '&lt;E&gt;'): 1314,\n ('y', '&lt;E&gt;'): 2007,\n ('i', 'z'): 277,\n ('z', 'a'): 860,\n ('e', 't'): 580,\n ('t', 'h'): 647,\n ('h', '&lt;E&gt;'): 2409,\n ('r', 'y'): 773,\n ('o', 'f'): 34,\n ('f', 'i'): 160,\n ('c', 'a'): 815,\n ('r', 'i'): 3033,\n ('s', 'c'): 60,\n ('l', 'e'): 2921,\n ('t', '&lt;E&gt;'): 483,\n ('&lt;S&gt;', 'v'): 376,\n ('i', 'c'): 509,\n ('c', 't'): 35,\n ('t', 'o'): 667,\n ('o', 'r'): 1059,\n ('a', 'd'): 1042,\n ('d', 'i'): 674,\n ('o', 'n'): 2411,\n ('&lt;S&gt;', 'l'): 1572,\n ('l', 'u'): 324,\n ('u', 'n'): 275,\n ('n', 'a'): 2977,\n ('&lt;S&gt;', 'g'): 669,\n ('g', 'r'): 201,\n ('r', 'a'): 2356,\n ('a', 'c'): 470,\n ('c', 'e'): 551,\n ('h', 'l'): 185,\n ('o', 'e'): 132,\n ('&lt;S&gt;', 'p'): 515,\n ('e', 'n'): 2675,\n ('n', 'e'): 1359,\n ('a', 'y'): 2050,\n ('y', 'l'): 1104,\n ('&lt;S&gt;', 'r'): 1639,\n ('e', 'y'): 1070,\n ('&lt;S&gt;', 'z'): 929,\n ('z', 'o'): 110,\n ('&lt;S&gt;', 'n'): 1146,\n ('n', 'o'): 496,\n ('e', 'a'): 679,\n ('a', 'n'): 5438,\n ('n', 'n'): 1906,\n ('a', 'h'): 2332,\n ('d', 'd'): 149,\n ('a', 'u'): 381,\n ('u', 'b'): 103,\n ('b', 'r'): 842,\n ('r', 'e'): 1697,\n ('i', 'e'): 1653,\n ('s', 't'): 765,\n ('a', 't'): 687,\n ('t', 'a'): 1027,\n ('a', 'l'): 2528,\n ('a', 'z'): 435,\n ('z', 'e'): 373,\n ('i', 'o'): 588,\n ('u', 'r'): 414,\n ('r', 'o'): 869,\n ('u', 'd'): 136,\n ('d', 'r'): 424,\n ('&lt;S&gt;', 'b'): 1306,\n ('o', 'o'): 115,\n ('o', 'k'): 68,\n ('k', 'l'): 139,\n ('c', 'l'): 116,\n ('i', 'r'): 849,\n ('s', 'k'): 82,\n ('k', 'y'): 379,\n ('u', 'c'): 103,\n ('c', 'y'): 104,\n ('p', 'a'): 209,\n ('s', 'l'): 279,\n ('i', 'n'): 2126,\n ('o', 'v'): 176,\n ('g', 'e'): 334,\n ('e', 's'): 861,\n ('s', 'i'): 684,\n ('s', '&lt;E&gt;'): 1169,\n ('&lt;S&gt;', 'k'): 2963,\n ('k', 'e'): 895,\n ('e', 'd'): 384,\n ('d', 'y'): 317,\n ('n', 't'): 443,\n ('y', 'a'): 2143,\n ('&lt;S&gt;', 'w'): 307,\n ('w', 'i'): 148,\n ('o', 'w'): 114,\n ('w', '&lt;E&gt;'): 51,\n ('k', 'i'): 509,\n ('n', 's'): 278,\n ('a', 'o'): 63,\n ('o', 'm'): 261,\n ('i', '&lt;E&gt;'): 2489,\n ('a', 'a'): 556,\n ('i', 'y'): 779,\n ('d', 'e'): 1283,\n ('c', 'o'): 380,\n ('r', 'u'): 252,\n ('b', 'y'): 83,\n ('s', 'e'): 884,\n ('n', 'i'): 1725,\n ('i', 't'): 541,\n ('t', 'y'): 341,\n ('u', 't'): 82,\n ('t', 'u'): 78,\n ('u', 'm'): 154,\n ('m', 'n'): 20,\n ('g', 'i'): 190,\n ('t', 'i'): 532,\n ('&lt;S&gt;', 'q'): 92,\n ('q', 'u'): 206,\n ('u', 'i'): 121,\n ('a', 'e'): 692,\n ('e', 'h'): 152,\n ('v', 'y'): 121,\n ('p', 'i'): 61,\n ('i', 'p'): 53,\n ('y', 'd'): 272,\n ('e', 'x'): 132,\n ('x', 'a'): 103,\n ('&lt;S&gt;', 'j'): 2422,\n ('j', 'o'): 479,\n ('o', 's'): 504,\n ('e', 'p'): 83,\n ('j', 'u'): 202,\n ('u', 'l'): 301,\n ('&lt;S&gt;', 'd'): 1690,\n ('k', 'a'): 1731,\n ('e', 'e'): 1271,\n ('y', 't'): 104,\n ('d', 'l'): 60,\n ('c', 'k'): 316,\n ('n', 'z'): 145,\n ('z', 'i'): 364,\n ('a', 'g'): 168,\n ('d', 'a'): 1303,\n ('j', 'a'): 1473,\n ('h', 'e'): 674,\n ('&lt;S&gt;', 'x'): 134,\n ('x', 'i'): 102,\n ('i', 'm'): 427,\n ('e', 'i'): 818,\n ('&lt;S&gt;', 't'): 1308,\n ('&lt;S&gt;', 'f'): 417,\n ('f', 'a'): 242,\n ('n', 'd'): 704,\n ('r', 'g'): 76,\n ('a', 's'): 1118,\n ('s', 'h'): 1285,\n ('b', 'a'): 321,\n ('k', 'h'): 307,\n ('s', 'm'): 90,\n ('o', 'd'): 190,\n ('r', 's'): 190,\n ('g', 'h'): 360,\n ('s', 'y'): 215,\n ('y', 's'): 401,\n ('s', 's'): 461,\n ('e', 'c'): 153,\n ('c', 'i'): 271,\n ('m', 'o'): 452,\n ('r', 'k'): 90,\n ('n', 'l'): 195,\n ('d', 'n'): 31,\n ('r', 'd'): 187,\n ('o', 'i'): 69,\n ('t', 'r'): 352,\n ('m', 'b'): 112,\n ('r', 'm'): 162,\n ('n', 'y'): 465,\n ('d', 'o'): 378,\n ('o', 'a'): 149,\n ('o', 'c'): 114,\n ('m', 'y'): 287,\n ('s', 'u'): 185,\n ('m', 'c'): 51,\n ('p', 'r'): 151,\n ('o', 'u'): 275,\n ('r', 'n'): 140,\n ('w', 'a'): 280,\n ('e', 'b'): 121,\n ('c', 'c'): 42,\n ('a', 'w'): 161,\n ('w', 'y'): 73,\n ('y', 'e'): 301,\n ('e', 'o'): 269,\n ('a', 'k'): 568,\n ('n', 'g'): 273,\n ('k', 'o'): 344,\n ('b', 'l'): 103,\n ('h', 'o'): 287,\n ('e', 'g'): 125,\n ('f', 'r'): 114,\n ('s', 'p'): 51,\n ('l', 's'): 94,\n ('y', 'z'): 78,\n ('g', 'g'): 25,\n ('z', 'u'): 73,\n ('i', 'd'): 440,\n ('m', '&lt;E&gt;'): 516,\n ('o', 'g'): 44,\n ('j', 'e'): 440,\n ('g', 'n'): 27,\n ('y', 'r'): 291,\n ('c', '&lt;E&gt;'): 97,\n ('c', 'q'): 11,\n ('u', 'e'): 169,\n ('i', 'f'): 101,\n ('f', 'e'): 123,\n ('i', 'x'): 89,\n ('x', '&lt;E&gt;'): 164,\n ('o', 'y'): 103,\n ('g', 'o'): 83,\n ('g', 't'): 31,\n ('l', 't'): 77,\n ('g', 'w'): 26,\n ('w', 'e'): 149,\n ('l', 'd'): 138,\n ('a', 'p'): 82,\n ('h', 'n'): 138,\n ('t', 'l'): 134,\n ('m', 'r'): 97,\n ('n', 'c'): 213,\n ('l', 'b'): 52,\n ('i', 'k'): 445,\n ('&lt;S&gt;', 'y'): 535,\n ('t', 'z'): 105,\n ('h', 'r'): 204,\n ('j', 'i'): 119,\n ('h', 't'): 71,\n ('r', 'r'): 425,\n ('z', 'l'): 123,\n ('w', 'r'): 22,\n ('b', 'b'): 38,\n ('r', 't'): 208,\n ('l', 'v'): 72,\n ('e', 'j'): 55,\n ('o', 'h'): 171,\n ('u', 's'): 474,\n ('i', 'b'): 110,\n ('g', 'l'): 32,\n ('h', 'y'): 213,\n ('p', 'o'): 59,\n ('p', 'p'): 39,\n ('p', 'y'): 12,\n ('n', 'r'): 44,\n ('z', 'm'): 35,\n ('v', 'o'): 153,\n ('l', 'm'): 60,\n ('o', 'x'): 45,\n ('d', '&lt;E&gt;'): 516,\n ('i', 'u'): 109,\n ('v', '&lt;E&gt;'): 88,\n ('f', 'f'): 44,\n ('b', 'o'): 105,\n ('e', 'k'): 178,\n ('c', 'r'): 76,\n ('d', 'g'): 25,\n ('r', 'c'): 99,\n ('r', 'h'): 121,\n ('n', 'k'): 58,\n ('h', 'u'): 166,\n ('d', 's'): 29,\n ('a', 'x'): 182,\n ('y', 'c'): 115,\n ('e', 'w'): 50,\n ('v', 'k'): 3,\n ('z', 'h'): 43,\n ('w', 'h'): 23,\n ('t', 'n'): 22,\n ('x', 'l'): 39,\n ('g', 'u'): 85,\n ('u', 'a'): 163,\n ('u', 'p'): 16,\n ('u', 'g'): 47,\n ('d', 'u'): 92,\n ('l', 'c'): 25,\n ('r', 'b'): 41,\n ('a', 'q'): 60,\n ('b', '&lt;E&gt;'): 114,\n ('g', 'y'): 31,\n ('y', 'p'): 15,\n ('p', 't'): 17,\n ('e', 'z'): 181,\n ('z', 'r'): 32,\n ('f', 'l'): 20,\n ('o', '&lt;E&gt;'): 855,\n ('o', 'b'): 140,\n ('u', 'z'): 45,\n ('z', '&lt;E&gt;'): 160,\n ('i', 'q'): 52,\n ('y', 'v'): 106,\n ('n', 'v'): 55,\n ('d', 'h'): 118,\n ('g', 'd'): 19,\n ('t', 's'): 35,\n ('n', 'h'): 26,\n ('y', 'j'): 23,\n ('k', 'r'): 109,\n ('z', 'b'): 4,\n ('g', '&lt;E&gt;'): 108,\n ('a', 'j'): 175,\n ('r', 'j'): 25,\n ('m', 'p'): 38,\n ('p', 'b'): 2,\n ('y', 'o'): 271,\n ('z', 'y'): 147,\n ('p', 'l'): 16,\n ('l', 'k'): 24,\n ('i', 'j'): 76,\n ('x', 'e'): 36,\n ('y', 'u'): 141,\n ('l', 'n'): 14,\n ('u', 'x'): 34,\n ('i', 'h'): 95,\n ('w', 's'): 20,\n ('k', 's'): 95,\n ('m', 'u'): 139,\n ('y', 'k'): 86,\n ('e', 'f'): 82,\n ('k', '&lt;E&gt;'): 363,\n ('y', 'm'): 148,\n ('z', 'z'): 45,\n ('m', 'd'): 24,\n ('s', 'r'): 55,\n ('e', 'u'): 69,\n ('l', 'h'): 19,\n ('a', 'f'): 134,\n ('r', 'w'): 21,\n ('n', 'u'): 96,\n ('v', 'r'): 48,\n ('m', 's'): 35,\n ('&lt;S&gt;', 'u'): 78,\n ('f', 's'): 6,\n ('y', 'b'): 27,\n ('x', 'o'): 41,\n ('g', 's'): 30,\n ('x', 'y'): 30,\n ('w', 'n'): 58,\n ('j', 'h'): 45,\n ('f', 'n'): 4,\n ('n', 'j'): 44,\n ('r', 'v'): 80,\n ('n', 'm'): 19,\n ('t', 'c'): 17,\n ('s', 'w'): 24,\n ('k', 't'): 17,\n ('f', 't'): 18,\n ('x', 't'): 70,\n ('u', 'v'): 37,\n ('k', 'k'): 20,\n ('s', 'n'): 24,\n ('u', '&lt;E&gt;'): 155,\n ('j', 'r'): 11,\n ('y', 'x'): 28,\n ('h', 'm'): 117,\n ('e', 'q'): 14,\n ('u', 'o'): 10,\n ('f', '&lt;E&gt;'): 80,\n ('h', 'z'): 20,\n ('h', 'k'): 29,\n ('y', 'g'): 30,\n ('q', 'r'): 1,\n ('v', 'n'): 8,\n ('s', 'd'): 9,\n ('y', 'i'): 192,\n ('n', 'w'): 11,\n ('d', 'v'): 17,\n ('h', 'v'): 39,\n ('x', 'w'): 3,\n ('o', 'z'): 54,\n ('k', 'u'): 50,\n ('u', 'h'): 58,\n ('k', 'n'): 26,\n ('s', 'b'): 21,\n ('i', 'i'): 82,\n ('y', 'y'): 23,\n ('r', 'z'): 23,\n ('l', 'g'): 6,\n ('l', 'p'): 15,\n ('p', '&lt;E&gt;'): 33,\n ('b', 'u'): 45,\n ('f', 'u'): 10,\n ('b', 'h'): 41,\n ('f', 'y'): 14,\n ('u', 'w'): 86,\n ('x', 'u'): 5,\n ('q', '&lt;E&gt;'): 28,\n ('l', 'r'): 18,\n ('m', 'h'): 5,\n ('l', 'w'): 16,\n ('j', '&lt;E&gt;'): 71,\n ('s', 'v'): 14,\n ('m', 'l'): 5,\n ('n', 'f'): 11,\n ('u', 'j'): 14,\n ('f', 'o'): 60,\n ('j', 'l'): 9,\n ('t', 'g'): 2,\n ('j', 'm'): 5,\n ('v', 'v'): 7,\n ('p', 's'): 16,\n ('t', 'w'): 11,\n ('x', 'c'): 4,\n ('u', 'k'): 93,\n ('v', 'l'): 14,\n ('h', 'd'): 24,\n ('l', 'z'): 10,\n ('k', 'w'): 34,\n ('n', 'b'): 8,\n ('q', 's'): 2,\n ('i', 'w'): 8,\n ('c', 's'): 5,\n ('h', 's'): 31,\n ('m', 't'): 4,\n ('h', 'w'): 10,\n ('x', 'x'): 38,\n ('t', 'x'): 2,\n ('d', 'z'): 1,\n ('x', 'z'): 19,\n ('t', 'm'): 4,\n ('t', 'j'): 3,\n ('u', 'q'): 10,\n ('q', 'a'): 13,\n ('f', 'k'): 2,\n ('z', 'n'): 4,\n ('l', 'j'): 6,\n ('j', 'w'): 6,\n ('v', 'u'): 7,\n ('c', 'j'): 3,\n ('h', 'b'): 8,\n ('z', 't'): 4,\n ('p', 'u'): 4,\n ('m', 'z'): 11,\n ('x', 's'): 31,\n ('b', 't'): 2,\n ('u', 'y'): 13,\n ('d', 'j'): 9,\n ('j', 's'): 7,\n ('w', 'u'): 25,\n ('o', 'j'): 16,\n ('b', 's'): 8,\n ('d', 'w'): 23,\n ('w', 'o'): 36,\n ('j', 'n'): 2,\n ('w', 't'): 8,\n ('l', 'f'): 22,\n ('d', 'm'): 30,\n ('p', 'j'): 1,\n ('j', 'y'): 10,\n ('y', 'f'): 12,\n ('q', 'i'): 13,\n ('j', 'v'): 5,\n ('q', 'l'): 1,\n ('s', 'z'): 10,\n ('k', 'm'): 9,\n ('w', 'l'): 13,\n ('p', 'f'): 1,\n ('q', 'w'): 3,\n ('n', 'x'): 6,\n ('k', 'c'): 2,\n ('t', 'v'): 15,\n ('c', 'u'): 35,\n ('z', 'k'): 2,\n ('c', 'z'): 4,\n ('y', 'q'): 6,\n ('y', 'h'): 22,\n ('r', 'f'): 9,\n ('s', 'j'): 2,\n ('h', 'j'): 9,\n ('g', 'b'): 3,\n ('u', 'f'): 19,\n ('s', 'f'): 2,\n ('q', 'e'): 1,\n ('b', 'c'): 1,\n ('c', 'd'): 1,\n ('z', 'j'): 2,\n ('n', 'q'): 2,\n ('m', 'f'): 1,\n ('p', 'n'): 1,\n ('f', 'z'): 2,\n ('b', 'n'): 4,\n ('w', 'd'): 8,\n ('w', 'b'): 1,\n ('b', 'd'): 65,\n ('z', 's'): 4,\n ('p', 'c'): 1,\n ('h', 'g'): 2,\n ('m', 'j'): 7,\n ('w', 'w'): 2,\n ('k', 'j'): 2,\n ('h', 'p'): 1,\n ('j', 'k'): 2,\n ('o', 'q'): 3,\n ('f', 'w'): 4,\n ('f', 'h'): 1,\n ('w', 'm'): 2,\n ('b', 'j'): 1,\n ('r', 'q'): 16,\n ('z', 'c'): 2,\n ('z', 'v'): 2,\n ('f', 'g'): 1,\n ('n', 'p'): 5,\n ('z', 'g'): 1,\n ('d', 't'): 4,\n ('w', 'f'): 2,\n ('d', 'f'): 5,\n ('w', 'k'): 6,\n ('q', 'm'): 2,\n ('k', 'z'): 2,\n ('j', 'j'): 2,\n ('c', 'p'): 1,\n ('p', 'k'): 1,\n ('p', 'm'): 1,\n ('j', 'd'): 4,\n ('r', 'x'): 3,\n ('x', 'n'): 1,\n ('d', 'c'): 3,\n ('g', 'j'): 3,\n ('x', 'f'): 3,\n ('j', 'c'): 4,\n ('s', 'q'): 1,\n ('k', 'f'): 1,\n ('z', 'p'): 2,\n ('j', 't'): 2,\n ('k', 'b'): 2,\n ('m', 'k'): 1,\n ('m', 'w'): 2,\n ('x', 'h'): 1,\n ('h', 'f'): 2,\n ('x', 'd'): 5,\n ('y', 'w'): 4,\n ('z', 'w'): 3,\n ('d', 'k'): 3,\n ('c', 'g'): 2,\n ('u', 'u'): 3,\n ('t', 'f'): 2,\n ('g', 'm'): 6,\n ('m', 'v'): 3,\n ('c', 'x'): 3,\n ('h', 'c'): 2,\n ('g', 'f'): 1,\n ('q', 'o'): 2,\n ('l', 'q'): 3,\n ('v', 'b'): 1,\n ('j', 'p'): 1,\n ('k', 'd'): 2,\n ('g', 'z'): 1,\n ('v', 'd'): 1,\n ('d', 'b'): 1,\n ('v', 'h'): 1,\n ('k', 'v'): 2,\n ('h', 'h'): 1,\n ('s', 'g'): 2,\n ('g', 'v'): 1,\n ('d', 'q'): 1,\n ('x', 'b'): 1,\n ('w', 'z'): 1,\n ('h', 'q'): 1,\n ('j', 'b'): 1,\n ('z', 'd'): 2,\n ('x', 'm'): 1,\n ('w', 'g'): 1,\n ('t', 'b'): 1,\n ('z', 'x'): 1}</pre> In\u00a0[33]: Copied! <pre>b.items()   #We know 'b' is a dictionary. So .items() basically gives us that it's values in a (Key, Value) set\n</pre> b.items()   #We know 'b' is a dictionary. So .items() basically gives us that it's values in a (Key, Value) set Out[33]: <pre>dict_items([(('&lt;S&gt;', 'e'), 1531), (('e', 'm'), 769), (('m', 'm'), 168), (('m', 'a'), 2590), (('a', '&lt;E&gt;'), 6640), (('&lt;S&gt;', 'o'), 394), (('o', 'l'), 619), (('l', 'i'), 2480), (('i', 'v'), 269), (('v', 'i'), 911), (('i', 'a'), 2445), (('&lt;S&gt;', 'a'), 4410), (('a', 'v'), 834), (('v', 'a'), 642), (('&lt;S&gt;', 'i'), 591), (('i', 's'), 1316), (('s', 'a'), 1201), (('a', 'b'), 541), (('b', 'e'), 655), (('e', 'l'), 3248), (('l', 'l'), 1345), (('l', 'a'), 2623), (('&lt;S&gt;', 's'), 2055), (('s', 'o'), 531), (('o', 'p'), 95), (('p', 'h'), 204), (('h', 'i'), 729), (('&lt;S&gt;', 'c'), 1542), (('c', 'h'), 664), (('h', 'a'), 2244), (('a', 'r'), 3264), (('r', 'l'), 413), (('l', 'o'), 692), (('o', 't'), 118), (('t', 't'), 374), (('t', 'e'), 716), (('e', '&lt;E&gt;'), 3983), (('&lt;S&gt;', 'm'), 2538), (('m', 'i'), 1256), (('a', 'm'), 1634), (('m', 'e'), 818), (('&lt;S&gt;', 'h'), 874), (('r', 'p'), 14), (('p', 'e'), 197), (('e', 'r'), 1958), (('r', '&lt;E&gt;'), 1377), (('e', 'v'), 463), (('v', 'e'), 568), (('l', 'y'), 1588), (('y', 'n'), 1826), (('n', '&lt;E&gt;'), 6763), (('b', 'i'), 217), (('i', 'g'), 428), (('g', 'a'), 330), (('a', 'i'), 1650), (('i', 'l'), 1345), (('l', '&lt;E&gt;'), 1314), (('y', '&lt;E&gt;'), 2007), (('i', 'z'), 277), (('z', 'a'), 860), (('e', 't'), 580), (('t', 'h'), 647), (('h', '&lt;E&gt;'), 2409), (('r', 'y'), 773), (('o', 'f'), 34), (('f', 'i'), 160), (('c', 'a'), 815), (('r', 'i'), 3033), (('s', 'c'), 60), (('l', 'e'), 2921), (('t', '&lt;E&gt;'), 483), (('&lt;S&gt;', 'v'), 376), (('i', 'c'), 509), (('c', 't'), 35), (('t', 'o'), 667), (('o', 'r'), 1059), (('a', 'd'), 1042), (('d', 'i'), 674), (('o', 'n'), 2411), (('&lt;S&gt;', 'l'), 1572), (('l', 'u'), 324), (('u', 'n'), 275), (('n', 'a'), 2977), (('&lt;S&gt;', 'g'), 669), (('g', 'r'), 201), (('r', 'a'), 2356), (('a', 'c'), 470), (('c', 'e'), 551), (('h', 'l'), 185), (('o', 'e'), 132), (('&lt;S&gt;', 'p'), 515), (('e', 'n'), 2675), (('n', 'e'), 1359), (('a', 'y'), 2050), (('y', 'l'), 1104), (('&lt;S&gt;', 'r'), 1639), (('e', 'y'), 1070), (('&lt;S&gt;', 'z'), 929), (('z', 'o'), 110), (('&lt;S&gt;', 'n'), 1146), (('n', 'o'), 496), (('e', 'a'), 679), (('a', 'n'), 5438), (('n', 'n'), 1906), (('a', 'h'), 2332), (('d', 'd'), 149), (('a', 'u'), 381), (('u', 'b'), 103), (('b', 'r'), 842), (('r', 'e'), 1697), (('i', 'e'), 1653), (('s', 't'), 765), (('a', 't'), 687), (('t', 'a'), 1027), (('a', 'l'), 2528), (('a', 'z'), 435), (('z', 'e'), 373), (('i', 'o'), 588), (('u', 'r'), 414), (('r', 'o'), 869), (('u', 'd'), 136), (('d', 'r'), 424), (('&lt;S&gt;', 'b'), 1306), (('o', 'o'), 115), (('o', 'k'), 68), (('k', 'l'), 139), (('c', 'l'), 116), (('i', 'r'), 849), (('s', 'k'), 82), (('k', 'y'), 379), (('u', 'c'), 103), (('c', 'y'), 104), (('p', 'a'), 209), (('s', 'l'), 279), (('i', 'n'), 2126), (('o', 'v'), 176), (('g', 'e'), 334), (('e', 's'), 861), (('s', 'i'), 684), (('s', '&lt;E&gt;'), 1169), (('&lt;S&gt;', 'k'), 2963), (('k', 'e'), 895), (('e', 'd'), 384), (('d', 'y'), 317), (('n', 't'), 443), (('y', 'a'), 2143), (('&lt;S&gt;', 'w'), 307), (('w', 'i'), 148), (('o', 'w'), 114), (('w', '&lt;E&gt;'), 51), (('k', 'i'), 509), (('n', 's'), 278), (('a', 'o'), 63), (('o', 'm'), 261), (('i', '&lt;E&gt;'), 2489), (('a', 'a'), 556), (('i', 'y'), 779), (('d', 'e'), 1283), (('c', 'o'), 380), (('r', 'u'), 252), (('b', 'y'), 83), (('s', 'e'), 884), (('n', 'i'), 1725), (('i', 't'), 541), (('t', 'y'), 341), (('u', 't'), 82), (('t', 'u'), 78), (('u', 'm'), 154), (('m', 'n'), 20), (('g', 'i'), 190), (('t', 'i'), 532), (('&lt;S&gt;', 'q'), 92), (('q', 'u'), 206), (('u', 'i'), 121), (('a', 'e'), 692), (('e', 'h'), 152), (('v', 'y'), 121), (('p', 'i'), 61), (('i', 'p'), 53), (('y', 'd'), 272), (('e', 'x'), 132), (('x', 'a'), 103), (('&lt;S&gt;', 'j'), 2422), (('j', 'o'), 479), (('o', 's'), 504), (('e', 'p'), 83), (('j', 'u'), 202), (('u', 'l'), 301), (('&lt;S&gt;', 'd'), 1690), (('k', 'a'), 1731), (('e', 'e'), 1271), (('y', 't'), 104), (('d', 'l'), 60), (('c', 'k'), 316), (('n', 'z'), 145), (('z', 'i'), 364), (('a', 'g'), 168), (('d', 'a'), 1303), (('j', 'a'), 1473), (('h', 'e'), 674), (('&lt;S&gt;', 'x'), 134), (('x', 'i'), 102), (('i', 'm'), 427), (('e', 'i'), 818), (('&lt;S&gt;', 't'), 1308), (('&lt;S&gt;', 'f'), 417), (('f', 'a'), 242), (('n', 'd'), 704), (('r', 'g'), 76), (('a', 's'), 1118), (('s', 'h'), 1285), (('b', 'a'), 321), (('k', 'h'), 307), (('s', 'm'), 90), (('o', 'd'), 190), (('r', 's'), 190), (('g', 'h'), 360), (('s', 'y'), 215), (('y', 's'), 401), (('s', 's'), 461), (('e', 'c'), 153), (('c', 'i'), 271), (('m', 'o'), 452), (('r', 'k'), 90), (('n', 'l'), 195), (('d', 'n'), 31), (('r', 'd'), 187), (('o', 'i'), 69), (('t', 'r'), 352), (('m', 'b'), 112), (('r', 'm'), 162), (('n', 'y'), 465), (('d', 'o'), 378), (('o', 'a'), 149), (('o', 'c'), 114), (('m', 'y'), 287), (('s', 'u'), 185), (('m', 'c'), 51), (('p', 'r'), 151), (('o', 'u'), 275), (('r', 'n'), 140), (('w', 'a'), 280), (('e', 'b'), 121), (('c', 'c'), 42), (('a', 'w'), 161), (('w', 'y'), 73), (('y', 'e'), 301), (('e', 'o'), 269), (('a', 'k'), 568), (('n', 'g'), 273), (('k', 'o'), 344), (('b', 'l'), 103), (('h', 'o'), 287), (('e', 'g'), 125), (('f', 'r'), 114), (('s', 'p'), 51), (('l', 's'), 94), (('y', 'z'), 78), (('g', 'g'), 25), (('z', 'u'), 73), (('i', 'd'), 440), (('m', '&lt;E&gt;'), 516), (('o', 'g'), 44), (('j', 'e'), 440), (('g', 'n'), 27), (('y', 'r'), 291), (('c', '&lt;E&gt;'), 97), (('c', 'q'), 11), (('u', 'e'), 169), (('i', 'f'), 101), (('f', 'e'), 123), (('i', 'x'), 89), (('x', '&lt;E&gt;'), 164), (('o', 'y'), 103), (('g', 'o'), 83), (('g', 't'), 31), (('l', 't'), 77), (('g', 'w'), 26), (('w', 'e'), 149), (('l', 'd'), 138), (('a', 'p'), 82), (('h', 'n'), 138), (('t', 'l'), 134), (('m', 'r'), 97), (('n', 'c'), 213), (('l', 'b'), 52), (('i', 'k'), 445), (('&lt;S&gt;', 'y'), 535), (('t', 'z'), 105), (('h', 'r'), 204), (('j', 'i'), 119), (('h', 't'), 71), (('r', 'r'), 425), (('z', 'l'), 123), (('w', 'r'), 22), (('b', 'b'), 38), (('r', 't'), 208), (('l', 'v'), 72), (('e', 'j'), 55), (('o', 'h'), 171), (('u', 's'), 474), (('i', 'b'), 110), (('g', 'l'), 32), (('h', 'y'), 213), (('p', 'o'), 59), (('p', 'p'), 39), (('p', 'y'), 12), (('n', 'r'), 44), (('z', 'm'), 35), (('v', 'o'), 153), (('l', 'm'), 60), (('o', 'x'), 45), (('d', '&lt;E&gt;'), 516), (('i', 'u'), 109), (('v', '&lt;E&gt;'), 88), (('f', 'f'), 44), (('b', 'o'), 105), (('e', 'k'), 178), (('c', 'r'), 76), (('d', 'g'), 25), (('r', 'c'), 99), (('r', 'h'), 121), (('n', 'k'), 58), (('h', 'u'), 166), (('d', 's'), 29), (('a', 'x'), 182), (('y', 'c'), 115), (('e', 'w'), 50), (('v', 'k'), 3), (('z', 'h'), 43), (('w', 'h'), 23), (('t', 'n'), 22), (('x', 'l'), 39), (('g', 'u'), 85), (('u', 'a'), 163), (('u', 'p'), 16), (('u', 'g'), 47), (('d', 'u'), 92), (('l', 'c'), 25), (('r', 'b'), 41), (('a', 'q'), 60), (('b', '&lt;E&gt;'), 114), (('g', 'y'), 31), (('y', 'p'), 15), (('p', 't'), 17), (('e', 'z'), 181), (('z', 'r'), 32), (('f', 'l'), 20), (('o', '&lt;E&gt;'), 855), (('o', 'b'), 140), (('u', 'z'), 45), (('z', '&lt;E&gt;'), 160), (('i', 'q'), 52), (('y', 'v'), 106), (('n', 'v'), 55), (('d', 'h'), 118), (('g', 'd'), 19), (('t', 's'), 35), (('n', 'h'), 26), (('y', 'j'), 23), (('k', 'r'), 109), (('z', 'b'), 4), (('g', '&lt;E&gt;'), 108), (('a', 'j'), 175), (('r', 'j'), 25), (('m', 'p'), 38), (('p', 'b'), 2), (('y', 'o'), 271), (('z', 'y'), 147), (('p', 'l'), 16), (('l', 'k'), 24), (('i', 'j'), 76), (('x', 'e'), 36), (('y', 'u'), 141), (('l', 'n'), 14), (('u', 'x'), 34), (('i', 'h'), 95), (('w', 's'), 20), (('k', 's'), 95), (('m', 'u'), 139), (('y', 'k'), 86), (('e', 'f'), 82), (('k', '&lt;E&gt;'), 363), (('y', 'm'), 148), (('z', 'z'), 45), (('m', 'd'), 24), (('s', 'r'), 55), (('e', 'u'), 69), (('l', 'h'), 19), (('a', 'f'), 134), (('r', 'w'), 21), (('n', 'u'), 96), (('v', 'r'), 48), (('m', 's'), 35), (('&lt;S&gt;', 'u'), 78), (('f', 's'), 6), (('y', 'b'), 27), (('x', 'o'), 41), (('g', 's'), 30), (('x', 'y'), 30), (('w', 'n'), 58), (('j', 'h'), 45), (('f', 'n'), 4), (('n', 'j'), 44), (('r', 'v'), 80), (('n', 'm'), 19), (('t', 'c'), 17), (('s', 'w'), 24), (('k', 't'), 17), (('f', 't'), 18), (('x', 't'), 70), (('u', 'v'), 37), (('k', 'k'), 20), (('s', 'n'), 24), (('u', '&lt;E&gt;'), 155), (('j', 'r'), 11), (('y', 'x'), 28), (('h', 'm'), 117), (('e', 'q'), 14), (('u', 'o'), 10), (('f', '&lt;E&gt;'), 80), (('h', 'z'), 20), (('h', 'k'), 29), (('y', 'g'), 30), (('q', 'r'), 1), (('v', 'n'), 8), (('s', 'd'), 9), (('y', 'i'), 192), (('n', 'w'), 11), (('d', 'v'), 17), (('h', 'v'), 39), (('x', 'w'), 3), (('o', 'z'), 54), (('k', 'u'), 50), (('u', 'h'), 58), (('k', 'n'), 26), (('s', 'b'), 21), (('i', 'i'), 82), (('y', 'y'), 23), (('r', 'z'), 23), (('l', 'g'), 6), (('l', 'p'), 15), (('p', '&lt;E&gt;'), 33), (('b', 'u'), 45), (('f', 'u'), 10), (('b', 'h'), 41), (('f', 'y'), 14), (('u', 'w'), 86), (('x', 'u'), 5), (('q', '&lt;E&gt;'), 28), (('l', 'r'), 18), (('m', 'h'), 5), (('l', 'w'), 16), (('j', '&lt;E&gt;'), 71), (('s', 'v'), 14), (('m', 'l'), 5), (('n', 'f'), 11), (('u', 'j'), 14), (('f', 'o'), 60), (('j', 'l'), 9), (('t', 'g'), 2), (('j', 'm'), 5), (('v', 'v'), 7), (('p', 's'), 16), (('t', 'w'), 11), (('x', 'c'), 4), (('u', 'k'), 93), (('v', 'l'), 14), (('h', 'd'), 24), (('l', 'z'), 10), (('k', 'w'), 34), (('n', 'b'), 8), (('q', 's'), 2), (('i', 'w'), 8), (('c', 's'), 5), (('h', 's'), 31), (('m', 't'), 4), (('h', 'w'), 10), (('x', 'x'), 38), (('t', 'x'), 2), (('d', 'z'), 1), (('x', 'z'), 19), (('t', 'm'), 4), (('t', 'j'), 3), (('u', 'q'), 10), (('q', 'a'), 13), (('f', 'k'), 2), (('z', 'n'), 4), (('l', 'j'), 6), (('j', 'w'), 6), (('v', 'u'), 7), (('c', 'j'), 3), (('h', 'b'), 8), (('z', 't'), 4), (('p', 'u'), 4), (('m', 'z'), 11), (('x', 's'), 31), (('b', 't'), 2), (('u', 'y'), 13), (('d', 'j'), 9), (('j', 's'), 7), (('w', 'u'), 25), (('o', 'j'), 16), (('b', 's'), 8), (('d', 'w'), 23), (('w', 'o'), 36), (('j', 'n'), 2), (('w', 't'), 8), (('l', 'f'), 22), (('d', 'm'), 30), (('p', 'j'), 1), (('j', 'y'), 10), (('y', 'f'), 12), (('q', 'i'), 13), (('j', 'v'), 5), (('q', 'l'), 1), (('s', 'z'), 10), (('k', 'm'), 9), (('w', 'l'), 13), (('p', 'f'), 1), (('q', 'w'), 3), (('n', 'x'), 6), (('k', 'c'), 2), (('t', 'v'), 15), (('c', 'u'), 35), (('z', 'k'), 2), (('c', 'z'), 4), (('y', 'q'), 6), (('y', 'h'), 22), (('r', 'f'), 9), (('s', 'j'), 2), (('h', 'j'), 9), (('g', 'b'), 3), (('u', 'f'), 19), (('s', 'f'), 2), (('q', 'e'), 1), (('b', 'c'), 1), (('c', 'd'), 1), (('z', 'j'), 2), (('n', 'q'), 2), (('m', 'f'), 1), (('p', 'n'), 1), (('f', 'z'), 2), (('b', 'n'), 4), (('w', 'd'), 8), (('w', 'b'), 1), (('b', 'd'), 65), (('z', 's'), 4), (('p', 'c'), 1), (('h', 'g'), 2), (('m', 'j'), 7), (('w', 'w'), 2), (('k', 'j'), 2), (('h', 'p'), 1), (('j', 'k'), 2), (('o', 'q'), 3), (('f', 'w'), 4), (('f', 'h'), 1), (('w', 'm'), 2), (('b', 'j'), 1), (('r', 'q'), 16), (('z', 'c'), 2), (('z', 'v'), 2), (('f', 'g'), 1), (('n', 'p'), 5), (('z', 'g'), 1), (('d', 't'), 4), (('w', 'f'), 2), (('d', 'f'), 5), (('w', 'k'), 6), (('q', 'm'), 2), (('k', 'z'), 2), (('j', 'j'), 2), (('c', 'p'), 1), (('p', 'k'), 1), (('p', 'm'), 1), (('j', 'd'), 4), (('r', 'x'), 3), (('x', 'n'), 1), (('d', 'c'), 3), (('g', 'j'), 3), (('x', 'f'), 3), (('j', 'c'), 4), (('s', 'q'), 1), (('k', 'f'), 1), (('z', 'p'), 2), (('j', 't'), 2), (('k', 'b'), 2), (('m', 'k'), 1), (('m', 'w'), 2), (('x', 'h'), 1), (('h', 'f'), 2), (('x', 'd'), 5), (('y', 'w'), 4), (('z', 'w'), 3), (('d', 'k'), 3), (('c', 'g'), 2), (('u', 'u'), 3), (('t', 'f'), 2), (('g', 'm'), 6), (('m', 'v'), 3), (('c', 'x'), 3), (('h', 'c'), 2), (('g', 'f'), 1), (('q', 'o'), 2), (('l', 'q'), 3), (('v', 'b'), 1), (('j', 'p'), 1), (('k', 'd'), 2), (('g', 'z'), 1), (('v', 'd'), 1), (('d', 'b'), 1), (('v', 'h'), 1), (('k', 'v'), 2), (('h', 'h'), 1), (('s', 'g'), 2), (('g', 'v'), 1), (('d', 'q'), 1), (('x', 'b'), 1), (('w', 'z'), 1), (('h', 'q'), 1), (('j', 'b'), 1), (('z', 'd'), 2), (('x', 'm'), 1), (('w', 'g'), 1), (('t', 'b'), 1), (('z', 'x'), 1)])</pre> In\u00a0[34]: Copied! <pre>sorted(b.items()) #Now this by default sorts the values based on the Key\n</pre> sorted(b.items()) #Now this by default sorts the values based on the Key Out[34]: <pre>[(('&lt;S&gt;', 'a'), 4410),\n (('&lt;S&gt;', 'b'), 1306),\n (('&lt;S&gt;', 'c'), 1542),\n (('&lt;S&gt;', 'd'), 1690),\n (('&lt;S&gt;', 'e'), 1531),\n (('&lt;S&gt;', 'f'), 417),\n (('&lt;S&gt;', 'g'), 669),\n (('&lt;S&gt;', 'h'), 874),\n (('&lt;S&gt;', 'i'), 591),\n (('&lt;S&gt;', 'j'), 2422),\n (('&lt;S&gt;', 'k'), 2963),\n (('&lt;S&gt;', 'l'), 1572),\n (('&lt;S&gt;', 'm'), 2538),\n (('&lt;S&gt;', 'n'), 1146),\n (('&lt;S&gt;', 'o'), 394),\n (('&lt;S&gt;', 'p'), 515),\n (('&lt;S&gt;', 'q'), 92),\n (('&lt;S&gt;', 'r'), 1639),\n (('&lt;S&gt;', 's'), 2055),\n (('&lt;S&gt;', 't'), 1308),\n (('&lt;S&gt;', 'u'), 78),\n (('&lt;S&gt;', 'v'), 376),\n (('&lt;S&gt;', 'w'), 307),\n (('&lt;S&gt;', 'x'), 134),\n (('&lt;S&gt;', 'y'), 535),\n (('&lt;S&gt;', 'z'), 929),\n (('a', '&lt;E&gt;'), 6640),\n (('a', 'a'), 556),\n (('a', 'b'), 541),\n (('a', 'c'), 470),\n (('a', 'd'), 1042),\n (('a', 'e'), 692),\n (('a', 'f'), 134),\n (('a', 'g'), 168),\n (('a', 'h'), 2332),\n (('a', 'i'), 1650),\n (('a', 'j'), 175),\n (('a', 'k'), 568),\n (('a', 'l'), 2528),\n (('a', 'm'), 1634),\n (('a', 'n'), 5438),\n (('a', 'o'), 63),\n (('a', 'p'), 82),\n (('a', 'q'), 60),\n (('a', 'r'), 3264),\n (('a', 's'), 1118),\n (('a', 't'), 687),\n (('a', 'u'), 381),\n (('a', 'v'), 834),\n (('a', 'w'), 161),\n (('a', 'x'), 182),\n (('a', 'y'), 2050),\n (('a', 'z'), 435),\n (('b', '&lt;E&gt;'), 114),\n (('b', 'a'), 321),\n (('b', 'b'), 38),\n (('b', 'c'), 1),\n (('b', 'd'), 65),\n (('b', 'e'), 655),\n (('b', 'h'), 41),\n (('b', 'i'), 217),\n (('b', 'j'), 1),\n (('b', 'l'), 103),\n (('b', 'n'), 4),\n (('b', 'o'), 105),\n (('b', 'r'), 842),\n (('b', 's'), 8),\n (('b', 't'), 2),\n (('b', 'u'), 45),\n (('b', 'y'), 83),\n (('c', '&lt;E&gt;'), 97),\n (('c', 'a'), 815),\n (('c', 'c'), 42),\n (('c', 'd'), 1),\n (('c', 'e'), 551),\n (('c', 'g'), 2),\n (('c', 'h'), 664),\n (('c', 'i'), 271),\n (('c', 'j'), 3),\n (('c', 'k'), 316),\n (('c', 'l'), 116),\n (('c', 'o'), 380),\n (('c', 'p'), 1),\n (('c', 'q'), 11),\n (('c', 'r'), 76),\n (('c', 's'), 5),\n (('c', 't'), 35),\n (('c', 'u'), 35),\n (('c', 'x'), 3),\n (('c', 'y'), 104),\n (('c', 'z'), 4),\n (('d', '&lt;E&gt;'), 516),\n (('d', 'a'), 1303),\n (('d', 'b'), 1),\n (('d', 'c'), 3),\n (('d', 'd'), 149),\n (('d', 'e'), 1283),\n (('d', 'f'), 5),\n (('d', 'g'), 25),\n (('d', 'h'), 118),\n (('d', 'i'), 674),\n (('d', 'j'), 9),\n (('d', 'k'), 3),\n (('d', 'l'), 60),\n (('d', 'm'), 30),\n (('d', 'n'), 31),\n (('d', 'o'), 378),\n (('d', 'q'), 1),\n (('d', 'r'), 424),\n (('d', 's'), 29),\n (('d', 't'), 4),\n (('d', 'u'), 92),\n (('d', 'v'), 17),\n (('d', 'w'), 23),\n (('d', 'y'), 317),\n (('d', 'z'), 1),\n (('e', '&lt;E&gt;'), 3983),\n (('e', 'a'), 679),\n (('e', 'b'), 121),\n (('e', 'c'), 153),\n (('e', 'd'), 384),\n (('e', 'e'), 1271),\n (('e', 'f'), 82),\n (('e', 'g'), 125),\n (('e', 'h'), 152),\n (('e', 'i'), 818),\n (('e', 'j'), 55),\n (('e', 'k'), 178),\n (('e', 'l'), 3248),\n (('e', 'm'), 769),\n (('e', 'n'), 2675),\n (('e', 'o'), 269),\n (('e', 'p'), 83),\n (('e', 'q'), 14),\n (('e', 'r'), 1958),\n (('e', 's'), 861),\n (('e', 't'), 580),\n (('e', 'u'), 69),\n (('e', 'v'), 463),\n (('e', 'w'), 50),\n (('e', 'x'), 132),\n (('e', 'y'), 1070),\n (('e', 'z'), 181),\n (('f', '&lt;E&gt;'), 80),\n (('f', 'a'), 242),\n (('f', 'e'), 123),\n (('f', 'f'), 44),\n (('f', 'g'), 1),\n (('f', 'h'), 1),\n (('f', 'i'), 160),\n (('f', 'k'), 2),\n (('f', 'l'), 20),\n (('f', 'n'), 4),\n (('f', 'o'), 60),\n (('f', 'r'), 114),\n (('f', 's'), 6),\n (('f', 't'), 18),\n (('f', 'u'), 10),\n (('f', 'w'), 4),\n (('f', 'y'), 14),\n (('f', 'z'), 2),\n (('g', '&lt;E&gt;'), 108),\n (('g', 'a'), 330),\n (('g', 'b'), 3),\n (('g', 'd'), 19),\n (('g', 'e'), 334),\n (('g', 'f'), 1),\n (('g', 'g'), 25),\n (('g', 'h'), 360),\n (('g', 'i'), 190),\n (('g', 'j'), 3),\n (('g', 'l'), 32),\n (('g', 'm'), 6),\n (('g', 'n'), 27),\n (('g', 'o'), 83),\n (('g', 'r'), 201),\n (('g', 's'), 30),\n (('g', 't'), 31),\n (('g', 'u'), 85),\n (('g', 'v'), 1),\n (('g', 'w'), 26),\n (('g', 'y'), 31),\n (('g', 'z'), 1),\n (('h', '&lt;E&gt;'), 2409),\n (('h', 'a'), 2244),\n (('h', 'b'), 8),\n (('h', 'c'), 2),\n (('h', 'd'), 24),\n (('h', 'e'), 674),\n (('h', 'f'), 2),\n (('h', 'g'), 2),\n (('h', 'h'), 1),\n (('h', 'i'), 729),\n (('h', 'j'), 9),\n (('h', 'k'), 29),\n (('h', 'l'), 185),\n (('h', 'm'), 117),\n (('h', 'n'), 138),\n (('h', 'o'), 287),\n (('h', 'p'), 1),\n (('h', 'q'), 1),\n (('h', 'r'), 204),\n (('h', 's'), 31),\n (('h', 't'), 71),\n (('h', 'u'), 166),\n (('h', 'v'), 39),\n (('h', 'w'), 10),\n (('h', 'y'), 213),\n (('h', 'z'), 20),\n (('i', '&lt;E&gt;'), 2489),\n (('i', 'a'), 2445),\n (('i', 'b'), 110),\n (('i', 'c'), 509),\n (('i', 'd'), 440),\n (('i', 'e'), 1653),\n (('i', 'f'), 101),\n (('i', 'g'), 428),\n (('i', 'h'), 95),\n (('i', 'i'), 82),\n (('i', 'j'), 76),\n (('i', 'k'), 445),\n (('i', 'l'), 1345),\n (('i', 'm'), 427),\n (('i', 'n'), 2126),\n (('i', 'o'), 588),\n (('i', 'p'), 53),\n (('i', 'q'), 52),\n (('i', 'r'), 849),\n (('i', 's'), 1316),\n (('i', 't'), 541),\n (('i', 'u'), 109),\n (('i', 'v'), 269),\n (('i', 'w'), 8),\n (('i', 'x'), 89),\n (('i', 'y'), 779),\n (('i', 'z'), 277),\n (('j', '&lt;E&gt;'), 71),\n (('j', 'a'), 1473),\n (('j', 'b'), 1),\n (('j', 'c'), 4),\n (('j', 'd'), 4),\n (('j', 'e'), 440),\n (('j', 'h'), 45),\n (('j', 'i'), 119),\n (('j', 'j'), 2),\n (('j', 'k'), 2),\n (('j', 'l'), 9),\n (('j', 'm'), 5),\n (('j', 'n'), 2),\n (('j', 'o'), 479),\n (('j', 'p'), 1),\n (('j', 'r'), 11),\n (('j', 's'), 7),\n (('j', 't'), 2),\n (('j', 'u'), 202),\n (('j', 'v'), 5),\n (('j', 'w'), 6),\n (('j', 'y'), 10),\n (('k', '&lt;E&gt;'), 363),\n (('k', 'a'), 1731),\n (('k', 'b'), 2),\n (('k', 'c'), 2),\n (('k', 'd'), 2),\n (('k', 'e'), 895),\n (('k', 'f'), 1),\n (('k', 'h'), 307),\n (('k', 'i'), 509),\n (('k', 'j'), 2),\n (('k', 'k'), 20),\n (('k', 'l'), 139),\n (('k', 'm'), 9),\n (('k', 'n'), 26),\n (('k', 'o'), 344),\n (('k', 'r'), 109),\n (('k', 's'), 95),\n (('k', 't'), 17),\n (('k', 'u'), 50),\n (('k', 'v'), 2),\n (('k', 'w'), 34),\n (('k', 'y'), 379),\n (('k', 'z'), 2),\n (('l', '&lt;E&gt;'), 1314),\n (('l', 'a'), 2623),\n (('l', 'b'), 52),\n (('l', 'c'), 25),\n (('l', 'd'), 138),\n (('l', 'e'), 2921),\n (('l', 'f'), 22),\n (('l', 'g'), 6),\n (('l', 'h'), 19),\n (('l', 'i'), 2480),\n (('l', 'j'), 6),\n (('l', 'k'), 24),\n (('l', 'l'), 1345),\n (('l', 'm'), 60),\n (('l', 'n'), 14),\n (('l', 'o'), 692),\n (('l', 'p'), 15),\n (('l', 'q'), 3),\n (('l', 'r'), 18),\n (('l', 's'), 94),\n (('l', 't'), 77),\n (('l', 'u'), 324),\n (('l', 'v'), 72),\n (('l', 'w'), 16),\n (('l', 'y'), 1588),\n (('l', 'z'), 10),\n (('m', '&lt;E&gt;'), 516),\n (('m', 'a'), 2590),\n (('m', 'b'), 112),\n (('m', 'c'), 51),\n (('m', 'd'), 24),\n (('m', 'e'), 818),\n (('m', 'f'), 1),\n (('m', 'h'), 5),\n (('m', 'i'), 1256),\n (('m', 'j'), 7),\n (('m', 'k'), 1),\n (('m', 'l'), 5),\n (('m', 'm'), 168),\n (('m', 'n'), 20),\n (('m', 'o'), 452),\n (('m', 'p'), 38),\n (('m', 'r'), 97),\n (('m', 's'), 35),\n (('m', 't'), 4),\n (('m', 'u'), 139),\n (('m', 'v'), 3),\n (('m', 'w'), 2),\n (('m', 'y'), 287),\n (('m', 'z'), 11),\n (('n', '&lt;E&gt;'), 6763),\n (('n', 'a'), 2977),\n (('n', 'b'), 8),\n (('n', 'c'), 213),\n (('n', 'd'), 704),\n (('n', 'e'), 1359),\n (('n', 'f'), 11),\n (('n', 'g'), 273),\n (('n', 'h'), 26),\n (('n', 'i'), 1725),\n (('n', 'j'), 44),\n (('n', 'k'), 58),\n (('n', 'l'), 195),\n (('n', 'm'), 19),\n (('n', 'n'), 1906),\n (('n', 'o'), 496),\n (('n', 'p'), 5),\n (('n', 'q'), 2),\n (('n', 'r'), 44),\n (('n', 's'), 278),\n (('n', 't'), 443),\n (('n', 'u'), 96),\n (('n', 'v'), 55),\n (('n', 'w'), 11),\n (('n', 'x'), 6),\n (('n', 'y'), 465),\n (('n', 'z'), 145),\n (('o', '&lt;E&gt;'), 855),\n (('o', 'a'), 149),\n (('o', 'b'), 140),\n (('o', 'c'), 114),\n (('o', 'd'), 190),\n (('o', 'e'), 132),\n (('o', 'f'), 34),\n (('o', 'g'), 44),\n (('o', 'h'), 171),\n (('o', 'i'), 69),\n (('o', 'j'), 16),\n (('o', 'k'), 68),\n (('o', 'l'), 619),\n (('o', 'm'), 261),\n (('o', 'n'), 2411),\n (('o', 'o'), 115),\n (('o', 'p'), 95),\n (('o', 'q'), 3),\n (('o', 'r'), 1059),\n (('o', 's'), 504),\n (('o', 't'), 118),\n (('o', 'u'), 275),\n (('o', 'v'), 176),\n (('o', 'w'), 114),\n (('o', 'x'), 45),\n (('o', 'y'), 103),\n (('o', 'z'), 54),\n (('p', '&lt;E&gt;'), 33),\n (('p', 'a'), 209),\n (('p', 'b'), 2),\n (('p', 'c'), 1),\n (('p', 'e'), 197),\n (('p', 'f'), 1),\n (('p', 'h'), 204),\n (('p', 'i'), 61),\n (('p', 'j'), 1),\n (('p', 'k'), 1),\n (('p', 'l'), 16),\n (('p', 'm'), 1),\n (('p', 'n'), 1),\n (('p', 'o'), 59),\n (('p', 'p'), 39),\n (('p', 'r'), 151),\n (('p', 's'), 16),\n (('p', 't'), 17),\n (('p', 'u'), 4),\n (('p', 'y'), 12),\n (('q', '&lt;E&gt;'), 28),\n (('q', 'a'), 13),\n (('q', 'e'), 1),\n (('q', 'i'), 13),\n (('q', 'l'), 1),\n (('q', 'm'), 2),\n (('q', 'o'), 2),\n (('q', 'r'), 1),\n (('q', 's'), 2),\n (('q', 'u'), 206),\n (('q', 'w'), 3),\n (('r', '&lt;E&gt;'), 1377),\n (('r', 'a'), 2356),\n (('r', 'b'), 41),\n (('r', 'c'), 99),\n (('r', 'd'), 187),\n (('r', 'e'), 1697),\n (('r', 'f'), 9),\n (('r', 'g'), 76),\n (('r', 'h'), 121),\n (('r', 'i'), 3033),\n (('r', 'j'), 25),\n (('r', 'k'), 90),\n (('r', 'l'), 413),\n (('r', 'm'), 162),\n (('r', 'n'), 140),\n (('r', 'o'), 869),\n (('r', 'p'), 14),\n (('r', 'q'), 16),\n (('r', 'r'), 425),\n (('r', 's'), 190),\n (('r', 't'), 208),\n (('r', 'u'), 252),\n (('r', 'v'), 80),\n (('r', 'w'), 21),\n (('r', 'x'), 3),\n (('r', 'y'), 773),\n (('r', 'z'), 23),\n (('s', '&lt;E&gt;'), 1169),\n (('s', 'a'), 1201),\n (('s', 'b'), 21),\n (('s', 'c'), 60),\n (('s', 'd'), 9),\n (('s', 'e'), 884),\n (('s', 'f'), 2),\n (('s', 'g'), 2),\n (('s', 'h'), 1285),\n (('s', 'i'), 684),\n (('s', 'j'), 2),\n (('s', 'k'), 82),\n (('s', 'l'), 279),\n (('s', 'm'), 90),\n (('s', 'n'), 24),\n (('s', 'o'), 531),\n (('s', 'p'), 51),\n (('s', 'q'), 1),\n (('s', 'r'), 55),\n (('s', 's'), 461),\n (('s', 't'), 765),\n (('s', 'u'), 185),\n (('s', 'v'), 14),\n (('s', 'w'), 24),\n (('s', 'y'), 215),\n (('s', 'z'), 10),\n (('t', '&lt;E&gt;'), 483),\n (('t', 'a'), 1027),\n (('t', 'b'), 1),\n (('t', 'c'), 17),\n (('t', 'e'), 716),\n (('t', 'f'), 2),\n (('t', 'g'), 2),\n (('t', 'h'), 647),\n (('t', 'i'), 532),\n (('t', 'j'), 3),\n (('t', 'l'), 134),\n (('t', 'm'), 4),\n (('t', 'n'), 22),\n (('t', 'o'), 667),\n (('t', 'r'), 352),\n (('t', 's'), 35),\n (('t', 't'), 374),\n (('t', 'u'), 78),\n (('t', 'v'), 15),\n (('t', 'w'), 11),\n (('t', 'x'), 2),\n (('t', 'y'), 341),\n (('t', 'z'), 105),\n (('u', '&lt;E&gt;'), 155),\n (('u', 'a'), 163),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('u', 'd'), 136),\n (('u', 'e'), 169),\n (('u', 'f'), 19),\n (('u', 'g'), 47),\n (('u', 'h'), 58),\n (('u', 'i'), 121),\n (('u', 'j'), 14),\n (('u', 'k'), 93),\n (('u', 'l'), 301),\n (('u', 'm'), 154),\n (('u', 'n'), 275),\n (('u', 'o'), 10),\n (('u', 'p'), 16),\n (('u', 'q'), 10),\n (('u', 'r'), 414),\n (('u', 's'), 474),\n (('u', 't'), 82),\n (('u', 'u'), 3),\n (('u', 'v'), 37),\n (('u', 'w'), 86),\n (('u', 'x'), 34),\n (('u', 'y'), 13),\n (('u', 'z'), 45),\n (('v', '&lt;E&gt;'), 88),\n (('v', 'a'), 642),\n (('v', 'b'), 1),\n (('v', 'd'), 1),\n (('v', 'e'), 568),\n (('v', 'h'), 1),\n (('v', 'i'), 911),\n (('v', 'k'), 3),\n (('v', 'l'), 14),\n (('v', 'n'), 8),\n (('v', 'o'), 153),\n (('v', 'r'), 48),\n (('v', 'u'), 7),\n (('v', 'v'), 7),\n (('v', 'y'), 121),\n (('w', '&lt;E&gt;'), 51),\n (('w', 'a'), 280),\n (('w', 'b'), 1),\n (('w', 'd'), 8),\n (('w', 'e'), 149),\n (('w', 'f'), 2),\n (('w', 'g'), 1),\n (('w', 'h'), 23),\n (('w', 'i'), 148),\n (('w', 'k'), 6),\n (('w', 'l'), 13),\n (('w', 'm'), 2),\n (('w', 'n'), 58),\n (('w', 'o'), 36),\n (('w', 'r'), 22),\n (('w', 's'), 20),\n (('w', 't'), 8),\n (('w', 'u'), 25),\n (('w', 'w'), 2),\n (('w', 'y'), 73),\n (('w', 'z'), 1),\n (('x', '&lt;E&gt;'), 164),\n (('x', 'a'), 103),\n (('x', 'b'), 1),\n (('x', 'c'), 4),\n (('x', 'd'), 5),\n (('x', 'e'), 36),\n (('x', 'f'), 3),\n (('x', 'h'), 1),\n (('x', 'i'), 102),\n (('x', 'l'), 39),\n (('x', 'm'), 1),\n (('x', 'n'), 1),\n (('x', 'o'), 41),\n (('x', 's'), 31),\n (('x', 't'), 70),\n (('x', 'u'), 5),\n (('x', 'w'), 3),\n (('x', 'x'), 38),\n (('x', 'y'), 30),\n (('x', 'z'), 19),\n (('y', '&lt;E&gt;'), 2007),\n (('y', 'a'), 2143),\n (('y', 'b'), 27),\n (('y', 'c'), 115),\n (('y', 'd'), 272),\n (('y', 'e'), 301),\n (('y', 'f'), 12),\n (('y', 'g'), 30),\n (('y', 'h'), 22),\n (('y', 'i'), 192),\n (('y', 'j'), 23),\n (('y', 'k'), 86),\n (('y', 'l'), 1104),\n (('y', 'm'), 148),\n (('y', 'n'), 1826),\n (('y', 'o'), 271),\n (('y', 'p'), 15),\n (('y', 'q'), 6),\n (('y', 'r'), 291),\n (('y', 's'), 401),\n (('y', 't'), 104),\n (('y', 'u'), 141),\n (('y', 'v'), 106),\n (('y', 'w'), 4),\n (('y', 'x'), 28),\n (('y', 'y'), 23),\n (('y', 'z'), 78),\n (('z', '&lt;E&gt;'), 160),\n (('z', 'a'), 860),\n (('z', 'b'), 4),\n (('z', 'c'), 2),\n (('z', 'd'), 2),\n (('z', 'e'), 373),\n (('z', 'g'), 1),\n (('z', 'h'), 43),\n (('z', 'i'), 364),\n (('z', 'j'), 2),\n (('z', 'k'), 2),\n (('z', 'l'), 123),\n (('z', 'm'), 35),\n (('z', 'n'), 4),\n (('z', 'o'), 110),\n (('z', 'p'), 2),\n (('z', 'r'), 32),\n (('z', 's'), 4),\n (('z', 't'), 4),\n (('z', 'u'), 73),\n (('z', 'v'), 2),\n (('z', 'w'), 3),\n (('z', 'x'), 1),\n (('z', 'y'), 147),\n (('z', 'z'), 45)]</pre> In\u00a0[35]: Copied! <pre>sorted(b.items(), key= lambda kv: -kv[1])  #Now, here we are specifying we want to sort based on the values. So we select the key, then in the lambda function, we take the keyvalue (kv) and select the second element in the set, which is what we want\n# - sign is to go backwards\n</pre> sorted(b.items(), key= lambda kv: -kv[1])  #Now, here we are specifying we want to sort based on the values. So we select the key, then in the lambda function, we take the keyvalue (kv) and select the second element in the set, which is what we want # - sign is to go backwards Out[35]: <pre>[(('n', '&lt;E&gt;'), 6763),\n (('a', '&lt;E&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;S&gt;', 'a'), 4410),\n (('e', '&lt;E&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;S&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;S&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;E&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;S&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;E&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;S&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;E&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;S&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;S&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;S&gt;', 'l'), 1572),\n (('&lt;S&gt;', 'c'), 1542),\n (('&lt;S&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;E&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;E&gt;'), 1314),\n (('&lt;S&gt;', 't'), 1308),\n (('&lt;S&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;E&gt;'), 1169),\n (('&lt;S&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;S&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;S&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;E&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;S&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;S&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;S&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;E&gt;'), 516),\n (('d', '&lt;E&gt;'), 516),\n (('&lt;S&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;E&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;S&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;S&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;S&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;E&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;S&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;E&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;E&gt;'), 160),\n (('u', '&lt;E&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;S&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;E&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;E&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;E&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;S&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;E&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;E&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;S&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;E&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;E&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;E&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;E&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]</pre> <p>Counting bigrams in a 2D torch tensor (\"training the model\") 00:12:45</p> In\u00a0[2]: Copied! <pre>import torch\n</pre> import torch In\u00a0[37]: Copied! <pre>N = torch.zeros((28, 28), dtype = torch.int32)\n</pre> N = torch.zeros((28, 28), dtype = torch.int32) In\u00a0[38]: Copied! <pre>chars = sorted(list(set(''.join(words))))\n\nstoi = {s:i for i,s in enumerate(chars)}\nstoi['&lt;S&gt;'] = 26\nstoi['&lt;E&gt;'] = 27\n</pre> chars = sorted(list(set(''.join(words))))  stoi = {s:i for i,s in enumerate(chars)} stoi[''] = 26 stoi[''] = 27 In\u00a0[39]: Copied! <pre>stoi    #Btw this is just 's to i'\n</pre> stoi    #Btw this is just 's to i' Out[39]: <pre>{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '&lt;S&gt;': 26,\n '&lt;E&gt;': 27}</pre> In\u00a0[\u00a0]: Copied! <pre>for word in words:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1, ix2] += 1\n</pre> for word in words:     chs = [''] + list(word) + ['']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         N[ix1, ix2] += 1 In\u00a0[\u00a0]: Copied! <pre>N\n</pre> N Out[\u00a0]: <pre>tensor([[ 556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568, 2528,\n         1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,  182,\n         2050,  435,    0, 6640],\n        [ 321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,  103,\n            0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,    0,\n           83,    0,    0,  114],\n        [ 815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,  116,\n            0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,    3,\n          104,    4,    0,   97],\n        [1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,   60,\n           30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,    0,\n          317,    1,    0,  516],\n        [ 679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178, 3248,\n          769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,  132,\n         1070,  181,    0, 3983],\n        [ 242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,   20,\n            0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,    0,\n           14,    2,    0,   80],\n        [ 330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,   32,\n            6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,    0,\n           31,    1,    0,  108],\n        [2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,  185,\n          117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,    0,\n          213,   20,    0, 2409],\n        [2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445, 1345,\n          427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,   89,\n          779,  277,    0, 2489],\n        [1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,    9,\n            5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,    0,\n           10,    0,    0,   71],\n        [1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,  139,\n            9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,    0,\n          379,    2,    0,  363],\n        [2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24, 1345,\n           60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,    0,\n         1588,   10,    0, 1314],\n        [2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,    5,\n          168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,    0,\n          287,   11,    0,  516],\n        [2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,  195,\n           19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,    6,\n          465,  145,    0, 6763],\n        [ 149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,  619,\n          261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,   45,\n          103,   54,    0,  855],\n        [ 209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,   16,\n            1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,    0,\n           12,    0,    0,   33],\n        [  13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,    1,\n            2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,    0,\n            0,    0,    0,   28],\n        [2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,  413,\n          162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,    3,\n          773,   23,    0, 1377],\n        [1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,  279,\n           90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,    0,\n          215,   10,    0, 1169],\n        [1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,  134,\n            4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,    2,\n          341,  105,    0,  483],\n        [ 163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,  301,\n          154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,   34,\n           13,   45,    0,  155],\n        [ 642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,   14,\n            0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,    0,\n          121,    0,    0,   88],\n        [ 280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,   13,\n            2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,    0,\n           73,    1,    0,   51],\n        [ 103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,   39,\n            1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,   38,\n           30,   19,    0,  164],\n        [2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86, 1104,\n          148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,   28,\n           23,   78,    0, 2007],\n        [ 860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,  123,\n           35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,    1,\n          147,   45,    0,  160],\n        [4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963, 1572,\n         2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,  134,\n          535,  929,    0,    0],\n        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]], dtype=torch.int32)</pre> <p>Visualizing the bigram tensor 00:18:19</p> In\u00a0[\u00a0]: Copied! <pre>#Okay lmao yeah, dont run this cell. Although the logic provides the same output, it's causing some issue with the plotting\n\n# itos = {i: s for i, s in enumerate(chars)}\n# itos\n</pre> #Okay lmao yeah, dont run this cell. Although the logic provides the same output, it's causing some issue with the plotting  # itos = {i: s for i, s in enumerate(chars)} # itos In\u00a0[3]: Copied! <pre>itos = {i:s for s,i in stoi.items()}\nitos\n</pre> itos = {i:s for s,i in stoi.items()} itos In\u00a0[\u00a0]: Copied! <pre>#This code is directly taken from the video. Andrej prolly spent a lot of time designing this so\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(28):\n    for j in range(28):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off');\n</pre> #This code is directly taken from the video. Andrej prolly spent a lot of time designing this so  import matplotlib.pyplot as plt %matplotlib inline  plt.figure(figsize=(16,16)) plt.imshow(N, cmap='Blues') for i in range(28):     for j in range(28):         chstr = itos[i] + itos[j]         plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')         plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray') plt.axis('off'); <p>Deleting spurious (S) and (E) tokens in favor of a single . token 00:20:54</p> In\u00a0[18]: Copied! <pre>N = torch.zeros((27, 27), dtype = torch.int32)\n\nchars = sorted(list(set(''.join(words))))\n\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\n</pre> N = torch.zeros((27, 27), dtype = torch.int32)  chars = sorted(list(set(''.join(words))))  stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 In\u00a0[19]: Copied! <pre>itos = {i:s for s,i in stoi.items()}\n</pre> itos = {i:s for s,i in stoi.items()} In\u00a0[20]: Copied! <pre>for word in words:\n    chs = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1, ix2] += 1\n</pre> for word in words:     chs = ['.'] + list(word) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         N[ix1, ix2] += 1 In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(27):\n    for j in range(27):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off');\n</pre> import matplotlib.pyplot as plt %matplotlib inline  plt.figure(figsize=(16,16)) plt.imshow(N, cmap='Blues') for i in range(27):     for j in range(27):         chstr = itos[i] + itos[j]         plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')         plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray') plt.axis('off'); <p>Sampling from the model 00:24:02</p> In\u00a0[\u00a0]: Copied! <pre>N[0]    #Viewing just the first row\n</pre> N[0]    #Viewing just the first row Out[\u00a0]: <pre>tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)</pre> <p>First we make them all into float.  Then we make a probability distribution  We do that by dividing <code>p</code> with <code>p.sum()</code></p> In\u00a0[9]: Copied! <pre>p = N[0].float()\np = p / p.sum()\np\n</pre> p = N[0].float() p = p / p.sum() p Out[9]: <pre>tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])</pre> In\u00a0[11]: Copied! <pre>p.sum().item()\n</pre> p.sum().item() Out[11]: <pre>1.0</pre> <p>So the total probability sums up to 1. Therefore now we have the probability values for each of those characters.</p> In\u00a0[13]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\np = torch.rand(3, generator=g)\np = p / p.sum()\np\n</pre> g = torch.Generator().manual_seed(2147483647) p = torch.rand(3, generator=g) p = p / p.sum() p Out[13]: <pre>tensor([0.6064, 0.3033, 0.0903])</pre> In\u00a0[14]: Copied! <pre>torch.multinomial(p, num_samples=20, replacement=True, generator=g)\n</pre> torch.multinomial(p, num_samples=20, replacement=True, generator=g) Out[14]: <pre>tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1])</pre> <p>So based on the probability percentages <code>p</code> we get a bunch of sample values  So <code>0</code> should be 60%, <code>1</code> should be 30%, <code>2</code> should be 10% of the total samples generated</p> In\u00a0[19]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\nix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitos[ix]\n</pre> g = torch.Generator().manual_seed(2147483647) ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() itos[ix] Out[19]: <pre>'.'</pre> <p>So now, if we had got another sampled value, lets say 'm' (so we have taken the column), now we go to the row containing 'm' and then check for its correspondant character.      Keeping this jest in mind, we will be making this into a loop.</p> In\u00a0[20]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        p = N[ix].float()\n        p = p / p.sum()\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n</pre> g = torch.Generator().manual_seed(2147483647)  for i in range(10):     out = []     ix = 0     while True:         p = N[ix].float()         p = p / p.sum()         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()         out.append(itos[ix])         if ix == 0:             break     print(''.join(out)) <pre>junide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\n</pre> <p>And this is why the Bigram model is so bad lol. The output generated not to great (Andrej said \"terrible\" xd), for example for <code>p.</code> , is that the model doesn't understand that 'p' should have had something before or after it. Right now, it considers that as a name itself.</p> <p>But now we will see why the output made by the model is not exactly too terrible</p> In\u00a0[21]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        # p = N[ix].float()\n        # p = p / p.sum()\n        p = torch.ones(27) / 27.0\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n</pre> g = torch.Generator().manual_seed(2147483647)  for i in range(10):     out = []     ix = 0     while True:         # p = N[ix].float()         # p = p / p.sum()         p = torch.ones(27) / 27.0         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()         out.append(itos[ix])         if ix == 0:             break     print(''.join(out)) <pre>juwjdvdipkcqaz.\np.\ncfqywocnzqfjiirltozcogsjgwzvudlhnpauyjbilevhajkdbduinrwibtlzsnjyievyvaftbzffvmumthyfodtumjrpfytszwjhrjagq.\ncoreaysezocfkyjjabdywejfmoifmwyfinwagaasnhsvfihofszxhddgosfmptpagicz.\nrjpiufmthdt.\nrkrrsru.\niyumuyfy.\nmjekujcbkhvupwyhvpvhvccragr.\nwdkhwfdztta.\nmplyisbxlyhuuiqzavmpocbzthqmimvyqwat.\n</pre> <p>So now this is what we get when the model is completely untrained, it gives you a garbage of values.   This is happening because we removed the probability distribution and added a distribution of uniform values. So all of the characters are equally likely to occur.</p> <p>So yeah, if we train it with a Bigram, then its a lot better output. So ultimately it is actually working, just that Bigram is not so great for this xD</p> In\u00a0[\u00a0]: Copied! <pre>#Solving the inefficiency problem\nP = N.float()\nP = P / P.sum(1, keepdim=True) #Here is where we are applying the sum and broadcasting rules. Sum for the function and Broadcasting for the division part that takes place. \n\n# 27 27\n# 27  1\n</pre> #Solving the inefficiency problem P = N.float() P = P / P.sum(1, keepdim=True) #Here is where we are applying the sum and broadcasting rules. Sum for the function and Broadcasting for the division part that takes place.   # 27 27 # 27  1 In\u00a0[\u00a0]: Copied! <pre>P[0].sum() #This should return the tensor object with value 1. So that entire row as been normalised\n</pre> P[0].sum() #This should return the tensor object with value 1. So that entire row as been normalised <p>So the rule says:</p> <pre><code>Two tensors are \u201cbroadcastable\u201d if the following rules hold:\n\nEach tensor has at least one dimension.\n\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n</code></pre> <p>So in our case, one of the dimentional sizes is one i.e 27 1  And, the dimension sizes are equal:  27 27</p> In\u00a0[\u00a0]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        p = P[ix]\n        # p = N[ix].float()\n        # p = p / p.sum()\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n</pre> g = torch.Generator().manual_seed(2147483647)  for i in range(10):     out = []     ix = 0     while True:         p = P[ix]         # p = N[ix].float()         # p = p / p.sum()         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()         out.append(itos[ix])         if ix == 0:             break     print(''.join(out)) In\u00a0[\u00a0]: Copied! <pre>#That will produce the same output.\n</pre> #That will produce the same output. In\u00a0[6]: Copied! <pre>P = N.float()\nP /= P.sum(1, keepdim=True)\n</pre> P = N.float() P /= P.sum(1, keepdim=True) In\u00a0[\u00a0]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        p = P[ix]\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n</pre> g = torch.Generator().manual_seed(2147483647)  for i in range(10):     out = []     ix = 0     while True:         p = P[ix]         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()         out.append(itos[ix])         if ix == 0:             break     print(''.join(out)) <p>Evaluating our model</p> In\u00a0[3]: Copied! <pre>import torch\n</pre> import torch In\u00a0[12]: Copied! <pre>N = torch.zeros((27, 27), dtype = torch.int32)\n\nchars = sorted(list(set(''.join(words))))\n\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\n</pre> N = torch.zeros((27, 27), dtype = torch.int32)  chars = sorted(list(set(''.join(words))))  stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 In\u00a0[\u00a0]: Copied! <pre>itos = {i:s for s,i in stoi.items()}\n</pre> itos = {i:s for s,i in stoi.items()} In\u00a0[21]: Copied! <pre>P = N.float()\nP /= P.sum(1, keepdim=True)\n</pre> P = N.float() P /= P.sum(1, keepdim=True) In\u00a0[\u00a0]: Copied! <pre>log_likelihood = 0.0\nn = 0\n\nfor word in words[:3]:\n    chs = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        prob = P[ix1, ix2] #Likelihood - product of all the values\n        logprob = torch.log(prob) #Log Likelihood\n        log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values\n        n += 1 #Log Likelihood - For the average\n        print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')\n\nprint(f'{log_likelihood=}')\nnll = -log_likelihood #Negative Log Likelihood\nprint(f'{nll=}')\nprint(f'{nll/n}') #Negative Log Likelihood - Average value\n</pre> log_likelihood = 0.0 n = 0  for word in words[:3]:     chs = ['.'] + list(word) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         prob = P[ix1, ix2] #Likelihood - product of all the values         logprob = torch.log(prob) #Log Likelihood         log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values         n += 1 #Log Likelihood - For the average         print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')  print(f'{log_likelihood=}') nll = -log_likelihood #Negative Log Likelihood print(f'{nll=}') print(f'{nll/n}') #Negative Log Likelihood - Average value  <pre>.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=tensor(-38.7856)\nnll=tensor(38.7856)\n2.424102306365967\n</pre> In\u00a0[28]: Copied! <pre>#Model smoothening\nP = (N+1).float()\nP /= P.sum(1, keepdim=True)\n</pre> #Model smoothening P = (N+1).float() P /= P.sum(1, keepdim=True) In\u00a0[31]: Copied! <pre>log_likelihood = 0.0\nn = 0\n\n# for word in words[:3]:\nfor word in [\"muzzammil\"]:\n    chs = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        prob = P[ix1, ix2] #Likelihood - product of all the values\n        logprob = torch.log(prob) #Log Likelihood\n        log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values\n        n += 1 #Log Likelihood - For the average\n        print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')\n\nprint(f'{log_likelihood=}')\nnll = -log_likelihood #Negative Log Likelihood\nprint(f'{nll=}')\nprint(f'{nll/n}') #Negative Log Likelihood - Average value\n</pre> log_likelihood = 0.0 n = 0  # for word in words[:3]: for word in [\"muzzammil\"]:     chs = ['.'] + list(word) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         prob = P[ix1, ix2] #Likelihood - product of all the values         logprob = torch.log(prob) #Log Likelihood         log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values         n += 1 #Log Likelihood - For the average         print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')  print(f'{log_likelihood=}') nll = -log_likelihood #Negative Log Likelihood print(f'{nll=}') print(f'{nll/n}') #Negative Log Likelihood - Average value  <pre>.m: 0.0792 -2.5358\nmu: 0.0210 -3.8636\nuz: 0.0145 -4.2303\nzz: 0.0190 -3.9649\nza: 0.3551 -1.0355\nam: 0.0482 -3.0321\nmm: 0.0253 -3.6753\nmi: 0.1885 -1.6687\nil: 0.0759 -2.5780\nl.: 0.0940 -2.3641\nlog_likelihood=tensor(-28.9485)\nnll=tensor(28.9485)\n2.894852876663208\n</pre>"},{"location":"ZeroToHero/Makemore-part1/A-Main-Notebook/#set-a-notebook","title":"SET A - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/","title":"SET A - LECTURE NOTES","text":"<p>In this we are basically going to do next-prediction based on characters. So for each letter, we will be providing a large dataset of names of people, and it will be able to generate many more related/fancy names. So right now, it is character level wise, soon we will go words to generate an entire document of text.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#introduction","title":"Introduction","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-000000","title":"Timestamp: 00:00:00","text":"<p>It is a character level language model, therefore we are trying to produce the next character in the sequence. We are also going to implement a large number of Character Language Models, in terms of the neural networks that are involved in predicting the next character of the sequence: Bigram, Bag of words, MLP, RNN, GRU and Transformer (Some many change as the course progresses).</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#reading-and-exploring-the-dataset","title":"Reading and exploring the dataset","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-000303","title":"Timestamp: 00:03:03","text":"<p>We have loaded a dataset containing up to 32K names. We are starting with the first Language Model - Bigram. Which basically considers only 2 characters at once: One is the previous character which is there and using that to predict the next one. (It is a weak language model, but is a great place to start).</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#exploring-the-bigrams-in-the-dataset","title":"Exploring the bigrams in the dataset","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-000624","title":"Timestamp: 00:06:24","text":"<p>Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#counting-bigrams-in-a-python-dictionary","title":"Counting bigrams in a python dictionary","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-000924","title":"Timestamp: 00:09:24","text":"<p>Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#counting-bigrams-in-a-2d-torch-tensor-training-the-model","title":"Counting bigrams in a 2D torch tensor (\"training the model\")","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-001245","title":"Timestamp: 00:12:45","text":"<p>Here we are making a 2D Array where we can see in how many instance the second character will follow the first character. We have basically created a 2D Array using PyTorch, Instead of that dictionary we manually made. This is a lot cleaner. Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#visualizing-the-bigram-tensor","title":"Visualizing the bigram tensor","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-001819","title":"Timestamp: 00:18:19","text":"<p>Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#deleting-spurious-s-and-e-tokens-in-favor-of-a-single-token","title":"Deleting spurious (S) and (E) tokens in favor of a single . token","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-002054","title":"Timestamp: 00:20:54","text":"<p>So the graph that was plotted in the above chapter, there are these one additional row and column where the values are for the cases if <code>&lt;S&gt;</code> is in the end and if <code>&lt;E&gt;</code> is at the beginning, which doesn't make sense because we used them to map the start and end. Instead we will be replacing them with a <code>.</code> So in the graph it will be in the corner end <code>..</code> as 0. Therefore we also save space. Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#sampling-from-the-model","title":"Sampling from the model","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-002402","title":"Timestamp: 00:24:02","text":"<p>Now to perform sampling we are using 'multinomial' from PyTorch, which is like this built-in function which says \"You give me distributions and I will give you a sample of integers\".</p> <p>Along with that we are also using 'generator' object from PyTorch again to make everything more deterministic - So the values that we get while running the code will remain the same (Won't keep changing)</p> <p>Finally we saw how a Bigram model performs better than an untrained model in terms of the output produced. Check in code file</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#efficiency-vectorized-normalization-of-the-rows-tensor-broadcasting","title":"\u2b50Efficiency! vectorized normalization of the rows, tensor broadcasting","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-003617","title":"Timestamp: 00:36:17","text":"<p>Note from the author:</p> <p>This is an important lecture. As there is one small concept that has been broken down into its most detailed. This also needs to be understood separately and then come back to this video and understand this. </p> <p>As Andrej said, \"This is not to be done fast and loose. So I encourage you to understand, read through, watch tutorials and practice. As we can easily run into major bugs here\".</p> <p>Important Timestamps for words of advise:</p> <p>37:15 - 37:40 -&gt; very important advise / hint for next lectures</p> <p>42:05 -&gt; First intro of broadcasting rules</p> <p>44:23 - 44:45 -&gt; The \"let me scare you a little bit\"</p> <p>Links to refer:</p> <ul> <li> <p>PyTorch Documentation on sum() function</p> </li> <li> <p>The important and to be respected - Broadcasting Semantics</p> </li> </ul> <p>This part was done and the two lines of code which handled this was also executed: <code>P = N.float()</code> and <code>P /= P.sum(1, keepdims=True)</code>. The second one is where we have used the sum and broadcasting rules.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#loss-function-the-negative-log-likelihood-of-the-data-under-our-model","title":"Loss function (the negative log likelihood of the data under our model)","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-005014","title":"Timestamp: 00:50:14","text":"<p>So here we are trying to evaluate the performance of our model. First we check the likelihood, which is basically the product of all parameters, but since that won't be very convenient to work with on a large scale (since most of the values are almost close to 0), we take the log likelihood.</p> <p>Likelihood is the product of all the individual probabilities. Log Likelihood is the sum of all the log of the individual properties i.e. log(a * b * c) = log(a) + log(b) + log(c)</p> <p>Now, we usually see how large can the log likelihood value get, to see that we take the Negative Log Likelihood value (we're basically turning the value positive now). Another practice which is usually followed here, is that we take the overall average of the NLL, rather than the sum. -&gt; This becomes the final considered value, this value represents the quality of our model. The lower it is, the better off we are and the higher it is, the worse off.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#model-smoothing-with-fake-counts","title":"Model smoothing with fake counts","text":""},{"location":"ZeroToHero/Makemore-part1/A-main-makemore-part1/#timestamp-010050","title":"Timestamp: 01:00:50","text":"<p>We do model smoothing, where we basically just add values to the list of values (Like adding all values with 1, 5 etc. - More usually means more smoothing. Like adding 10000, so they all more or less get the same count). This is done so that we avoid the possibility of getting an infinite probability.</p> <p> </p> <p>Okay so now, we've trained a respectable, Bigram language model. </p> <p>And we saw that: </p> <ul> <li> <p>We have both sort of trained that model by looking at the counts of all the bigrams and normalizing the rows, to get probability distributions. </p> </li> <li> <p>We also saw that then we can also then use the parameters of this model to perform sampling of new words (So we sampled new names according to those distributions)</p> </li> <li> <p>Finally, we also saw that we can evaluate the quality of this model. And the quality of this model is summarized in a single number, which is the negative log likelihood (nll)</p> </li> <li> <p>So, lower the nll number is, the better the model is, because it is giving high probabilities to the actual next characters in all the bigrams in our training set.</p> </li> </ul> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-Main-Notebook/","title":"Jupyter Notebook","text":"In\u00a0[23]: Copied! <pre>words = open('names.txt', 'r').read().splitlines()\n</pre> words = open('names.txt', 'r').read().splitlines() In\u00a0[24]: Copied! <pre>import torch\n\nN = torch.zeros((27, 27), dtype = torch.int32)\n\nchars = sorted(list(set(''.join(words))))\n\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\n\nitos = {i:s for s,i in stoi.items()}\n</pre> import torch  N = torch.zeros((27, 27), dtype = torch.int32)  chars = sorted(list(set(''.join(words))))  stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0  itos = {i:s for s,i in stoi.items()} In\u00a0[25]: Copied! <pre>P = N.float()\nP /= P.sum(1, keepdim=True)\n</pre> P = N.float() P /= P.sum(1, keepdim=True) In\u00a0[26]: Copied! <pre>#Creating the training set of bigrams (x,y)\nxs, ys = [], []\n\nfor word in words[:1]:\n    chs = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        print(ch1, ch2)\n        xs.append(ix1)\n        ys.append(ix2)\n\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n</pre> #Creating the training set of bigrams (x,y) xs, ys = [], []  for word in words[:1]:     chs = ['.'] + list(word) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         print(ch1, ch2)         xs.append(ix1)         ys.append(ix2)  xs = torch.tensor(xs) ys = torch.tensor(ys) <pre>. e\ne m\nm m\nm a\na .\n</pre> In\u00a0[5]: Copied! <pre>xs\n</pre> xs Out[5]: <pre>tensor([ 0,  5, 13, 13,  1])</pre> In\u00a0[6]: Copied! <pre>ys\n</pre> ys Out[6]: <pre>tensor([ 5, 13, 13,  1,  0])</pre> In\u00a0[18]: Copied! <pre>#Feeding these examples into a neural network\nimport torch.nn.functional as F\nxenc = F.one_hot(xs, num_classes=27).float() #IMP: manual type casting\nxenc\n</pre> #Feeding these examples into a neural network import torch.nn.functional as F xenc = F.one_hot(xs, num_classes=27).float() #IMP: manual type casting xenc Out[18]: <pre>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre> In\u00a0[20]: Copied! <pre>xenc.shape\n</pre> xenc.shape Out[20]: <pre>torch.Size([5, 27])</pre> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[21]: Copied! <pre>plt.imshow(xenc)\n</pre> plt.imshow(xenc) Out[21]: <pre>&lt;matplotlib.image.AxesImage at 0x24c6d3e5ae0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>W = torch.randn((27, 27))   #Generating the weights\nxenc @ W    #Doing matrix multiplication\n</pre> W = torch.randn((27, 27))   #Generating the weights xenc @ W    #Doing matrix multiplication Out[\u00a0]: <pre>tensor([[ 0.5838, -0.8614,  0.1874, -0.5662,  0.2449,  1.4738,  1.8403,  0.3233,\n          1.0014,  0.0263, -0.5269, -0.8413,  0.0329, -0.0670, -0.7272, -0.2977,\n         -0.5083,  0.1050, -0.5482,  1.0237,  1.2359,  1.6366, -1.6188,  0.3283,\n          0.7180, -0.9729, -1.5425],\n        [ 1.4868, -0.0457,  0.2224,  1.5423, -0.0151, -0.2254,  0.7613, -0.4738,\n         -0.2175, -0.9024,  0.0148,  0.6673, -0.1291, -1.4357,  0.2100, -0.5559,\n         -0.0711, -0.1631,  0.1704,  0.5689, -1.2534, -0.0207,  0.2485,  0.9525,\n          0.1465,  0.1339,  0.1875],\n        [-0.3253,  0.6007,  1.3449,  0.0990, -0.6273,  0.4972, -0.2262,  0.4910,\n         -1.6546,  0.5298, -0.3165, -0.7659,  0.9075, -0.4458,  0.9129, -2.7461,\n          0.0098,  0.9013,  0.7363, -0.7745, -0.8155,  1.5463,  0.0723, -0.5926,\n         -0.2548,  0.4572, -0.9398],\n        [-0.3253,  0.6007,  1.3449,  0.0990, -0.6273,  0.4972, -0.2262,  0.4910,\n         -1.6546,  0.5298, -0.3165, -0.7659,  0.9075, -0.4458,  0.9129, -2.7461,\n          0.0098,  0.9013,  0.7363, -0.7745, -0.8155,  1.5463,  0.0723, -0.5926,\n         -0.2548,  0.4572, -0.9398],\n        [-0.6620,  0.3081,  0.4002,  1.4361, -0.9089, -0.3304,  0.1364, -1.0887,\n          0.6219,  0.6222, -0.6723,  0.9616, -0.4970,  0.2513, -0.2499,  1.1944,\n          0.7755,  1.2483,  0.8315, -0.1463,  0.2847, -0.4837, -0.7275, -2.0723,\n         -2.0994, -0.3072, -1.8622]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Checking for one element\n(xenc @ W)[3, 13]\n</pre> #Checking for one element (xenc @ W)[3, 13] Out[\u00a0]: <pre>tensor(-0.4458)</pre> In\u00a0[\u00a0]: Copied! <pre>#Doing manual multiplication for verifying\n(xenc[3] * W[:,13]).sum()\n</pre> #Doing manual multiplication for verifying (xenc[3] * W[:,13]).sum() Out[\u00a0]: <pre>tensor(-0.4458)</pre> In\u00a0[\u00a0]: Copied! <pre>logits = xenc @ W   #log-counts\ncounts = logits.exp()   #equivalent to N, as done in A-Main-Notebook\nprobs = counts / counts.sum(1, keepdims=True)   #Normalising the rows (as we had done in A-Main as well. To calculate the probability)\nprobs\n</pre> logits = xenc @ W   #log-counts counts = logits.exp()   #equivalent to N, as done in A-Main-Notebook probs = counts / counts.sum(1, keepdims=True)   #Normalising the rows (as we had done in A-Main as well. To calculate the probability) probs Out[\u00a0]: <pre>tensor([[0.0415, 0.0098, 0.0279, 0.0132, 0.0296, 0.1012, 0.1459, 0.0320, 0.0631,\n         0.0238, 0.0137, 0.0100, 0.0239, 0.0217, 0.0112, 0.0172, 0.0139, 0.0257,\n         0.0134, 0.0645, 0.0797, 0.1190, 0.0046, 0.0322, 0.0475, 0.0088, 0.0050],\n        [0.1218, 0.0263, 0.0344, 0.1287, 0.0271, 0.0220, 0.0589, 0.0171, 0.0221,\n         0.0112, 0.0279, 0.0537, 0.0242, 0.0066, 0.0340, 0.0158, 0.0256, 0.0234,\n         0.0326, 0.0486, 0.0079, 0.0270, 0.0353, 0.0714, 0.0319, 0.0315, 0.0332],\n        [0.0199, 0.0501, 0.1055, 0.0303, 0.0147, 0.0452, 0.0219, 0.0449, 0.0053,\n         0.0467, 0.0200, 0.0128, 0.0681, 0.0176, 0.0685, 0.0018, 0.0278, 0.0677,\n         0.0574, 0.0127, 0.0122, 0.1290, 0.0295, 0.0152, 0.0213, 0.0434, 0.0107],\n        [0.0199, 0.0501, 0.1055, 0.0303, 0.0147, 0.0452, 0.0219, 0.0449, 0.0053,\n         0.0467, 0.0200, 0.0128, 0.0681, 0.0176, 0.0685, 0.0018, 0.0278, 0.0677,\n         0.0574, 0.0127, 0.0122, 0.1290, 0.0295, 0.0152, 0.0213, 0.0434, 0.0107],\n        [0.0146, 0.0385, 0.0422, 0.1188, 0.0114, 0.0203, 0.0324, 0.0095, 0.0526,\n         0.0526, 0.0144, 0.0739, 0.0172, 0.0363, 0.0220, 0.0933, 0.0614, 0.0985,\n         0.0649, 0.0244, 0.0376, 0.0174, 0.0137, 0.0036, 0.0035, 0.0208, 0.0044]])</pre> In\u00a0[\u00a0]: Copied! <pre># SUMMARY ------------------------------&gt;&gt;&gt;&gt;\n#Run the first 4 cells of this notebook and then continue\n</pre> # SUMMARY ------------------------------&gt;&gt;&gt;&gt; #Run the first 4 cells of this notebook and then continue In\u00a0[27]: Copied! <pre>xs\n</pre> xs Out[27]: <pre>tensor([ 0,  5, 13, 13,  1])</pre> In\u00a0[28]: Copied! <pre>ys\n</pre> ys Out[28]: <pre>tensor([ 5, 13, 13,  1,  0])</pre> In\u00a0[29]: Copied! <pre># randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g)\n</pre> # randomly initialize 27 neurons' weights. each neuron receives 27 inputs g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g) In\u00a0[30]: Copied! <pre>xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n# btw: the last 2 lines here are together called a 'softmax'\n</pre>  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding logits = xenc @ W # predict log-counts counts = logits.exp() # counts, equivalent to N probs = counts / counts.sum(1, keepdims=True) # probabilities for next character # btw: the last 2 lines here are together called a 'softmax' In\u00a0[31]: Copied! <pre>probs.shape\n</pre> probs.shape Out[31]: <pre>torch.Size([5, 27])</pre> In\u00a0[32]: Copied! <pre>nlls = torch.zeros(5)\nfor i in range(5):\n  # i-th bigram:\n  x = xs[i].item() # input character index\n  y = ys[i].item() # label character index\n  print('--------')\n  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n  print('input to the neural net:', x)\n  print('output probabilities from the neural net:', probs[i])\n  print('label (actual next character):', y)\n  p = probs[i, y]\n  print('probability assigned by the net to the the correct character:', p.item())\n  logp = torch.log(p)\n  print('log likelihood:', logp.item())\n  nll = -logp\n  print('negative log likelihood:', nll.item())\n  nlls[i] = nll\n\nprint('=========')\nprint('average negative log likelihood, i.e. loss =', nlls.mean().item())\n</pre> nlls = torch.zeros(5) for i in range(5):   # i-th bigram:   x = xs[i].item() # input character index   y = ys[i].item() # label character index   print('--------')   print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')   print('input to the neural net:', x)   print('output probabilities from the neural net:', probs[i])   print('label (actual next character):', y)   p = probs[i, y]   print('probability assigned by the net to the the correct character:', p.item())   logp = torch.log(p)   print('log likelihood:', logp.item())   nll = -logp   print('negative log likelihood:', nll.item())   nlls[i] = nll  print('=========') print('average negative log likelihood, i.e. loss =', nlls.mean().item()) <pre>--------\nbigram example 1: .e (indexes 0,5)\ninput to the neural net: 0\noutput probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\nlabel (actual next character): 5\nprobability assigned by the net to the the correct character: 0.01228625513613224\nlog likelihood: -4.399273872375488\nnegative log likelihood: 4.399273872375488\n--------\nbigram example 2: em (indexes 5,13)\ninput to the neural net: 5\noutput probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.018050700426101685\nlog likelihood: -4.014570713043213\nnegative log likelihood: 4.014570713043213\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.026691533625125885\nlog likelihood: -3.623408794403076\nnegative log likelihood: 3.623408794403076\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 1\nprobability assigned by the net to the the correct character: 0.07367686182260513\nlog likelihood: -2.6080665588378906\nnegative log likelihood: 2.6080665588378906\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the neural net: 1\noutput probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\nlabel (actual next character): 0\nprobability assigned by the net to the the correct character: 0.014977526850998402\nlog likelihood: -4.201204299926758\nnegative log likelihood: 4.201204299926758\n=========\naverage negative log likelihood, i.e. loss = 3.7693049907684326\n</pre>"},{"location":"ZeroToHero/Makemore-part1/B-Main-Notebook/#set-b-notebook","title":"SET B - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/","title":"SET B - LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#part-2-the-neural-network-approach-intro","title":"PART 2: the neural network approach: intro","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-010257","title":"Timestamp: 01:02:57","text":"<p>(Start from 01:03:40) Now in the first half of this, we arrived at the model doing everything explicitly. We were performing counts and we were normalizing those counts.  Now, we'll be doing an alternative approach but the final output will be the same.</p> <p>Here we are going to cast the problem of Bigram Character level language modelling into a neural network</p> <p>So our NN will still be a character level language model.</p> <p>So we have an input character -&gt; given to the neural network and then it is gonna predict the probability -&gt; of the next character that is likely to follow. </p> <p>And in addition to that, we are going to be able to evaluate any setting of the parameters of the langauage model, because we have a loss function value (The NLL).</p> <p>So we are going to look at the probability distributions and we are going to look at its labels (in the NN) which are basically the identity of the next character in the Bigram. </p> <p>So knowing what character comes next is the bigram, allows us to check what will be the probability value assigned to that character (So higher the value, the better. Because it is another way of saying that the loss is low).</p> <p>We're gonna use gradient based optimization to tune the parameters of this network. Because we have a loss function and we're gonna minimize it.  We're gonna tune the weights, so that the NN is gonna correctly predict the next probability of the next characters.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#creating-the-bigram-dataset-for-the-neural-net","title":"Creating the bigram dataset for the neural net","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-010526","title":"Timestamp: 01:05:26","text":"<p>We have created a training dataset. Where we have integer representations of the letter from a word/name. <pre><code>. e \ne m \nm m \nm a \na .\n\ntensor([ 0, 5, 13, 13, 1])\n\ntensor([ 5, 13, 13, 1, 0])\n</code></pre></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#feeding-integers-into-neural-nets-one-hot-encodings","title":"Feeding integers into neural nets? one-hot encodings","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-011001","title":"Timestamp: 01:10:01","text":"<p>Now we can't just directly feed those integer values into the NN. As we have seen before, for each neuron we have these certain x values and some weights w which are multiplied to it, so it doesn't really make sense to add directly add those integer values that we found into it.</p> <p>Instead what we are going to do is to follow a method called 'One-Hot code encoding' (For our case this is a common way to encode integers).</p> <p>So in One-Hot Code encoding, for example (consider the code snippet in the previous chapter) in case of 13, we basically create a vector, where the entire values are 0, except the position where 13 is which will turn to 1.</p> <p>PyTorch has a 'ONE_HOT' function - Documentation</p> <ul> <li> <p>So it takes inputs as integers in tensor</p> </li> <li> <p>And how long you want your vector to be (The length or number of elements) in num_classes</p> </li> </ul> <p>IMP: When we feed inputs into Neural Nets, we don't want them to be integers, they must be floating point numbers that can take on various values.</p> <ul> <li> <p>By default the datatype of the vector that we create (encoded value we call it, xenc (x encoded)), in PyTorch is integer and there is no explicit mention of the dtype in the one-hot function (unlike TensorFlow where we can mention it).</p> </li> <li> <p>So here we make sure that we type case the dtype of the output encoded vector to float type because by default it is in integer.</p> </li> </ul> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#the-neural-net-one-linear-layer-of-neurons-implemented-with-matrix-multiplication","title":"The \"neural net\": one linear layer of neurons implemented with matrix multiplication","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-011353","title":"Timestamp: 01:13:53","text":"<p>Here we are multiplying the input values to the weights. We are performing matrix multiplication (<code>@</code> is used for matrix multiplication in PyTorch) <code>(5, 27) x (27, 27) -&gt; (5, 27)</code></p> <p>We also use <code>rand()</code> while generating the weights, which actually follows normal distribution for selecting the values within a certain range.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#transforming-neural-net-outputs-into-probabilities-the-softmax","title":"Transforming neural net outputs into probabilities: the softmax","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-011846","title":"Timestamp: 01:18:46","text":"<p>In this Neural Network, we have the 27 inputs, 27 weights and that's it. We are only going to be doing the w times x.</p> <ul> <li> <p>It wont be having any bias b</p> </li> <li> <p>It wont be having non-linearity like tanh. We are gonna leave them as a linear layer.</p> </li> <li> <p>And there won't be more additional layers</p> </li> </ul> <p>It is gonna be the most simplest, dumest NN which is just a single linear layer.</p> <p>Now we are essentially trying to predict the probability of the next occurring character in the input.</p> <p>For the output, we are obviously trying to achieve what we did in A-Main, where (that blue matrix table) each cell had a count of the prob of the character to occur next.</p> <ul> <li> <p>In NN, we cannot output integers. So we will be calculating the Log Counts (a.k.a. logits) instead and then exponentiate them.</p> </li> <li> <p>So when we exponentiate them, the negative value numbers will turn into values &lt;1 and the positive value numbers will turn into more positive. So now we have values withing a specific range.</p> </li> <li> <p>Finally, for those values we normalize them to get the probability distributions (As we had done in A-main with <code>keepdims=True</code>)</p> </li> </ul> <p>Now as we tune the weights <code>W</code> we can control the probability value coming out. So the aim is to find a good <code>W</code> such that the probability distribution is pretty good and the way we measure the \"pretty good\" is by the loss function.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#summary-preview-to-next-steps-reference-to-micrograd","title":"Summary, preview to next steps, reference to micrograd","text":""},{"location":"ZeroToHero/Makemore-part1/B-main-makemore-part1/#timestamp-012617","title":"Timestamp: 01:26:17","text":"<p>Did a breakdown of the entire process and Andrej also wrote this piece of code where we see the step by step breakdown of one particular word.</p> <p>Even saw how we have implemented the Softmax Layer into this.  <pre><code>counts = logits.exp() # counts, equivalent to N\n\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n\n# btw: these 2 lines here are together called a 'softmax'\n</code></pre></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-Main-Notebook/","title":"Jupyter Notebook","text":"In\u00a0[1]: Copied! <pre>words = open('names.txt', 'r').read().splitlines()\n</pre> words = open('names.txt', 'r').read().splitlines() In\u00a0[2]: Copied! <pre>import torch\n\nN = torch.zeros((27, 27), dtype = torch.int32)\n\nchars = sorted(list(set(''.join(words))))\n\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\n\nitos = {i:s for s,i in stoi.items()}\n</pre> import torch  N = torch.zeros((27, 27), dtype = torch.int32)  chars = sorted(list(set(''.join(words))))  stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0  itos = {i:s for s,i in stoi.items()} In\u00a0[3]: Copied! <pre>P = N.float()\nP /= P.sum(1, keepdim=True)\n</pre> P = N.float() P /= P.sum(1, keepdim=True) In\u00a0[4]: Copied! <pre>#Creating the training set of bigrams (x,y)\nxs, ys = [], []\n\nfor word in words[:1]:\n    chs = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        print(ch1, ch2)\n        xs.append(ix1)\n        ys.append(ix2)\n\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n</pre> #Creating the training set of bigrams (x,y) xs, ys = [], []  for word in words[:1]:     chs = ['.'] + list(word) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         ix1 = stoi[ch1]         ix2 = stoi[ch2]         print(ch1, ch2)         xs.append(ix1)         ys.append(ix2)  xs = torch.tensor(xs) ys = torch.tensor(ys) <pre>. e\ne m\nm m\nm a\na .\n</pre> In\u00a0[5]: Copied! <pre>#Feeding these examples into a neural network\nimport torch.nn.functional as F\n</pre> #Feeding these examples into a neural network import torch.nn.functional as F In\u00a0[\u00a0]: Copied! <pre>#&lt;=========OPTIMIZATION============&gt;\n</pre> #&lt;=========OPTIMIZATION============&gt; In\u00a0[6]: Copied! <pre>xs\n</pre> xs Out[6]: <pre>tensor([ 0,  5, 13, 13,  1])</pre> In\u00a0[7]: Copied! <pre>ys\n</pre> ys Out[7]: <pre>tensor([ 5, 13, 13,  1,  0])</pre> In\u00a0[12]: Copied! <pre># randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True) #Adding the third parameter here for the Backward pass (as remember in micrograd we had done the same thing)\n</pre> # randomly initialize 27 neurons' weights. each neuron receives 27 inputs g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g, requires_grad=True) #Adding the third parameter here for the Backward pass (as remember in micrograd we had done the same thing) In\u00a0[13]: Copied! <pre>#FORWARD PASS\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\nloss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL)\n</pre> #FORWARD PASS xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding logits = xenc @ W # predict log-counts counts = logits.exp() # counts, equivalent to N probs = counts / counts.sum(1, keepdims=True) # probabilities for next character loss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL) In\u00a0[\u00a0]: Copied! <pre>loss #This will be similar to the one we also calculated in the SUMMARY part of B-Main\n</pre> loss #This will be similar to the one we also calculated in the SUMMARY part of B-Main Out[\u00a0]: <pre>tensor(3.7693)</pre> In\u00a0[14]: Copied! <pre>#BACKWARD PASS\nW.grad = None #the gradient is first set to zero\nloss.backward()\n</pre> #BACKWARD PASS W.grad = None #the gradient is first set to zero loss.backward() In\u00a0[15]: Copied! <pre>W.grad.shape\n</pre> W.grad.shape Out[15]: <pre>torch.Size([27, 27])</pre> In\u00a0[\u00a0]: Copied! <pre>W.grad\n</pre> W.grad In\u00a0[\u00a0]: Copied! <pre>#UPDATE\nW.data += -0.1 * W.grad\n</pre> #UPDATE W.data += -0.1 * W.grad In\u00a0[\u00a0]: Copied! <pre>#JUST PUTTING THEM TOGETHER TO PERFORM GRADIENT DESCENT\n</pre> #JUST PUTTING THEM TOGETHER TO PERFORM GRADIENT DESCENT In\u00a0[\u00a0]: Copied! <pre>#ONLY RUN THIS THE FIRST TIME\n# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True) #Adding the third parameter here for the Backward pass (as remember in micrograd we had done the same thing)\n</pre> #ONLY RUN THIS THE FIRST TIME # randomly initialize 27 neurons' weights. each neuron receives 27 inputs g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g, requires_grad=True) #Adding the third parameter here for the Backward pass (as remember in micrograd we had done the same thing) In\u00a0[34]: Copied! <pre>#FORWARD PASS\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\nloss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL)\n</pre> #FORWARD PASS xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding logits = xenc @ W # predict log-counts counts = logits.exp() # counts, equivalent to N probs = counts / counts.sum(1, keepdims=True) # probabilities for next character loss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL) In\u00a0[35]: Copied! <pre>print(loss.item()) #CHECKING THE LOSS VALUE\n</pre> print(loss.item()) #CHECKING THE LOSS VALUE <pre>3.6891887187957764\n</pre> In\u00a0[32]: Copied! <pre>#BACKWARD PASS\nW.grad = None #the gradient is first set to zero\nloss.backward()\n</pre> #BACKWARD PASS W.grad = None #the gradient is first set to zero loss.backward() In\u00a0[33]: Copied! <pre>#UPDATE\nW.data += -0.1 * W.grad\n</pre> #UPDATE W.data += -0.1 * W.grad <p>Yay, that worked. Noice</p> <p>PUTTING THEM ALL TOGETHER</p> In\u00a0[36]: Copied! <pre># create the dataset\nxs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n# initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n</pre> # create the dataset xs, ys = [], [] for w in words:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     xs.append(ix1)     ys.append(ix2) xs = torch.tensor(xs) ys = torch.tensor(ys) num = xs.nelement() print('number of examples: ', num)  # initialize the 'network' g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g, requires_grad=True) <pre>number of examples:  228146\n</pre> In\u00a0[37]: Copied! <pre># gradient descent\nfor k in range(20):\n  \n  # forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n  logits = xenc @ W # predict log-counts\n  counts = logits.exp() # counts, equivalent to N\n  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n  print(loss.item())\n  \n  # backward pass\n  W.grad = None # set to zero the gradient\n  loss.backward()\n  \n  # update\n  W.data += -50 * W.grad\n</pre> # gradient descent for k in range(20):      # forward pass   xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding   logits = xenc @ W # predict log-counts   counts = logits.exp() # counts, equivalent to N   probs = counts / counts.sum(1, keepdims=True) # probabilities for next character   loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()   print(loss.item())      # backward pass   W.grad = None # set to zero the gradient   loss.backward()      # update   W.data += -50 * W.grad <pre>3.7686190605163574\n3.378804922103882\n3.1610896587371826\n3.0271859169006348\n2.9344847202301025\n2.867231607437134\n2.816654920578003\n2.777147054672241\n2.7452545166015625\n2.7188305854797363\n2.6965057849884033\n2.6773722171783447\n2.6608052253723145\n2.6463513374328613\n2.633665084838867\n2.622471332550049\n2.6125471591949463\n2.6037065982818604\n2.595794439315796\n2.5886802673339844\n</pre> <p>SO WE ALMOST ACHIEVED A VERY LOW LOSS VALUE. SIMILAR TO THE LOSS VALUE WE CALCULATED IN A-MAIN, WHEN WE TYPED OUR OWN NAME AND SAW HOW IT PERFORMS</p> <p>Finally drumrolls, we are going to see how sampling from this model produces the outputs (Spoiler alert: it will be the same as how we made the model manually, coz... it is the same model just that we made it using Neural nets)</p> In\u00a0[38]: Copied! <pre># finally, sample from the 'neural net' model\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    \n    # ----------\n    # BEFORE:\n    #p = P[ix]\n    # ----------\n    # NOW:\n    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n    logits = xenc @ W # predict log-counts\n    counts = logits.exp() # counts, equivalent to N\n    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n    # ----------\n    \n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n</pre> # finally, sample from the 'neural net' model g = torch.Generator().manual_seed(2147483647)  for i in range(5):      out = []   ix = 0   while True:          # ----------     # BEFORE:     #p = P[ix]     # ----------     # NOW:     xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()     logits = xenc @ W # predict log-counts     counts = logits.exp() # counts, equivalent to N     p = counts / counts.sum(1, keepdims=True) # probabilities for next character     # ----------          ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()     out.append(itos[ix])     if ix == 0:       break   print(''.join(out)) <pre>juwjde.\njanaqah.\npxzfby.\na.\nnn.\n</pre>"},{"location":"ZeroToHero/Makemore-part1/C-Main-Notebook/#set-c-notebook","title":"SET C - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/","title":"SET C - LECTURE NOTES","text":"<p>Now we are gonna fine-tune the W to optimize/minimize the loss, so we find a good value of W using gradient based optimization | Timestamp: 01:33:13 </p> <p>Recap of micrograd, revised the forward pass, backward pass and the calculation of the loss value. </p> <p>Note: In Micrograd, we had built everything from scratch. The Value object, Calculation functions (for forward and backward) passes. Here we'll be using PyTorch itself, so it's a direct syntactical application.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#vectorized-loss","title":"Vectorized loss","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#timestamp-013549","title":"Timestamp: 01:35:49","text":"<p>First in the forward pass, we have to calculate the loss value. Now, unlike in Micrograd where we used Mean Squared Error, here we'll use the Negative Log Likelihood. Because there is was a Regression problem, now it is a classification one.</p> <p>Here we are basically seeing what is the probability value that the model assigns to the next occurring character in the sequence. So <code>xs</code> is the first character and <code>ys</code> is the next occurring character, so we check the probability value according to that.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#backward-and-update-in-pytorch","title":"Backward and update, in PyTorch","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#timestamp-013836","title":"Timestamp: 01:38:36","text":"<p>Backward: We calculated the backward pass. First we get the grad values to zero and then backpropagate through (Using PyTorch, so it is a lot more easier). Finally we get the W.grad values. Now those values (essentially weights) tells us how much influence they have on the final output loss value. So if it is positive and we add more to it, the loss value will increase.</p> <p>Update: Finally, here we just update the W.data values, basically nudging them slightly to decrease the loss value.</p> <p>Then finally we can perform the gradient descent cycle. After we update the value, when we do forward pass again, it should slightly decrease the loss value.  So when we achieve low loss, it means that the NN is assigning high probabilities to the next correct character.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#putting-everything-together","title":"Putting everything together","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#timestamp-014255","title":"Timestamp: 01:42:55","text":"<p>We put all of the codes and process we have done right now all together. We can see how it is more efficient to perform these steps using a neural network. </p> <p>Here is it a very simple problem, as we are only predicting the next character (so there is only 2 in total) but when it increases, this entire step actually almost remains the same! </p> <p>Just that the way we calculate the <code>logits</code> in forward pass changes!</p> <pre><code># create the dataset\n\nxs, ys = [], []\n\nfor w in words:\n\n\u00a0 chs = ['.'] + list(w) + ['.']\n\n\u00a0 for ch1, ch2 in zip(chs, chs[1:]):\n\n\u00a0 \u00a0 ix1 = stoi[ch1]\n\u00a0 \u00a0 ix2 = stoi[ch2]\n\u00a0 \u00a0 xs.append(ix1)\n\u00a0 \u00a0 ys.append(ix2)\n\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n# initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n</code></pre> <pre><code># gradient descent\n\nfor k in range(20):\n\n\u00a0 # forward pass\n\u00a0 xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n\n\u00a0 logits = xenc @ W # predict log-counts #THIS STEP HERE!\n\u00a0 counts = logits.exp() # counts, equivalent to N\n\u00a0 probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n\n\u00a0 loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n\n\u00a0 print(loss.item())\n\n\u00a0 # backward pass\n\u00a0 W.grad = None # set to zero the gradient\n\u00a0 loss.backward()\n\n\u00a0 # update\n\u00a0 W.data += -50 * W.grad\n</code></pre> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#notes","title":"Notes","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#note-1-one-hot-encoding-really-just-selects-a-row-of-the-next-linear-layers-weight-matrix-timestamp-014749","title":"Note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix | Timestamp: 01:47:49","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#note-2-model-smoothing-as-regularization-loss-timestamp-015018","title":"Note 2: model smoothing as regularization loss | Timestamp: 01:50:18","text":"<p>So the above 2 chapters were like these additional notes where he compares how the steps we followed during the manual steps is almost exactly similar to the NN approach. Pretty cool, but I guess I'll understand it a lot better if I watch it a couple of more times.</p> <p>But the second chapter (1:50:18) he had done a step called 'Regularization loss' where he added like this additional line to our NLL calculation (This is already added in our above code) <code>loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()</code></p> <p>This part: <code>+ 0.01*(W**2).mean()</code> This basically helped with the smoothening (Like how we did in the manual process of adding value to N, either 1,2 or 1000. So they all become uniform/smoothened)</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#sampling-from-the-neural-net","title":"Sampling from the neural net","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#timestamp-015431","title":"Timestamp: 01:54:31","text":"<p>Finally here we just saw how sampling from this model produces the outputs (Spoiler alert: it will be the same as how we made the model manually, coz... it is the same model just that we made it using Neural nets) :)</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#conclusion","title":"Conclusion","text":""},{"location":"ZeroToHero/Makemore-part1/C-main-makemore-part1/#timestamp-015616","title":"Timestamp: 01:56:16","text":"<p>We introduced the bigram character level language model. We saw how we could: Train the model, Sample from the model and Evaluate the quality of the model using the Negative Log Likelihood (NLL) loss. We actually trained the model in two completely different ways that actually gave the same result (and the same model) In the first way, we just counted up the frequency of all the bigrams and normalized. In the second way, we used the NLL loss as a guide to optimizing the counts matrix(The blue table matrix)/counts array so that the loss is minimized in a gradient based framework. We saw that both of them gave the same result.</p>"},{"location":"ZeroToHero/Makemore-part2/","title":"Makemore Part 2","text":""},{"location":"ZeroToHero/Makemore-part2/#language-model-2","title":"LANGUAGE MODEL - 2","text":"<p>Timeline: 26th November - 11th December, 2024</p>"},{"location":"ZeroToHero/Makemore-part2/#introduction","title":"Introduction","text":"<p>Welcome to my documentation for Makemore Part 2 from Andrej Karpathy's Neural Networks: Zero to Hero series. This section focuses on implementing a Multilayer Perceptron (MLP) as a character-level language model. Here, I\u2019ve compiled my notes and insights from the lecture to serve as a reference for understanding the key concepts and practical implementations discussed.</p>"},{"location":"ZeroToHero/Makemore-part2/#overview-of-makemore-part-2","title":"Overview of Makemore Part 2","text":"<p>In this part of the series, I explored the following topics:</p> <p>Implementing a Multilayer Perceptron (MLP): The MLP architecture is fundamental in neural networks, and this lecture provided a hands-on approach to understanding how multiple layers can learn complex data representations.</p> <p>Key Concepts Covered:</p> <ul> <li>Model training techniques</li> <li>Learning rate tuning strategies</li> <li>Hyperparameter adjustments</li> <li>Evaluation metrics including loss functions and accuracy</li> <li>Insights into overfitting and underfitting</li> </ul>"},{"location":"ZeroToHero/Makemore-part2/#key-resources","title":"Key Resources","text":"<p>Video Lecture</p> <ul> <li>I watched the lecture on YouTube: Building Makemore Part 2</li> </ul> <p>Codes:</p> <ul> <li>The Jupyter notebooks and code implementations are available within this documentation itself.</li> <li>If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Language Model 2</li> </ul>"},{"location":"ZeroToHero/Makemore-part2/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li>The lecture documentation has been divided into 3 sets: Set A, Set B, and Set C.</li> <li>Each set has its own notes and notebook.</li> <li>Notes have been marked with timestamps to the video.</li> <li>This allows for simplicity and better understanding, as the lecture is long.</li> </ul> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/","title":"SET A - LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#introduction","title":"Introduction","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-000000","title":"Timestamp: 00:00:00","text":"<p>In the previous lecture, we had implemented a Bigram character level language model where we took one character and tried to predict the next one. This was all and good if we just wanted to predict two different characters. But we saw that it didn't do very well when we tried to predict words out of it, plus we only implemented a single layer of neuron. Now, if we go with the same approach (where we did counts and build a graph matrix), for each character the number of matrix rows and columns will increase i.e. from 27x27 to 27x27x27 and so on. </p> <p>So now we will be moving on to another model called MLP (Multi Layer Perceptron).</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#bengio-et-al-2003-mlp-language-model-paper-walkthrough","title":"Bengio et al. 2003 (MLP language model) paper walkthrough.","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-000148","title":"Timestamp: 00:01:48","text":"<p>Paper Review: (Now this obviously wasn't the first paper to introduce this, but it was definitely the most influential one) Paper Link</p> <p>Now, in the paper they have proposed a word level language model, but we will be implementing it for characters itself - so same approach but as a character level language model.</p> <p>The modelling approach suggested in the paper is also identical: We use a multi layer NN to predict the next word from the previous one. And we try to maximize the negative log likelihood of the training data.</p> <p>They are basically proposing a vector dimensional space (3D Space - You can revisit the imagery from here ) where the words most related to each other will be close by. So during testing if the model encounters a sentence which it may not have been trained on, it can still relate to the other words and complete the sentence. So within that embedded space, there is knowledge exchange and outcome is produced. </p> <p>First Explanation of the diagram 5:42 with an Overview (Will have to comeback to this as I progress through the lecture, there were some imagery/explanation which I couldn't grasp completely)</p> <p>Update: Yeah it all makes sense now lol</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#re-building-our-training-dataset","title":"(re-)building our training dataset","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-000903","title":"Timestamp: 00:09:03","text":"<p>We are preparing our dataset. Its the same <code>name.txt</code> file we used before.  We have made a slight change to how we formatted the dataset (The &lt; S &gt; and &lt; E &gt;), so here we are adding a <code>block size</code> which represents 'How many characters should we consider for predicting the next one'</p> <p>We've used 3 to follow the diagram in the research paper, the 3 different inputs present horizontally in the diagram at the bottom represent that. View page 6 for the diagram.</p> <p>Prepared the X and Y values.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#implementing-the-embedding-lookup-table","title":"Implementing the embedding lookup table","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-001219","title":"Timestamp: 00:12:19","text":"<p>(Basically showing a broken down alternative way of implementing this, but ultimately the point is to show how simple and direct it is to do indexing in PyTorch)</p> <p>In the diagram, we are basically implementing the 'Look up table in C'. So we have 27 possible characters and we're gonna embed them in a lower dimensional space. In the paper, they had taken 17000 words and crammed it into a 30 dimensional space. So, we'll be doing something like taking 27 characters and cramming them into a 2 dimensional space.</p> <p>This lookup table <code>C</code> will be random numbers, which will have 27 rows and 2 columns. So each one of the 27 characters will have a 2 dimensional embedding: <code>C = torch.randn((27,2))</code></p> <p>Now, if you look at the diagram, we are indexing each word (our case character) into the look up table <code>C</code>. Ultimately, you can see that entire structure as one layer of the NN (the first layer)</p> <p>So, adding that character to the look-up table is called INDEXING. There is also the method of one-hot encoding them, but we'll be discarding that as its simpler and much faster to do indexing.</p> <p>So long story short, in order to embed all of <code>X</code> (our 27 characters in 2 dimension) into <code>C</code>, we simply do <code>C[X]</code></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#implementing-the-hidden-layer-internals-of-torchtensor-storage-views","title":"Implementing the hidden layer + internals of torch.Tensor: storage, views","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-001835","title":"Timestamp: 00:18:35","text":"<p>Now we try to build the hidden layer. Here we consider the size of the embedding layer <code>C[X].shape</code> is <code>torch.Size([32, 3, 2])</code></p> <p>So we have 2 dimensional embedding layers and there are 3 of them. (Just consider the diagram itself, the 2D ones are the red circles and the 3 of them are the 3 rectangles)</p> <p>The hidden layer, we will consider as <code>W1</code> initializing with a bunch of random numbers. So taking that 2D in 3, we take 6. And for the number of neurons in the hidden layer we can consider any number of our choice, so we take 100. <code>W1 = torch.randn((6, 100))</code></p> <p>And we add bias to it <code>b1 = torch.randn(100)</code></p> <p>Now, normally we would wanna matrix multiply the embeddings with the weights in the hidden layer and add bias to it <code>emb @ W1 + b1</code> (Note: <code>emb</code> is basically <code>C[X]</code>. So, <code>emb = C[X]</code>)</p> <p>But we can't do that because the shape of <code>emb</code> is <code>[32, 3, 2]</code> and our <code>W1</code> is <code>[6, 100]</code>. So we need to somewhat, concatenate all 3 of those into one, so we get 3x2 i.e. 6.</p> <p>So those 3 different boxes that we have, we want to concatenate all of there values into one. And this is where we use different functions provided by PyTorch.</p> <p>In PyTorch concatenate function <code>torch.cat</code> we have to add the embedding values and then mention to which dimension you want to concatenate them to, hence we are adding that 1 in <code>torch.cat(----, 1)</code></p> <p>Now instead of adding the embeddings one by one like <code>torch.cat([ emb[:,0,:], emb[:, 1, :], emb[:, 2, :]], 1)</code></p> <p>we use this torch function called <code>unbind</code> which basically returns such a list. So we do <code>torch.unbind(emb, 1)</code>. Here also we are mentioning the dimension of each of those values (We are looping through this basically)</p> <p>So finally we get <code>torch.size( torch.unbind(emb, 1) ,1)</code></p> <p>But it turns out, even that is not very efficient, as for unbind we are using like a whole different set of memory.</p> <p>So resolve this, we will be converting the shape of it using PyTorch. So in PyTorch we have something called <code>.view</code> where we can change the dimensions as we want. So if the total elements is 18, we can view it as 9x2, 3x3x2, 6x3 anything. The reason is, PyTorch basically puts all the elements in its memory as a single dimensional array i.e. from 0 to 17 in our example. So as long as it's total number of elements remain the same, we can always ask PyTorch to view it in a different shape.</p> <p>So, instead we go back to the original matrix multiplication equation <code>emb @ W1 + b1</code> We simply just convert the shape of the embedding to match that of W1 for the multiplication, by simply asking PyTorch to view it differently. <code>emb.view(32, 6) @ W1 + b1</code></p> <p>Now we don't want to hardcode the value 32 and make it more dynamic, so we instead add <code>emb.view(shape[0], 6) @ W1 + b1</code></p> <p>or to make it even more efficient we do <code>emb.view(-1, 6) @ W1 + b1</code>. So when we had <code>-1</code>, PyTorch will itself know that it needs to look for the size and add there.</p> <p>And finally, since this is the hidden tanh layer, we implement that as well, so <code>h = torch.tanh(emb.view(-1, 6) @ W1 + b1)</code></p> <p>AND THAT'S OUR HIDDEN LAYER!</p> <p>(psst.. fun flashback. In the equation we also have the addition of biases to the weights before matrix multiplying them. W1 is <code>[6, 100]</code> and b1 is <code>100</code>, so here broadcasting is happening! so its <code>[1, 100]</code> for b1)</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#implementing-the-output-layer","title":"Implementing the output layer","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-002915","title":"Timestamp: 00:29:15","text":"<p>Now finally lets implement our final layer. So we assign W2 and b2. So W2 takes the input of 100 neurons from the hidden layer and we need the output as 27 as we have 27 characters and the bias is also set to 27.</p> <pre><code>W2 = torch.randn((100, 27))\nb2 = torch.randn(27)\n</code></pre> <p>And finally we calculate the <code>logits</code> which is the output of the final layer <pre><code>logits = h @ W2 + b2\n</code></pre></p> <p>So finally, our output layer (logits) dimension will be <code>[32, 27]</code></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#implementing-the-negative-log-likelihood-loss","title":"Implementing the negative log likelihood loss","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-002953","title":"Timestamp: 00:29:53","text":"<p>Now as we've seen in the part 1 of makemore, we need to take those logits values and first exponentiate them to get our \"fake counts\" and then normalize them to get the probability.</p> <pre><code>counts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\n</code></pre> <p>Now, we have the final piece of this equation, the <code>Y</code> for predicting the next possible character.</p> <p>So now we need to index Y into prob. Now first we need to label Y (i.e. its positioning 0 to 31) so we use <code>torch.arange(32)</code> and label them respectively to Y's values.</p> <p>torch.arange(32) -&gt; 0, 1, 2, 3 ..... 32 Y -&gt; 5, 13, 13, 1 ..... 0</p> <p><pre><code>prob[torch.arange(32), Y]\n</code></pre> So now we've indexed Y into the prob</p> <p>Then we find the log value of it, then its mean and the negative of it to finally get the negative log likelihood value, which is basically our loss value.</p> <pre><code>loss = -prob[torch.arange(32), Y].log().mean()\n</code></pre> <p>So this is the <code>loss</code> value that we would like to minimize, so that we can get the network the predict the next character of the sequence correctly.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#summary-of-the-full-network","title":"Summary of the full network","text":""},{"location":"ZeroToHero/Makemore-part2/A-main-makemore-part2/#timestamp-003217","title":"Timestamp: 00:32:17","text":"<p>Now we're just putting them altogether (To make it more respectable lol)</p> <pre><code>X.shape, Y.shape #dataset\n</code></pre> <pre><code>g = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,2), generator=g)\nW1 = torch.rand((6, 100), generator=g)\nb1 = torch.rand(100, generator=g)\nW2 = torch.rand((100, 27), generator=g)\nb2 = torch.rand(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</code></pre> <pre><code>sum(p.nelement() for p in parameters) #to check number of parameters in total\n</code></pre> <pre><code>emb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = - prob[torch.arange(32), Y].log().mean()\nloss\n</code></pre> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/A-main-notebook/","title":"Jupyter Notebook","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() words[:8] Out[2]: <pre>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</pre> In\u00a0[3]: Copied! <pre>len(words)\n</pre> len(words) Out[3]: <pre>32033</pre> In\u00a0[3]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nprint(itos)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} print(itos) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n</pre> In\u00a0[13]: Copied! <pre># build the dataset\n\nblock_size = 3 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words[:5]:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \nX = torch.tensor(X)\nY = torch.tensor(Y)\n</pre> # build the dataset  block_size = 3 # context length: how many characters do we take to predict the next one? X, Y = [], [] for w in words[:5]:      #print(w)   context = [0] * block_size   for ch in w + '.':     ix = stoi[ch]     X.append(context)     Y.append(ix)     print(''.join(itos[i] for i in context), '---&gt;', itos[ix])     context = context[1:] + [ix] # crop and append    X = torch.tensor(X) Y = torch.tensor(Y) <pre>... ---&gt; e\n..e ---&gt; m\n.em ---&gt; m\nemm ---&gt; a\nmma ---&gt; .\n... ---&gt; o\n..o ---&gt; l\n.ol ---&gt; i\noli ---&gt; v\nliv ---&gt; i\nivi ---&gt; a\nvia ---&gt; .\n... ---&gt; a\n..a ---&gt; v\n.av ---&gt; a\nava ---&gt; .\n... ---&gt; i\n..i ---&gt; s\n.is ---&gt; a\nisa ---&gt; b\nsab ---&gt; e\nabe ---&gt; l\nbel ---&gt; l\nell ---&gt; a\nlla ---&gt; .\n... ---&gt; s\n..s ---&gt; o\n.so ---&gt; p\nsop ---&gt; h\noph ---&gt; i\nphi ---&gt; a\nhia ---&gt; .\n</pre> In\u00a0[6]: Copied! <pre>X.shape, X.dtype, Y.shape, Y.dtype\n</pre> X.shape, X.dtype, Y.shape, Y.dtype Out[6]: <pre>(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)</pre> <p>So our dataset looks like this^   So, for each of those above 5 words,  <code>torch.Size([32, 3])</code> we have created a dataset of 32 examples and each input of the neural net is 3 integers =&gt; X  <code>torch.Size([32])</code> and these are the labels (single row, 32 values) =&gt; Y</p> In\u00a0[13]: Copied! <pre>X\n</pre> X Out[13]: <pre>tensor([[ 0,  0,  0],\n        [ 0,  0,  5],\n        [ 0,  5, 13],\n        [ 5, 13, 13],\n        [13, 13,  1],\n        [ 0,  0,  0],\n        [ 0,  0, 15],\n        [ 0, 15, 12],\n        [15, 12,  9],\n        [12,  9, 22],\n        [ 9, 22,  9],\n        [22,  9,  1],\n        [ 0,  0,  0],\n        [ 0,  0,  1],\n        [ 0,  1, 22],\n        [ 1, 22,  1],\n        [ 0,  0,  0],\n        [ 0,  0,  9],\n        [ 0,  9, 19],\n        [ 9, 19,  1],\n        [19,  1,  2],\n        [ 1,  2,  5],\n        [ 2,  5, 12],\n        [ 5, 12, 12],\n        [12, 12,  1],\n        [ 0,  0,  0],\n        [ 0,  0, 19],\n        [ 0, 19, 15],\n        [19, 15, 16],\n        [15, 16,  8],\n        [16,  8,  9],\n        [ 8,  9,  1]])</pre> In\u00a0[14]: Copied! <pre>Y\n</pre> Y Out[14]: <pre>tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])</pre> In\u00a0[8]: Copied! <pre>C = torch.rand((27, 2))\n</pre> C = torch.rand((27, 2)) In\u00a0[9]: Copied! <pre>emb = C[X]\n\nemb.shape\n</pre> emb = C[X]  emb.shape Out[9]: <pre>torch.Size([32, 3, 2])</pre> <p>(PyTorch indexing is awesome)   To index simultaneously all the elements of X, We simply do C[X]</p> In\u00a0[10]: Copied! <pre>W1 = torch.randn((6, 100))\nb1 = torch.rand(100)\n</pre> W1 = torch.randn((6, 100)) b1 = torch.rand(100) In\u00a0[11]: Copied! <pre>h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n</pre> h = torch.tanh(emb.view(-1, 6) @ W1 + b1) In\u00a0[12]: Copied! <pre>h\n</pre> h Out[12]: <pre>tensor([[ 0.9910,  0.8405,  0.4715,  ...,  0.9999,  0.8814,  0.9998],\n        [ 0.9763,  0.9163,  0.3350,  ...,  0.9991,  0.8249,  0.9992],\n        [ 0.9791,  0.8450, -0.0272,  ...,  0.9997,  0.9230,  0.9997],\n        ...,\n        [ 0.8995,  0.6590,  0.4667,  ...,  0.9995, -0.4144,  0.9988],\n        [ 0.9777,  0.7397,  0.2623,  ...,  0.9999,  0.9593,  0.9999],\n        [ 0.9402,  0.7154,  0.2493,  ...,  0.9980, -0.6247,  0.9979]])</pre> In\u00a0[13]: Copied! <pre>h.shape\n</pre> h.shape Out[13]: <pre>torch.Size([32, 100])</pre> <p>Hidden layer is now made^</p> In\u00a0[15]: Copied! <pre>W2 = torch.randn((100, 27))\nb2 = torch.rand(27)\n</pre> W2 = torch.randn((100, 27)) b2 = torch.rand(27) In\u00a0[16]: Copied! <pre>logits = h @ W2 + b2\n</pre> logits = h @ W2 + b2 In\u00a0[17]: Copied! <pre>logits.shape\n</pre> logits.shape Out[17]: <pre>torch.Size([32, 27])</pre> In\u00a0[18]: Copied! <pre>counts = logits.exp()\n</pre> counts = logits.exp() In\u00a0[19]: Copied! <pre>prob = counts / counts.sum(1, keepdims=True)\n</pre> prob = counts / counts.sum(1, keepdims=True) In\u00a0[21]: Copied! <pre>prob.shape\n</pre> prob.shape Out[21]: <pre>torch.Size([32, 27])</pre> In\u00a0[22]: Copied! <pre>loss = -prob[torch.arange(32), Y].log().mean()\nloss\n</pre> loss = -prob[torch.arange(32), Y].log().mean() loss Out[22]: <pre>tensor(13.4043)</pre> <p>We've made the final output layer^  Found the loss function value, which we have to reduce</p> <p>Summarising what we've done so far to make this more respectable :)</p> In\u00a0[14]: Copied! <pre>#Run the first 5 cells and then start from here\nX.shape, Y.shape #dataset\n</pre> #Run the first 5 cells and then start from here X.shape, Y.shape #dataset Out[14]: <pre>(torch.Size([32, 3]), torch.Size([32]))</pre> In\u00a0[15]: Copied! <pre>g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej\nC = torch.randn((27,2), generator=g)\nW1 = torch.rand((6, 100), generator=g)\nb1 = torch.rand(100, generator=g)\nW2 = torch.rand((100, 27), generator=g)\nb2 = torch.rand(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</pre> g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej C = torch.randn((27,2), generator=g) W1 = torch.rand((6, 100), generator=g) b1 = torch.rand(100, generator=g) W2 = torch.rand((100, 27), generator=g) b2 = torch.rand(27, generator=g) parameters = [C, W1, b1, W2, b2] In\u00a0[16]: Copied! <pre>sum(p.nelement() for p in parameters) #to check number of parameters in total\n</pre> sum(p.nelement() for p in parameters) #to check number of parameters in total Out[16]: <pre>3481</pre> In\u00a0[17]: Copied! <pre>emb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = - prob[torch.arange(32), Y].log().mean()\nloss\n</pre> emb = C[X] h = torch.tanh(emb.view(-1,6) @ W1 + b1) logits = h @ W2 + b2 counts = logits.exp() prob = counts / counts.sum(1, keepdims=True) loss = - prob[torch.arange(32), Y].log().mean() loss Out[17]: <pre>tensor(6.4365)</pre>"},{"location":"ZeroToHero/Makemore-part2/A-main-notebook/#set-a-notebook","title":"SET A - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/","title":"SET B - LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#introducing-fcross_entropy-and-why","title":"Introducing F.cross_entropy and why","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#timestamp-003249","title":"Timestamp: 00:32:49","text":"<p>Now, after finding/calculating the <code>logits</code> value, instead of finding its exponent, then its probability distribution and then the negative log likelihood, we directly use PyTorch's <code>F.cross_entropy()</code> - it basically does all of those three steps, but is much more efficient.</p> <p>Also it turns out, we've only been doing those steps for academic purposes and this won't happen during the actual training of NN. So using <code>F.cross_entropy()</code> is makes forward, backward pass much more efficient (and it saves memory of doing all those 3 steps cluttered into one)</p> <p>So we have this <pre><code>emb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = - prob[torch.arange(32), Y].log().mean()\nloss\n</code></pre></p> <p>to this <pre><code>emb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Y)\nloss\n</code></pre></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#implementing-the-training-loop-overfitting-one-batch","title":"Implementing the training loop, overfitting one batch","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#timestamp-003756","title":"Timestamp: 00:37:56","text":"<p>Now we implement the training of the neural net for that set of data (we just took the first five words for now)</p> <p>Now, we already have the forward pass where we find the value of the loss <pre><code>#forward pass\nemb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Y)\nloss\n</code></pre></p> <p>Then we calculate the backward pass <pre><code>#backward pass\nfor p in parameters:\n    p.grad = None\nloss.backward()\n</code></pre></p> <p>Then we update the values <pre><code>#update\nfor p in parameters:\n    p.data += -0.1 * p.grad\n</code></pre></p> <p>Now, we want all of this in a loop, so putting them all together: <pre><code>for _ in range(100):\n    #forward pass\n    emb = C[X]\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\nprint(loss.item())\n</code></pre></p> <p>Finally, just before this entire process, we need to declare <code>requires_grad</code> parameter to <code>True</code> <pre><code>for p in parameters:\n    p.requires_grad = True\n</code></pre></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#training-on-the-full-dataset-minibatches","title":"Training on the full dataset, minibatches","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#timestamp-004125","title":"Timestamp: 00:41:25","text":"<p>First we try to train on the entire dataset, we noticed that the loss was decreasing but there was a fair bit of lag during each step. So the gradient is moving in the right direction but with much larger time and steps.</p> <p>So we split them into mini batches, and only send those sets of batches for training and then we noticed that the training is almost instant.</p> <p>This is a much better practice to follow, as \"it is much better to take the approximate gradient and make more steps; then taking the exact gradient with fewer steps.\"</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#finding-a-good-initial-learning-rate","title":"Finding a good initial learning rate","text":""},{"location":"ZeroToHero/Makemore-part2/B-main-makemore-part2/#timestamp-004540","title":"Timestamp: 00:45:40","text":"<p>So, in the update section we had just randomly guessed a value for the learning rate, we put it as <code>0.1</code>. But there is also a way to determine the best possible learning rate.</p> <p>So we set like a range of values like the exp of -3 to exp of 0 (that is basically 0.001 to 1) and then plot a graph to see where the value gets the lowest.</p> <p><pre><code>lre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n</code></pre> <pre><code>#Remember to reset the parameters and only then run this\n\nlri = []\nlossi = []\n\nfor i in range(1000):\n\n\u00a0 \u00a0 #Minibatch\n\u00a0 \u00a0 xi = torch.randint(0, X.shape[0], (32,))\n\n\u00a0 \u00a0 #forward pass\n\u00a0 \u00a0 emb = C[X[xi]]\n\u00a0 \u00a0 h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n\u00a0 \u00a0 logits = h @ W2 + b2\n\u00a0 \u00a0 loss = F.cross_entropy(logits, Y[xi])\n\u00a0 \u00a0 #print(loss.item())\n\n\u00a0 \u00a0 #backward pass\n\u00a0 \u00a0 for p in parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 p.grad = None\n\u00a0 \u00a0 loss.backward()\n\n\u00a0 \u00a0 #update\n\u00a0 \u00a0 lr = lrs[i]\n\u00a0 \u00a0 for p in parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 p.data += -0.1 * p.grad\n\n\u00a0 \u00a0 #keeping track\n\u00a0 \u00a0 lri.append(lre[i]) #We are taking the exponent of the learning rate for the x-axis\n\u00a0 \u00a0 lossi.append(loss.item())\n\nprint(loss.item())\n</code></pre> <pre><code>plt.plot(lri, lossi)\n</code></pre></p> <p>So seeing the graph, we had closely to <code>0.1</code> (although it can be lower for me as I had a lower loss value to begin with) and so we continue with that value.</p> <p>We kept on training till the loss reduces and once we feel like we were close, we made the learning rate more smaller (this is called learning rate decay).</p> <p>So this is not exactly how we do in production, but the process is the same. We decide on a learning rate, train for a while and towards the end we do like a learning rate decay and then finally we get a trained neural net!</p> <p>(Also, we have surpassed the loss value we got in the bigram model! So we've already made improvements)</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/B-main-notebook/","title":"Jupyter Notebook","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n\n\n# build the dataset\n\nblock_size = 3 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    #print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \nX = torch.tensor(X)\nY = torch.tensor(Y)\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines()   # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()}   # build the dataset  block_size = 3 # context length: how many characters do we take to predict the next one? X, Y = [], [] for w in words:      #print(w)   context = [0] * block_size   for ch in w + '.':     ix = stoi[ch]     X.append(context)     Y.append(ix)     #print(''.join(itos[i] for i in context), '---&gt;', itos[ix])     context = context[1:] + [ix] # crop and append    X = torch.tensor(X) Y = torch.tensor(Y) In\u00a0[3]: Copied! <pre>X.shape, X.dtype, Y.shape, Y.dtype\n</pre> X.shape, X.dtype, Y.shape, Y.dtype Out[3]: <pre>(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)</pre> In\u00a0[4]: Copied! <pre>g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej\nC = torch.randn((27,2), generator=g)\nW1 = torch.rand((6, 100), generator=g)\nb1 = torch.rand(100, generator=g)\nW2 = torch.rand((100, 27), generator=g)\nb2 = torch.rand(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</pre> g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej C = torch.randn((27,2), generator=g) W1 = torch.rand((6, 100), generator=g) b1 = torch.rand(100, generator=g) W2 = torch.rand((100, 27), generator=g) b2 = torch.rand(27, generator=g) parameters = [C, W1, b1, W2, b2] In\u00a0[5]: Copied! <pre>emb = C[X]\nh = torch.tanh(emb.view(-1,6) @ W1 + b1)\nlogits = h @ W2 + b2\n# counts = logits.exp()\n# prob = counts / counts.sum(1, keepdims=True)\n# loss = - prob[torch.arange(32), Y].log().mean()\nloss = F.cross_entropy(logits, Y)\nloss\n</pre> emb = C[X] h = torch.tanh(emb.view(-1,6) @ W1 + b1) logits = h @ W2 + b2 # counts = logits.exp() # prob = counts / counts.sum(1, keepdims=True) # loss = - prob[torch.arange(32), Y].log().mean() loss = F.cross_entropy(logits, Y) loss Out[5]: <pre>tensor(6.4365)</pre> In\u00a0[\u00a0]: Copied! <pre>#Setting up the training of the Neural Net\n</pre> #Setting up the training of the Neural Net In\u00a0[5]: Copied! <pre>for p in parameters:\n    p.requires_grad = True #Coz we know PyTorch asks for this parameter, as it is set to false by default\n</pre> for p in parameters:     p.requires_grad = True #Coz we know PyTorch asks for this parameter, as it is set to false by default In\u00a0[\u00a0]: Copied! <pre>for _ in range(10):\n\n    #forward pass\n    emb = C[X]\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n    print(loss)\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\n# print(loss.item())\n</pre> for _ in range(10):      #forward pass     emb = C[X]     h = torch.tanh(emb.view(-1,6) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Y)     print(loss)      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     for p in parameters:         p.data += -0.1 * p.grad  # print(loss.item()) <pre>tensor(5.9912, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(4.9723, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(4.6059, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(4.3298, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(4.1185, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(3.9586, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(3.8382, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(3.7435, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(3.6644, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(3.5960, grad_fn=&lt;NllLossBackward0&gt;)\n</pre> <p>Adding mini-batches</p> In\u00a0[10]: Copied! <pre>for _ in range(1000):\n\n    #Minibatch\n    xi = torch.randint(0, X.shape[0], (32,))\n\n    #forward pass\n    emb = C[X[xi]]  #added for X\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y[xi])   #added for Y\n    #print(loss.item())\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\nprint(loss.item())\n</pre> for _ in range(1000):      #Minibatch     xi = torch.randint(0, X.shape[0], (32,))      #forward pass     emb = C[X[xi]]  #added for X     h = torch.tanh(emb.view(-1,6) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Y[xi])   #added for Y     #print(loss.item())      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     for p in parameters:         p.data += -0.1 * p.grad  print(loss.item()) <pre>2.398618459701538\n</pre> <p>Finding a good learning rate</p> In\u00a0[11]: Copied! <pre>X.shape, X.dtype, Y.shape, Y.dtype\n</pre> X.shape, X.dtype, Y.shape, Y.dtype Out[11]: <pre>(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)</pre> In\u00a0[22]: Copied! <pre>#Everytime you wanna restart just run this to reset the parameters\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,2), generator=g)\nW1 = torch.rand((6, 100), generator=g)\nb1 = torch.rand(100, generator=g)\nW2 = torch.rand((100, 27), generator=g)\nb2 = torch.rand(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</pre> #Everytime you wanna restart just run this to reset the parameters g = torch.Generator().manual_seed(2147483647) C = torch.randn((27,2), generator=g) W1 = torch.rand((6, 100), generator=g) b1 = torch.rand(100, generator=g) W2 = torch.rand((100, 27), generator=g) b2 = torch.rand(27, generator=g) parameters = [C, W1, b1, W2, b2] In\u00a0[23]: Copied! <pre>for p in parameters:\n    p.requires_grad = True\n</pre> for p in parameters:     p.requires_grad = True In\u00a0[24]: Copied! <pre>lre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n</pre> lre = torch.linspace(-3, 0, 1000) lrs = 10**lre In\u00a0[15]: Copied! <pre>lri = []\nlossi = []\n\nfor i in range(1000):\n\n    #Minibatch\n    xi = torch.randint(0, X.shape[0], (32,))\n\n    #forward pass\n    emb = C[X[xi]]\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y[xi])\n    #print(loss.item())\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\n    #keeping track\n    lri.append(lr)\n    lossi.append(loss.item())\n\nprint(loss.item())\n</pre> lri = [] lossi = []  for i in range(1000):      #Minibatch     xi = torch.randint(0, X.shape[0], (32,))      #forward pass     emb = C[X[xi]]     h = torch.tanh(emb.view(-1,6) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Y[xi])     #print(loss.item())      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     lr = lrs[i]     for p in parameters:         p.data += -0.1 * p.grad      #keeping track     lri.append(lr)     lossi.append(loss.item())  print(loss.item()) <pre>2.419145107269287\n</pre> In\u00a0[16]: Copied! <pre>plt.plot(lri, lossi)\n</pre> plt.plot(lri, lossi) Out[16]: <pre>[&lt;matplotlib.lines.Line2D at 0x225085e9b40&gt;]</pre> <p>But we would like to see which exponent value is recommended to use, so we'll update the x-axis</p> In\u00a0[20]: Copied! <pre>#Remember to reset the parameters and only then run this\n\nlri = []\nlossi = []\n\nfor i in range(1000):\n\n    #Minibatch\n    xi = torch.randint(0, X.shape[0], (32,))\n\n    #forward pass\n    emb = C[X[xi]]\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y[xi])\n    #print(loss.item())\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\n    #keeping track\n    lri.append(lre[i]) #We are taking the exponent of the learning rate for the x-axis\n    lossi.append(loss.item())\n\nprint(loss.item())\n</pre> #Remember to reset the parameters and only then run this  lri = [] lossi = []  for i in range(1000):      #Minibatch     xi = torch.randint(0, X.shape[0], (32,))      #forward pass     emb = C[X[xi]]     h = torch.tanh(emb.view(-1,6) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Y[xi])     #print(loss.item())      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     lr = lrs[i]     for p in parameters:         p.data += -0.1 * p.grad      #keeping track     lri.append(lre[i]) #We are taking the exponent of the learning rate for the x-axis     lossi.append(loss.item())  print(loss.item()) <pre>2.705171585083008\n</pre> In\u00a0[21]: Copied! <pre>plt.plot(lri, lossi)\n</pre> plt.plot(lri, lossi) Out[21]: <pre>[&lt;matplotlib.lines.Line2D at 0x22507dc3850&gt;]</pre> <p>^Here exp of <code>-1</code> is the closest to where the loss is less, so exponent of -1 is 0.1, which was the actual value we had considered anyway</p> <p>Just to cross-check we'll directly plot that value and see</p> In\u00a0[25]: Copied! <pre>#Remember to reset the parameters and only then run this\n\nlri = []\nlossi = []\n\nfor i in range(1000):\n\n    #Minibatch\n    xi = torch.randint(0, X.shape[0], (32,))\n\n    #forward pass\n    emb = C[X[xi]]\n    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y[xi])\n    #print(loss.item())\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\n    #keeping track\n    lri.append(lrs[i]) #We are taking the exponent of the learning rate for the x-axis\n    lossi.append(loss.item())\n\nprint(loss.item())\n</pre> #Remember to reset the parameters and only then run this  lri = [] lossi = []  for i in range(1000):      #Minibatch     xi = torch.randint(0, X.shape[0], (32,))      #forward pass     emb = C[X[xi]]     h = torch.tanh(emb.view(-1,6) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Y[xi])     #print(loss.item())      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     lr = lrs[i]     for p in parameters:         p.data += -0.1 * p.grad      #keeping track     lri.append(lrs[i]) #We are taking the exponent of the learning rate for the x-axis     lossi.append(loss.item())  print(loss.item()) <pre>2.7444117069244385\n</pre> In\u00a0[26]: Copied! <pre>plt.plot(lri, lossi)\n</pre> plt.plot(lri, lossi) Out[26]: <pre>[&lt;matplotlib.lines.Line2D at 0x22507e03700&gt;]</pre> <p>Yeah <code>0.1</code> seems fair I guess lol</p>"},{"location":"ZeroToHero/Makemore-part2/B-main-notebook/#set-b-notebook","title":"SET B - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/","title":"SET C - LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#splitting-up-the-dataset-into-trainvaltest-splits-and-why","title":"Splitting up the dataset into train/val/test splits and why","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-005320","title":"Timestamp: 00:53:20","text":"<p>Now, in our case the model isn't very large (our number of parameters is like upto 3481). These models can get larger and larger as we add more parameters and neurons. So as the capacity of the neural net grows, it becomes more and more capable of overfitting the training set.  So we wont be able to generate new data and we can check that by seeing the loss on any withheld data from training, there the loss value would be too high, so its not a very good value.</p> <p>Therefore, one standard that is been followed is to splitting up the dataset into 3: Training (80%), Dev/Validation (10%) and Test splits (10%)</p> <p>Training set: Used for Optimizing the parameters of the model, using gradient descent (like how we've done so far). Therefore, this is used to train the parameters</p> <p>Dev/Validation: Used for Development, over all the hyperparameters of the model. Hyperparameters for example would be the size of the hidden layer or the size of the embedding (the first layer).Therefore, this is used to train the hyperparameters</p> <p>Test set: This is used to check/evaluating the performance of the model at the end.  Now, you are only allowed to check the loss on the test set only a few times. As each time you learn something, it is used to train in the train set. So, if you do it too much, you risk overfitting the model.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#experiment-larger-hidden-layer","title":"Experiment: larger hidden layer","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-010049","title":"Timestamp: 01:00:49","text":"<p>In order to improve the performance of the model, we increase the size of the hidden layer.  But we notice that the loss doesn't improve by that much immediately (after a few iterations as well), now there maybe various reasons to it: The increase size of the hidden layer may take sometime to converge during training or the Batch size (32 in our case) maybe too low or the input embeddings maybe too small (right now its size if 2 dimensional, so we maybe cramping in too much data in too less space).</p> <p>So in the code, we first increased the size of the tanh (hidden layer) and then ran the model again. This time there was only a tiny difference between the dev and training loss (We keep re-running the model, twice).</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#visualizing-the-character-embeddings","title":"Visualizing the character embeddings","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-010527","title":"Timestamp: 01:05:27","text":"<p>so we made a visualization of how our embeddings look like right now (sensei provided the code obviously), since they are only 2 dimensional we could plot how the individual characters are plotted out. The results looked way different than that of sensei's but mine was a lot more group? But my loss value had been low from the start so lets see.</p> <p>Next we'll increase the embedding size to see if it reduces the loss value.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#experiment-larger-embedding-size","title":"Experiment: larger embedding size","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-010716","title":"Timestamp: 01:07:16","text":"<p>We'll be changing a couple of values here (For <code>C</code> 2 will be changed to 10 (so x5 times) so for <code>W1</code> 6 will be changed to 30), and in the rest of the codes as well (Normally we wouldn't hard code this value, but in our example we're doing this - as we are still learning).</p> <p>Okay so we got these values after increasing the embedding size: dev: <code>tensor(2.2085, grad_fn=&lt;NllLossBackward0&gt;)</code> training: <code>tensor(2.1633, grad_fn=&lt;NllLossBackward0&gt;)</code></p> <p>After doing a loss decay: dev: <code>tensor(2.1091, grad_fn=&lt;NllLossBackward0&gt;)</code> training: <code>tensor(2.0482, grad_fn=&lt;NllLossBackward0&gt;)</code></p> <p>So we are indeed decreasing the loss a little bit and we got a lower value than we did before we started all these steps! (Ending of B-main, previous notes which was about <code>2.3</code> if I'm not wrong)</p> <p>Useful note: 1:10:43 to 1:11:01 -&gt; On how you consider a certain loss value before adding it officially to a research paper.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#summary-of-our-final-code-conclusion","title":"Summary of our final code, conclusion","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-011146","title":"Timestamp: 01:11:46","text":"<p>Updated in the notebook: - Not much changes to what we have done so far, but just some code improvement for the lr value to change based on the iterations. - Here basically we are open to experimenting with different values, whether it is the inputs, size of the layers or the loss rate values to see how we can decrease the final loss value. - Ran for about 1 minute 55.5 seconds! (Longest so far lol)</p> <p>I think the loss values were decent: dev <code>2.1294</code>, train <code>2.1677</code></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#sampling-from-the-model","title":"Sampling from the model","text":""},{"location":"ZeroToHero/Makemore-part2/C-main-makemore-part2/#timestamp-011324","title":"Timestamp: 01:13:24","text":"<pre><code># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n\n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n\n    print(''.join(itos[i] for i in out))\n</code></pre> <p>We are going to be working with with 20 samples <code>for _ in range(20)</code></p> <p>First we begin with all dots (...) so that's the context <code>context = [0] * block_size</code></p> <p>So the <code>while</code> loop goes on until we generate the 0th character <code>ix == 0</code></p> <p>Inside the loop,</p> <p>First we are embedding the context in the embedding table <code>C</code> and here the first dimension is the size of the training set, but here we are considering only one example for simplicity <code>emb = C[torch.tensor([context])] # (1,block_size,d)</code></p> <p>That value gets projected into <code>h</code> then we calculate the <code>logits</code> and then the probability.</p> <p>For <code>probs</code> we are using softmax, we add that to logits, so that basically exponentiates the logits and makes them sum to 1. So similar to cross-entropy, softmax is careful that there is no overflows.</p> <p>Then we sample them by using <code>torch.multinomial</code>, <code>ix = torch.multinomial(probs, num_samples=1, generator=g).item()</code> to get our next index.</p> <p>Then we shift the context window to append the index <pre><code>context = context[1:] + [ix]\n      out.append(ix)\n</code></pre></p> <p>And then finally we decode all the integers to strings and print them out <code>print(''.join(itos[i] for i in out))</code></p>"},{"location":"ZeroToHero/Makemore-part2/C-main-notebook/","title":"Jupyter Notebook","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines()   # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} In\u00a0[3]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  for w in words:\n\n    #print(w)\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      #print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []   for w in words:      #print(w)     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       #print(''.join(itos[i] for i in context), '---&gt;', itos[ix])       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr, Ytr = build_dataset(words[:n1]) Xdev, Ydev = build_dataset(words[n1:n2]) Xte, Yte = build_dataset(words[n2:]) <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[4]: Copied! <pre>Xtr.shape, Ytr.shape #dataset\n</pre> Xtr.shape, Ytr.shape #dataset Out[4]: <pre>(torch.Size([182625, 3]), torch.Size([182625]))</pre> In\u00a0[20]: Copied! <pre>g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej\nC = torch.randn((27,10), generator=g)\nW1 = torch.rand((30, 300), generator=g)\nb1 = torch.rand(300, generator=g)\nW2 = torch.rand((300, 27), generator=g)\nb2 = torch.rand(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</pre> g = torch.Generator().manual_seed(2147483647) #For consistency ofcourse, to keep the same values as andrej C = torch.randn((27,10), generator=g) W1 = torch.rand((30, 300), generator=g) b1 = torch.rand(300, generator=g) W2 = torch.rand((300, 27), generator=g) b2 = torch.rand(27, generator=g) parameters = [C, W1, b1, W2, b2] In\u00a0[21]: Copied! <pre>sum(p.nelement() for p in parameters) # number of parameters in total\n</pre> sum(p.nelement() for p in parameters) # number of parameters in total Out[21]: <pre>17697</pre> In\u00a0[22]: Copied! <pre>for p in parameters:\n    p.requires_grad = True\n</pre> for p in parameters:     p.requires_grad = True In\u00a0[8]: Copied! <pre>lre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n</pre>  lre = torch.linspace(-3, 0, 1000) lrs = 10**lre In\u00a0[30]: Copied! <pre>lri = []\nlossi = []\nstepi = []\n\nfor i in range(40000):\n\n    #Minibatch\n    xi = torch.randint(0, Xtr.shape[0], (32,))\n\n    #forward pass\n    emb = C[Xtr[xi]]\n    h = torch.tanh(emb.view(-1,30) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Ytr[xi])\n    #print(loss.item())\n\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    #update\n    #lr = lrs[i]\n    lr = 0.01\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    #keeping track\n    #lri.append(lr)\n    stepi.append(i)\n    lossi.append(loss.item())\n\n#print(loss.item())\n</pre> lri = [] lossi = [] stepi = []  for i in range(40000):      #Minibatch     xi = torch.randint(0, Xtr.shape[0], (32,))      #forward pass     emb = C[Xtr[xi]]     h = torch.tanh(emb.view(-1,30) @ W1 + b1)     logits = h @ W2 + b2     loss = F.cross_entropy(logits, Ytr[xi])     #print(loss.item())      #backward pass     for p in parameters:         p.grad = None     loss.backward()      #update     #lr = lrs[i]     lr = 0.01     for p in parameters:         p.data += -lr * p.grad      #keeping track     #lri.append(lr)     stepi.append(i)     lossi.append(loss.item())  #print(loss.item()) <p>The above cell will take a couple of seconds to run. Training a neural net can take a while, but luckily this is a very small neural network.</p> <p>Evaluation:</p> In\u00a0[31]: Copied! <pre>emb = C[Xdev]\nh = torch.tanh(emb.view(-1,30) @ W1 + b1)\nlogits = h @ W2 + b2\ndevloss = F.cross_entropy(logits, Ydev)\ndevloss\n</pre> emb = C[Xdev] h = torch.tanh(emb.view(-1,30) @ W1 + b1) logits = h @ W2 + b2 devloss = F.cross_entropy(logits, Ydev) devloss Out[31]: <pre>tensor(2.1091, grad_fn=&lt;NllLossBackward0&gt;)</pre> In\u00a0[32]: Copied! <pre>emb = C[Xtr]\nh = torch.tanh(emb.view(-1,30) @ W1 + b1)\nlogits = h @ W2 + b2\ntrloss = F.cross_entropy(logits, Ytr)\ntrloss\n</pre> emb = C[Xtr] h = torch.tanh(emb.view(-1,30) @ W1 + b1) logits = h @ W2 + b2 trloss = F.cross_entropy(logits, Ytr) trloss Out[32]: <pre>tensor(2.0482, grad_fn=&lt;NllLossBackward0&gt;)</pre> <p>Training and Dev loss are almost the same. So we know we are not overfitting. But what it typically means is that the Neural Net is very small, so essentially it is underfitting the data.   Therefore to improve the performance we'll need to increase the size of the neural net.</p> In\u00a0[15]: Copied! <pre>plt.figure(figsize=(8,8))\nplt.scatter(C[:,0].data, C[:,1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(C[i,0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\nplt.grid('minor')\n</pre> plt.figure(figsize=(8,8)) plt.scatter(C[:,0].data, C[:,1].data, s=200) for i in range(C.shape[0]):     plt.text(C[i,0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\") plt.grid('minor') <p>Not much changes to what we have done so far, but just some code improvement for the lr value to change based on the iterations.</p> <p>Here basically we are open to experimenting with different values, whether it is the inputs, size of the layers or the loss rate values to see how we can decrease the final loss value.</p> In\u00a0[\u00a0]: Copied! <pre># ------------ now made respectable :) ---------------\n</pre> # ------------ now made respectable :) --------------- In\u00a0[33]: Copied! <pre>g = torch.Generator().manual_seed(2147483647) # for reproducibility\nC = torch.randn((27, 10), generator=g)\nW1 = torch.randn((30, 200), generator=g)\nb1 = torch.randn(200, generator=g)\nW2 = torch.randn((200, 27), generator=g)\nb2 = torch.randn(27, generator=g)\nparameters = [C, W1, b1, W2, b2]\n</pre> g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((27, 10), generator=g) W1 = torch.randn((30, 200), generator=g) b1 = torch.randn(200, generator=g) W2 = torch.randn((200, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] In\u00a0[34]: Copied! <pre>sum(p.nelement() for p in parameters) # number of parameters in total\n</pre> sum(p.nelement() for p in parameters) # number of parameters in total Out[34]: <pre>11897</pre> In\u00a0[35]: Copied! <pre>for p in parameters:\n  p.requires_grad = True\n</pre> for p in parameters:   p.requires_grad = True In\u00a0[\u00a0]: Copied! <pre>lre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n</pre> lre = torch.linspace(-3, 0, 1000) lrs = 10**lre In\u00a0[36]: Copied! <pre>lri = []\nlossi = []\nstepi = []\n</pre> lri = [] lossi = [] stepi = [] In\u00a0[37]: Copied! <pre>for i in range(200000):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (32,))\n  \n  # forward pass\n  emb = C[Xtr[ix]] # (32, 3, 10)\n  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 200)\n  logits = h @ W2 + b2 # (32, 27)\n  loss = F.cross_entropy(logits, Ytr[ix])\n  #print(loss.item())\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  #lr = lrs[i]\n  lr = 0.1 if i &lt; 100000 else 0.01\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  #lri.append(lre[i])\n  stepi.append(i)\n  lossi.append(loss.log10().item())\n\n#print(loss.item())\n</pre> for i in range(200000):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (32,))      # forward pass   emb = C[Xtr[ix]] # (32, 3, 10)   h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 200)   logits = h @ W2 + b2 # (32, 27)   loss = F.cross_entropy(logits, Ytr[ix])   #print(loss.item())      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update   #lr = lrs[i]   lr = 0.1 if i &lt; 100000 else 0.01   for p in parameters:     p.data += -lr * p.grad    # track stats   #lri.append(lre[i])   stepi.append(i)   lossi.append(loss.log10().item())  #print(loss.item()) In\u00a0[38]: Copied! <pre>plt.plot(stepi, lossi)\n</pre> plt.plot(stepi, lossi) Out[38]: <pre>[&lt;matplotlib.lines.Line2D at 0x17d66872770&gt;]</pre> In\u00a0[39]: Copied! <pre>emb = C[Xtr] # (32, 3, 2)\nh = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Ytr)\nloss\n</pre> emb = C[Xtr] # (32, 3, 2) h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100) logits = h @ W2 + b2 # (32, 27) loss = F.cross_entropy(logits, Ytr) loss Out[39]: <pre>tensor(2.1294, grad_fn=&lt;NllLossBackward0&gt;)</pre> In\u00a0[40]: Copied! <pre>emb = C[Xdev] # (32, 3, 2)\nh = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Ydev)\nloss\n</pre> emb = C[Xdev] # (32, 3, 2) h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100) logits = h @ W2 + b2 # (32, 27) loss = F.cross_entropy(logits, Ydev) loss Out[40]: <pre>tensor(2.1677, grad_fn=&lt;NllLossBackward0&gt;)</pre> <p>Sampling from the model</p> In\u00a0[41]: Copied! <pre>context = [0] * block_size\nC[torch.tensor([context])].shape\n</pre> context = [0] * block_size C[torch.tensor([context])].shape Out[41]: <pre>torch.Size([1, 3, 10])</pre> <p>Considering only one set of training set for simplicity rather than the entire training set^</p> In\u00a0[42]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       emb = C[torch.tensor([context])] # (1,block_size,d)       h = torch.tanh(emb.view(1, -1) @ W1 + b1)       logits = h @ W2 + b2       probs = F.softmax(logits, dim=1)       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       if ix == 0:         break          print(''.join(itos[i] for i in out)) <pre>mora.\nkayah.\nseel.\nndheyah.\nreimanield.\nleg.\nadeerdoeliah.\nmilopaleigh.\neson.\narleitzion.\nkalin.\nshuhporxhimiel.\nkin.\nreelle.\njoberlyn.\nbren.\nder.\nyarue.\nels.\nkaysh.\n</pre> <p>To be fair, most of them could make sense lol. But atleast this time they definetely sound more name like, so we are defo making progress. So lessgoo xD</p>"},{"location":"ZeroToHero/Makemore-part2/C-main-notebook/#set-c-notebook","title":"SET C - NOTEBOOK\u00b6","text":""},{"location":"ZeroToHero/Makemore-part3/","title":"Makemore Part 3","text":""},{"location":"ZeroToHero/Makemore-part3/#language-model-3","title":"LANGUAGE MODEL - 3","text":"<p>Timeline: 6th - 14th January, 2025</p>"},{"location":"ZeroToHero/Makemore-part3/#introduction","title":"Introduction","text":"<p>Welcome to my documentation for Makemore Part 3 from Andrej Karpathy's Neural Networks: Zero to Hero series. This section focuses on the intricacies of activations, gradients, and the introduction of Batch Normalization in the context of training deep neural networks. Here, I\u2019ve compiled my notes and insights from the lecture to serve as a reference for understanding these critical concepts and their practical applications.</p>"},{"location":"ZeroToHero/Makemore-part3/#overview-of-makemore-part-3","title":"Overview of Makemore Part 3","text":"<p>In this part of the series, I explored the following key topics:</p> <p>Understanding Activations and Gradients: The lecture emphasizes the importance of monitoring activations and gradients during training. It discusses how improper scaling can lead to issues such as saturation in activation functions (e.g., <code>tanh</code>), which can hinder learning.</p> <p>Key Concepts Covered:</p> <ul> <li> <p>Initialization Issues: The video begins by examining how weight initialization affects training. It highlights that initializing weights too high or too low can lead to poor performance and suggests using small random values instead.</p> </li> <li> <p>Saturated Activations: The lecture addresses the problem of saturated activations in the <code>tanh</code> function, where outputs can become stuck at extreme values (-1 or 1). This saturation can slow down learning significantly.</p> </li> <li> <p>Kaiming Initialization: A method for initializing weights that helps maintain a healthy scale of activations throughout the layers. This technique is crucial for ensuring effective training in deeper networks.</p> </li> <li> <p>Batch Normalization: The core innovation introduced in this lecture is Batch Normalization, which normalizes the inputs to each layer. This technique stabilizes learning and allows for faster convergence by reducing internal covariate shift.</p> </li> <li> <p>Visualizations for Diagnostics: Throughout the lecture, various visualizations are utilized to monitor forward pass activations and backward pass gradients. These tools help diagnose issues within the network and understand its health during training.</p> </li> </ul>"},{"location":"ZeroToHero/Makemore-part3/#key-resources","title":"Key Resources","text":"<p>Video Lecture</p> <ul> <li>I watched the lecture on YouTube: Building Makemore Part 3</li> </ul> <p>Codes:</p> <ul> <li>The Jupyter notebooks and code implementations are available within this documentation itself.</li> <li>If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Language Model 3</li> </ul>"},{"location":"ZeroToHero/Makemore-part3/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li>The lecture documentation has been divided into 1 set only, with one main notebook and two additional notebooks.</li> <li>Notes have been marked with timestamps to the video.</li> <li>This allows for simplicity and better understanding, as the lecture is long.</li> </ul> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/","title":"SET A - LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#introduction","title":"Introduction","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-000000","title":"Timestamp: 00:00:00","text":"<p>We'll be continuing with our implementation of Makemore. Now, we'll obviously like to move forward from the previous implementation to slightly more advanced implementations like RNN, GRU, Transformer etc. </p> <p>But we will be sticking around with MLP itself right now. The purpose of this is to have an intuitive understanding of the activations in the NN during training and especially the gradients that are falling backwards - how they behave and what they look like.</p> <p>So it is going to be very important to understand the history of the development of these architectures (RNN, GRU etc.). The reason is, RNN while they are very expressive, is the universal approximator and in principle can implement all the algorithms; we will see that they are not very easily optimizable with the first order gradient techniques we have available to us and that we use all the time.</p> <p>And the key to understanding why they are not optimizable so easily, is to understand the activations, the gradients and how they behave during training (And apparently it is also seen that all the variants after RNN have tried to improve that situation).</p> <p>So that is what we will be focusing on.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#starter-code","title":"Starter code","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-000122","title":"Timestamp: 00:01:22","text":"<p>Cleaned up code from Makemore Part 2 with some fixes. Will be starting from there, provided a starter code along with revision explanations, so go through that. You can find the code here</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#fixing-the-initial-loss","title":"Fixing the initial loss","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-000419","title":"Timestamp: 00:04:19","text":"<p>Now the first thing we would like to point out is the initialization. We can tell that there is multiple things that is wrong with our neural network especially at initialization, but lets start with the first one.</p> <p>In the zeroth iteration, our loss value is way too high. And as it moves to the next iteration the loss value comes down even more significantly. Now, usually during the training of the neural net we can almost always expect what our initial loss/loss value is going to be. And we can also calculate it in our case:</p> <p>At the start we basically expect it to record a constant/standard normal distribution as there can be only one possibility from 27 characters. So we consider that and then take the negative log likelihood <code>-torch.tensor(1/27.0).log()</code>  Then we can see that the loss value is a lot lower and is what we expect.</p> <p>Why this is happening is because, at the start the model/NN is 'confidently wrong'. So for some it records very high confidence and for some very low confidence.</p> <p>So how do we resolve this, we notice that the logits for the zeroth iteration are very high and we need them to be close to zero. and logits are calculated based on <code>logits = h @ W2 + b2</code> So we try to multiply the W2 and b2 values with values very close to zero.</p> <p>We added the values 0.01 and 0 to the W2 and b2 values (avoid adding 0 directly, although for this example multiplying it with 0 to the output layer might just be fine, but don't normally do it)</p> <p>So we finally see that, the initial loss value has been controlled and the graph output doesn't have that 'hockey stick' appearance. And the Train evaluation value was also reduced from 2.12 to 2.06</p> <p>This happened because we actually spent time optimizing the NN rather than just spending the initial several thousand iterations just squashing the weights and then optimizing it. </p> <p>So that was the Number 1 issue fix.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#fixing-the-saturated-tanh","title":"Fixing the saturated tanh","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-001259","title":"Timestamp: 00:12:59","text":"Note <p>This section will contain different code snippets which won't be present in the final implementation notebook, That is because these were used for just explaination purposes. For better understanding, copy the snippets, add it to the code to see how the outputs will turn out.</p> <p>Now, lets look at the Second problem. Although we were able to fix the initial loss there is still another problem lingering within this neural net. </p> <p>We have fixed the logits values, the problem now is with the <code>h</code> values - the activations of the hidden states. Lets see how the value distributions are for the activation function, you can depict those using a histogram:</p> <pre><code>plt.hist(h.view(-1).tolist(), 50)\n</code></pre> <pre><code>plt.hist(hpreact.view(-1).tolist(), 50)\n</code></pre> <p>In the first code you would see that the values would be active at 1 and -1 (tanh is being a play here) and in the second code, if we look at the 'h pre activation' values <code>hpreact</code> that we feed into the tanh, the distribution of the preactivations is very very broad.</p> <p>Now this may not really look like an issue and that is coz we are (I am) still a beginner in Neural Nets :) So, Watch these sections of the timestamps for a better visual understanding of the issue:</p> <ul> <li> <p>15:16 to 18:20 : Diving into the backpropagation process, especially in the tanh layer. He dives into what happens in tanh during the backward pass. So the concern here is that, if all of the outputs of <code>h</code> are in the flat regions of -1 and 1, then the gradients that are just flowing though the network would just get destroyed.</p> </li> <li> <p>18:21 to 20:35: A figure has been generated to see how much of the <code>h</code> values have been flattened in the tanh layer. It is a boolean expression graph, where if the condition is true it will be represented in white dots.      <pre><code>plt.figure(figsize=(20,10))\nplt.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest')\n</code></pre>     Now, if there was a column that was completely whites, then we have something what we call the 'dead neuron'. It means it never fell within the active part of the tanh i.e. the curve area between -1 and 1. And if any of the values fall in the flat regions (end points) i.e. at the -1 and 1, then that means the neuron will never learn and it is a dead neuron (so now you know why it was an issue there :) )</p> </li> <li> <p>20:36 to 23:23 : Behaviors of different types of Activation functions.</p> </li> <li> <p>23:24 to 25:53: Back to the black and white figure graph where we take the measures to resolve the issue of dead neurons.     So now, <code>hpreact</code> comes from <code>embcat</code> which comes from <code>C[Xb]</code> so these are uniform Gaussian, which are then multiplied and added with the weights and bias.     The <code>hpreact</code> values is right now way off zero and we want its values to be closer to zero. So therefore like how we did with logits, we modify the values of the W1 and b1 during the NN values initialization.     We have multiplied those values with values close to zero like <code>0.01</code> for b1 and <code>0.1</code> for W1, and we noticed that there are no values in the flat regions of the tanh layer therefore no white spots which is great. For example purposes, we increase the values for W1 by multiplying it with <code>0.2</code> instead so that we can see some white spots.</p> </li> </ul> Note <p>Use the code snippets provided within this lecture timestamp.</p> <p>So now we have improved the loss value a lot further. This is basically illustrating initialization and its impact on performance, just by being aware of the internals of the NN of its activations and gradients.</p> <p>Lastly, the purpose of those 2 explanations is that, right now we are only working with a single MLP. Now as we get a bigger NN and there are more depths into the neurons, these initializations errors can cause us a lot of problems and we may even come to a case where the NN doesn't train at all. So we must be very aware of this.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#calculating-the-init-scale-kaiming-init","title":"Calculating the init scale: \u201cKaiming init\u201d","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-002753","title":"Timestamp: 00:27:53","text":"<p>In our code till now, during the initialization of the Neural Network, we have been hardcoding the value/adding magic numbers to optimize the initial value. But we need to ensure that we follow a more dynamic approach. <pre><code>g = torch.Generator().manual_seed(2147483647)\nC \u00a0= torch.randn((vocab_size, n_embd), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2 #here\nb1 = torch.randn(n_hidden, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0generator=g) * 0.01\nb2 = torch.randn(vocab_size, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0generator=g) * 0\n</code></pre></p> <p>Upto 35:56 we have covered as to why that needs to be done (How activation function actually squash the values and how optimizing the initial values helps with the help of histogram) along with that we were also shown a research paper by Kaiming He which illustrates how we can perform such calculation of numbers during initialization. </p> <p>So in the paper he suggests to do a square root of 2/fin (fin is the input value). The square root of fin is there, the 2 was added because in their paper they considered ReLU as the activation function, so in that the values get squashed to half and then optimised. So in order to balance that they added 2 -&gt; <code>\u221a2/nl</code></p> <p>Now, this kaiming implementation is also done in PyTorch called 'Kaiming normal' which can be used as well.</p> Note <p>Go through till that part of the timestamp for a getting a more intuitive understanding, it was more of an explanation backup to show why we are doing what we are doing in the next steps.</p> <p>From 35:57: Now, back then when the above paper was released about 7 years ago, we had to be very careful about how we initialize the values for these input values of the NN. But now we have more modern innovations that handle this way better, which include: - Residual Connection (Which we will cover in later videos) - Use of different Normalization layers- Batch Normalization. Layer Normalization, Group Normalization - Much better Optimizers- Not just stochastic gradient descent the simple optimizer that we have bee using here, but slightly more complex optimizers like RMS prop and Adam(Adaptive Moment Estimation). So all these modern innovations make it much less necessary for us to precisely calculate the initialization of the neural net.</p> <p>The method which Andrej uses is dividing the initial value with the square root of the fin.</p> <p>As for our code in this, we will be doing the \"Kaiming init\" itself, provided by PyTorch. Now they have also mentioned a certain gain value which must be added on the numerator depending on the activation function used, as mentioned here. So in our case its tanh, so we use <code>5/3</code>, divide with the square root of the fin. So finally that would be: <code>W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)</code> instead of directly multiplying with a magic number which we predicted from looking at the histogram graphs.</p> <p>This approach is a lot better and will help us as we scale our NN and becomes much larger.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#batch-normalization","title":"Batch normalization","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-004040","title":"Timestamp: 00:40:40","text":"<p>Showed the paper which actually introduced the concept of batch normalization, researched by a team at google - Research paper and the idea of the concept which it explained.</p> <p>From 42:16 -&gt; Implemented the batch normalization step into the code (How and what needs to be done based on the paper)</p> <p>We will work on the pre-activation value <code>hpreact</code> just before they enter the tanh layer. These are already almost gaussian (because 1. if it is too small then tanh will have no effect on it and if it is too large then tanh will become to saturated, 2. this is just a single layer NN, but as it gets complex this is where we will be adding it)</p> <p>Now, we will be considering one batch, in this case the 0th column and calculate its mean and standard deviation. <pre><code>hpreact.mean(0, keepdim=True).shape\n</code></pre></p> <pre><code>hpreact.std(0, keepdim=True).shape\n</code></pre> <p>Now we perform the batch normalization on <code>hpreact</code> and make the values roughly gaussian. <pre><code>hpreact = (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True))\n</code></pre> So we subtract with the mean and divide with its standard deviation.</p> <p>Now, if we train the NN after doing this, we wont exactly get a good loss value. First of all, we don't want it to force it to be gaussian for all values, we need it just for the first value. Apart from that, we want it to spread around so that the values fed into the tanh has mixed reactions - some of them would trigger it, some may saturate it. </p> <p>So we introduce another concept from the paper called 'Scale and Shift' What we do is, take the normalized input and we are scaling it with some gain and offsetting with some bias, to get our final output from the layer (See page 2, right bottom box in the batch norm paper)</p> <p>Now we add those values <pre><code>bngain = torch.ones((1, n_hidden))\n</code></pre></p> <pre><code>bnbias = torch.zeros((1, n_hidden))\n</code></pre> <p>So we add these to the batch norm layer <pre><code>hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias\n</code></pre> Exactly as in the paper, multiply the gain value and add the bias.</p> <p>Then, we add them to the parameters of the NN as they will be trained during backpropagation. <code>parameters = [C, W1, b1, W2, b2]</code> to <code>parameters = [C, W1, b1, W2, b2, bngain, bnbias]</code></p> <p>(We add that batch norm layer^ for the validation set of code as well)</p> <p>In bigger NN, we usually recognize the single layer or the convolution layers and add the batch norm just after this to control the scale of these activations at every point of this NN. So it significantly stabilizes the training and that is why they are so popular.</p> <p>From 50:15 to 53:48 -&gt; The Story of Batch Normalization of how it acts as a regulariser, so it works for some training(very few cases) and how many people are trying to move away from it to other normalization techniques.</p> <p>From 54:04 -&gt;  Modifications to the code (the validation part), where it can handle cases where if only a single input is fed (as now it expects values in batches) -&gt; after that update again to make sure it is in a loop</p> <ul> <li>So in order to fix that, we have calculated the mean and standard deviation, so they are now fixed values of the tensors which are now passed:</li> </ul> <pre><code># calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n  # pass the training set through\n  emb = C[Xtr]\n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 # + b1\n  # measure the mean/std over the entire training set\n  bnmean = hpreact.mean(0, keepdim=True)\n  bnstd = hpreact.std(0, keepdim=True)\n</code></pre> <pre><code>@torch.no_grad()\ndef split_loss(split):\n\n\u00a0 x,y = {\n\u00a0 \u00a0 'train': (Xtr, Ytr),\n\u00a0 \u00a0 'val': (Xdev, Ydev),\n\u00a0 \u00a0 'test': (Xte, Yte),\n\u00a0 }[split]\n\u00a0 \n\u00a0 emb = C[x] # (N, block_size, n_embd)\n\u00a0 embcat = emb.view(emb.shape[0], -1)\n\u00a0 hpreact = embcat @ W1 + b1\n\u00a0 hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias #this is removed\n\u00a0 hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias #and updated to this\n\u00a0 h = torch.tanh(hpreact) \n\u00a0 logits = h @ W2 + b2\n\u00a0 loss = F.cross_entropy(logits, y)\n\u00a0 print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</code></pre> <ul> <li> <p>Contd from 56:24till 59:58 (Recommend to watch this part of the timestamp again) Now nobody wants to estimate the mean and standard deviation in the second stage i.e. having made a neural net. So the paper itself provides another method (the mean and std idea was received from the paper as well), where we add like a running state. So the moment the first stage of making NN is done, it automatically comes to this. </p> <p>So instead of adding this in the validation part of the code, we are going to add this in the training time itself. That way we are kind of creating this side job which does this work simultaneously for both training and val sets and we don't need that new block of code which we had added^</p> </li> </ul> <pre><code># BatchNorm parameters\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\nbnmean_running = torch.zeros((1, n_hidden))\nbnstd_running = torch.ones((1, n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\n</code></pre> <pre><code># BatchNorm layer\nbnmeani = hpreact.mean(0, keepdim=True)\nbnstdi = hpreact.std(0, keepdim=True)\n\nhpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n\nwith torch.no_grad():\n    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n</code></pre> <p>So if you run <code>bnmean</code> and <code>bnmean_running</code> | <code>bnstd</code> and <code>bnstd_running</code> (haven't removed the new code which we added before the val code chunk yet) you would see that they are very similar, not identical but similar.</p> <p>From 1:00:55-&gt; 2 more notes: 1. In the paper, in the box explanation with the formulas, under <code>//normalise</code> we see an epsilon (\u03f5), that in added there so that it essentially avoids a divide by zero. We haven't done that in our code because it won't really be necessary as our example is too small. 2. The <code>b1</code> that we add in the <code>hpreact</code> is pretty useless right now. Because in the batch norm we are already adding the bias <code>bnbias</code> and also before that subtracting the value of <code>hpreact</code> with <code>bnmean</code>. So essentially, in batch norm we are adding our own bias. So during the first addition of weights in the NN and if you are performing Batch norm, you don't have to add the bias, so you can just comment it out (Its not really having a catastrophic effect if you keep it, but we know its not really doing anything in our code so we just drop it).</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#batch-normalization-summary","title":"Batch normalization: summary","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-010307","title":"Timestamp: 01:03:07","text":"<p>(Useful for a quick yet detailed recap)</p> <ul> <li>We are using batch normalization to control the statistics of activations in the neural net.</li> <li>It is common to sprinkle the batch normalization layer across the neural net, and usually we place it after layers that have multiplications. Like a linear layer or Convolutional layer.</li> <li> <p>From 1:03:33 -&gt; Explanation of the different variables in the code.</p> <ul> <li> <p>Batch norm internally has parameters for the gain and the bias, that are trained using backpropagation. <pre><code>bngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\n</code></pre></p> </li> <li> <p>It also has two buffers: running mean and running std. <pre><code>bnmean_running = torch.zeros((1, n_hidden))\nbnstd_running = torch.ones((1, n_hidden))\n</code></pre> These are not trained using backpropagation, these are trained using the update (janky update as sensei calls is, the one we made) <pre><code>with torch.no_grad():\n\u00a0 \u00a0 bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n\u00a0 \u00a0 bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n</code></pre></p> </li> <li> <p>(i) These (above 2 points) are the parameters which calculate the mean and std of the activations, that are fed into the batch norm layer, over that batch -&gt; (ii) Then it is centering that batch to be unit gaussian and then it is offsetting and scaling it by the learned bias and gain -&gt; (iii) On top of that it is keeping track of the mean and std inputs and is maintaining the running mean and standard deviation (this will later be used as an inference so that we don't have to re-estimate the mean and std i.e. <code>bnmeani</code> and <code>bnstdi</code> all the time) and this allows us to basically forwards individual examples at test time. <pre><code>\u00a0 #----------------\n\u00a0 # BatchNorm layer\n\u00a0 #----------------\n\u00a0 bnmeani = hpreact.mean(0, keepdim=True) #(i)\n\u00a0 bnstdi = hpreact.std(0, keepdim=True) #(i)\n\u00a0 hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias #(ii)\n\u00a0 with torch.no_grad(): #(iii)\n\u00a0 \u00a0 bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani #(iii)\n\u00a0 \u00a0 bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi #(iii)\n\u00a0 #----------------\n</code></pre></p> </li> </ul> </li> </ul> <p>( yes this is fairly a complicated layer, but this is what it is doing internally :) )</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#real-example-resnet50-walkthrough","title":"Real example: resnet50 walkthrough","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-010450","title":"Timestamp: 01:04:50","text":"<p>RESNET: Residual Neural Network commonly used for image classification. Comparison of how the methodology implemented by us is almost similar to that in resnet implementation.</p> <p>From 1:11:20 -&gt; The comparison of batch norm: our implementation and in PyTorch. See PyTorch Documentation</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#summary-of-the-lecture","title":"Summary of the lecture","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-011410","title":"Timestamp: 01:14:10","text":"<p>Overall revision and finally with a note of advise to not use batch norm as there are many better normalization techniques available now.</p>"},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#additional-summary","title":"Additional Summary","text":""},{"location":"ZeroToHero/Makemore-part3/A-main-makemore-part3/#timestamp-011835-to-015134","title":"Timestamp 01:18:35 to 01:51:34 :","text":"<p>Info</p> <p>He does a quick summary explanations of all the different sections of the codes. It will be a lot easier to follow from the video, here are the timestamps:</p> <ul> <li>01:18:35 just kidding: part2: PyTorch-ifying the code </li> <li>01:26:51 viz #1: forward pass activations statistics </li> <li>01:30:54 viz #2: backward pass gradient statistics </li> <li>01:32:07 the fully linear case of no non-linearities </li> <li>01:36:15 viz #3: parameter activation and gradient statistics </li> <li>01:39:55 viz #4: update:data ratio over time </li> <li>01:46:04 bringing back batchnorm, looking at the visualizations </li> <li>01:51:34 summary of the lecture for real this time</li> </ul> <p>Also, it was supposed to be a quick summary of what all we have done, as for those visualization graph, some may have been a little challenging to understand, but we will get into its depth in the next upcoming videos. So keep learning!</p> <p>The main 3 points discussed:</p> <ol> <li> <p>Introduction to Batch Norm, one of the first modern methods that were introduced to help stabilize training deep neural networks. How it works and How it will be used in a Neural Network.</p> </li> <li> <p>PyTorch-ifying the code and writing it into modules/layers/classes -&gt; Linear, BatchNorm1d and tanh. These can be stacked up into neural nets like building blocks. So if you import <code>torch.nn</code> it will work here as well as the API calls are similar.</p> </li> <li> <p>Introducing the dynamic tools that are used to understand if the neural network is in a good state dynamically.     So we are looking at the statistics, histogram and activation of- (graph 1) the Forward pass activations, (graph 2) the Backward pass gradients and (graph 3) the ways it is going to get updated as part of the stochastic gradient descent (so we look at the mean, std and the ratio of the gradients to data) and (graph 4) finally the updates to the data (the final graph where we just compare it based on how it changes over time).</p> </li> </ol> <p>Now, there are a lot things that haven't been explained as well. Right now we are actually getting to the cutting edge of where the field actually is. We certainly still haven't solved Initialization, Backpropagation - it's still under research but we are making progress and we have tools which are telling us if things are on the right track or not.</p>"},{"location":"ZeroToHero/Makemore-part3/A-main-notebook/","title":"Jupyter Notebook","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() words[:8] Out[2]: <pre>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</pre> In\u00a0[3]: Copied! <pre>len(words)\n</pre> len(words) Out[3]: <pre>32033</pre> In\u00a0[4]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[5]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[7]: Copied! <pre># MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\n# BatchNorm parameters\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\nbnmean_running = torch.zeros((1, n_hidden))\nbnstd_running = torch.ones((1, n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # MLP revisited n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2 #b1 = torch.randn(n_hidden,                        generator=g) * 0.01 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01 b2 = torch.randn(vocab_size,                      generator=g) * 0  # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden))  parameters = [C, W1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>11897\n</pre> In\u00a0[10]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n  \n  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias #batch normalisation layer\n  #----------------\n  # BatchNorm layer\n  #----------------\n  bnmeani = hpreact.mean(0, keepdim=True)\n  bnstdi = hpreact.std(0, keepdim=True)\n    \n  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n    \n  with torch.no_grad():\n    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n  #----------------\n\n  h = torch.tanh(hpreact) # hidden layer\n  logits = h @ W2 + b2 # output layer\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n\n  #break #Add this while experienting so you dont have to print all the steps\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   embcat = emb.view(emb.shape[0], -1) # concatenate the vectors   hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation      #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias #batch normalisation layer   #----------------   # BatchNorm layer   #----------------   bnmeani = hpreact.mean(0, keepdim=True)   bnstdi = hpreact.std(0, keepdim=True)        hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias        with torch.no_grad():     bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani     bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi   #----------------    h = torch.tanh(hpreact) # hidden layer   logits = h @ W2 + b2 # output layer   loss = F.cross_entropy(logits, Yb) # loss function      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())    #break #Add this while experienting so you dont have to print all the steps <pre>      0/ 200000: 3.2342\n  10000/ 200000: 1.8947\n  20000/ 200000: 1.8914\n  30000/ 200000: 1.9489\n  40000/ 200000: 2.1701\n  50000/ 200000: 2.0639\n  60000/ 200000: 2.0728\n  70000/ 200000: 2.3965\n  80000/ 200000: 2.4142\n  90000/ 200000: 2.2257\n 100000/ 200000: 2.2824\n 110000/ 200000: 1.8584\n 120000/ 200000: 2.1613\n 130000/ 200000: 1.9009\n 140000/ 200000: 1.8430\n 150000/ 200000: 2.3324\n 160000/ 200000: 2.2026\n 170000/ 200000: 1.6905\n 180000/ 200000: 1.9502\n 190000/ 200000: 2.0909\n</pre> <p>Used for lecture : 00:12:59 fixing the saturated tanh</p> In\u00a0[\u00a0]: Copied! <pre>#plt.hist(h.view(-1).tolist(), 50)\n</pre> #plt.hist(h.view(-1).tolist(), 50) In\u00a0[\u00a0]: Copied! <pre>#plt.hist(hpreact.view(-1).tolist(), 50)\n</pre> #plt.hist(hpreact.view(-1).tolist(), 50) In\u00a0[\u00a0]: Copied! <pre>#plt.figure(figsize=(20,10))\n#plt.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest')\n</pre> #plt.figure(figsize=(20,10)) #plt.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest') <p>End of Used for lecture : 00:12:59 fixing the saturated tanh</p> In\u00a0[24]: Copied! <pre>plt.plot(lossi)\n</pre> plt.plot(lossi) Out[24]: <pre>[&lt;matplotlib.lines.Line2D at 0x21a782d9720&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre># # calibrate the batch norm at the end of training\n\n# with torch.no_grad():\n#   # pass the training set through\n#   emb = C[Xtr]\n#   embcat = emb.view(emb.shape[0], -1)\n#   hpreact = embcat @ W1 # + b1\n#   # measure the mean/std over the entire training set\n#   bnmean = hpreact.mean(0, keepdim=True)\n#   bnstd = hpreact.std(0, keepdim=True)\n</pre> # # calibrate the batch norm at the end of training  # with torch.no_grad(): #   # pass the training set through #   emb = C[Xtr] #   embcat = emb.view(emb.shape[0], -1) #   hpreact = embcat @ W1 # + b1 #   # measure the mean/std over the entire training set #   bnmean = hpreact.mean(0, keepdim=True) #   bnstd = hpreact.std(0, keepdim=True) In\u00a0[11]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 #+ b1\n  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias #batch normalisation layer\n  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   hpreact = embcat @ W1 #+ b1   #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias #batch normalisation layer   hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias   h = torch.tanh(hpreact) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.037672996520996\nval 2.107128620147705\n</pre> In\u00a0[11]: Copied! <pre>#The initial loss value that we expect\n-torch.tensor(1/27.0).log()\n</pre> #The initial loss value that we expect -torch.tensor(1/27.0).log() Out[11]: <pre>tensor(3.2958)</pre> In\u00a0[10]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       emb = C[torch.tensor([context])] # (1,block_size,d)       h = torch.tanh(emb.view(1, -1) @ W1 + b1)       logits = h @ W2 + b2       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break          print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>mora.\nmayah.\nsee.\nmel.\nrylee.\nemmadiejd.\nleg.\nadelyn.\nelin.\nshi.\njen.\neden.\nestanar.\nkayziquetta.\nnoshir.\nroshiriel.\nkendreth.\nkonnie.\ncasube.\nged.\n</pre>"},{"location":"ZeroToHero/Makemore-part3/executed-starter-code/","title":"Starter Code","text":"<p>Importing the PyTorch and Matplotlib utilities as before</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline <p>Reading all the words</p> In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() words[:8] Out[2]: <pre>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</pre> In\u00a0[3]: Copied! <pre>len(words)\n</pre> len(words) Out[3]: <pre>32033</pre> <p>Printing the vocabulary of all the lower case letters and the special dot token</p> In\u00a0[4]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> <p>Here we are reading the dataset and processing it. In the end of this cell, we are also splitting the dataset into three- Train, Dev and Loss split</p> In\u00a0[5]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> <p>Almost the same MLP, but we have cleaned it up to add those hard coded values into variables so we just have to modify them there</p> In\u00a0[6]: Copied! <pre># MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),          generator=g)\nb2 = torch.randn(vocab_size,                      generator=g)\n\nparameters = [C, W1, b1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # MLP revisited n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.randn(n_hidden,                        generator=g) W2 = torch.randn((n_hidden, vocab_size),          generator=g) b2 = torch.randn(vocab_size,                      generator=g)  parameters = [C, W1, b1, W2, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>11897\n</pre> <p>Here we are optimizing the NN. Same as before, just those hard coded numbers (or magic numbers as Andrej sensei calls it) have been replaced with variable names for more readability</p> In\u00a0[7]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n  hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n  h = torch.tanh(hpreact) # hidden layer\n  logits = h @ W2 + b2 # output layer\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   embcat = emb.view(emb.shape[0], -1) # concatenate the vectors   hpreact = embcat @ W1 + b1 # hidden layer pre-activation   h = torch.tanh(hpreact) # hidden layer   logits = h @ W2 + b2 # output layer   loss = F.cross_entropy(logits, Yb) # loss function      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item()) <pre>      0/ 200000: 27.8817\n  10000/ 200000: 2.8244\n  20000/ 200000: 2.5473\n  30000/ 200000: 2.8961\n  40000/ 200000: 2.0967\n  50000/ 200000: 2.5020\n  60000/ 200000: 2.4999\n  70000/ 200000: 2.0510\n  80000/ 200000: 2.4076\n  90000/ 200000: 2.3172\n 100000/ 200000: 2.0199\n 110000/ 200000: 2.3338\n 120000/ 200000: 1.8767\n 130000/ 200000: 2.3989\n 140000/ 200000: 2.2102\n 150000/ 200000: 2.1937\n 160000/ 200000: 2.0843\n 170000/ 200000: 1.8780\n 180000/ 200000: 1.9727\n 190000/ 200000: 1.8222\n</pre> <p>Here we plot the loss</p> In\u00a0[8]: Copied! <pre>plt.plot(lossi)\n</pre> plt.plot(lossi) Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x28412485fc0&gt;]</pre> <p>Seeing the loss in train and val loss. There is a slight modification to this as to how the splitting is done.</p> <p>Here the decorator <code>@torch.no_grad()</code> basically tells PyTorch to not maintain the grad value, as it assumes/anticipated that the backpropagation will be calculated after this and we are saying No.</p> In\u00a0[9]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.12243390083313\nval 2.1646578311920166\n</pre> <p>Sampling of the model: Forward pass -&gt; Sampling from the distribution -&gt; Continuing till we get the special token '.'</p> In\u00a0[10]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       emb = C[torch.tensor([context])] # (1,block_size,d)       h = torch.tanh(emb.view(1, -1) @ W1 + b1)       logits = h @ W2 + b2       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break          print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>mora.\nmayah.\nsee.\nmel.\nrylee.\nemmadiejd.\nleg.\nadelyn.\nelin.\nshi.\njen.\neden.\nestanar.\nkayziquetta.\nnoshir.\nroshiriel.\nkendreth.\nkonnie.\ncasube.\nged.\n</pre> <p>So yeah, this will be our starting point. Also use this as a revision for the previous lecture.</p>"},{"location":"ZeroToHero/Makemore-part3/visualization-tools/","title":"Visualization Code","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() words[:8] Out[2]: <pre>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</pre> In\u00a0[3]: Copied! <pre>len(words)\n</pre> len(words) Out[3]: <pre>32033</pre> In\u00a0[4]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[5]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[\u00a0]: Copied! <pre># SUMMARY + PYTORCHIFYING -----------\n</pre> # SUMMARY + PYTORCHIFYING ----------- In\u00a0[6]: Copied! <pre># Let's train a deeper network\n# The classes we create here are the same API as nn.Module in PyTorch\n\nclass Linear:\n  \n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      xmean = x.mean(0, keepdim=True) # batch mean\n      xvar = x.var(0, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\nclass Tanh:\n  def __call__(self, x):\n    self.out = torch.tanh(x)\n    return self.out\n  def parameters(self):\n    return []\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 100 # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\n\nC = torch.randn((vocab_size, n_embd),            generator=g)\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n]\n# layers = [\n#   Linear(n_embd * block_size, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, vocab_size),\n# ]\n\nwith torch.no_grad():\n  # last layer: make less confident\n  layers[-1].gamma *= 0.1\n  #layers[-1].weight *= 0.1\n  # all other layers: apply gain\n  for layer in layers[:-1]:\n    if isinstance(layer, Linear):\n      layer.weight *= 1.0 #5/3\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # Let's train a deeper network # The classes we create here are the same API as nn.Module in PyTorch  class Linear:      def __init__(self, fan_in, fan_out, bias=True):     self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5     self.bias = torch.zeros(fan_out) if bias else None      def __call__(self, x):     self.out = x @ self.weight     if self.bias is not None:       self.out += self.bias     return self.out      def parameters(self):     return [self.weight] + ([] if self.bias is None else [self.bias])   class BatchNorm1d:      def __init__(self, dim, eps=1e-5, momentum=0.1):     self.eps = eps     self.momentum = momentum     self.training = True     # parameters (trained with backprop)     self.gamma = torch.ones(dim)     self.beta = torch.zeros(dim)     # buffers (trained with a running 'momentum update')     self.running_mean = torch.zeros(dim)     self.running_var = torch.ones(dim)      def __call__(self, x):     # calculate the forward pass     if self.training:       xmean = x.mean(0, keepdim=True) # batch mean       xvar = x.var(0, keepdim=True) # batch variance     else:       xmean = self.running_mean       xvar = self.running_var     xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance     self.out = self.gamma * xhat + self.beta     # update the buffers     if self.training:       with torch.no_grad():         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean         self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar     return self.out      def parameters(self):     return [self.gamma, self.beta]  class Tanh:   def __call__(self, x):     self.out = torch.tanh(x)     return self.out   def parameters(self):     return []  n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 100 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility  C = torch.randn((vocab_size, n_embd),            generator=g) layers = [   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size), ] # layers = [ #   Linear(n_embd * block_size, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, vocab_size), # ]  with torch.no_grad():   # last layer: make less confident   layers[-1].gamma *= 0.1   #layers[-1].weight *= 0.1   # all other layers: apply gain   for layer in layers[:-1]:     if isinstance(layer, Linear):       layer.weight *= 1.0 #5/3  parameters = [C] + [p for layer in layers for p in layer.parameters()] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>47024\n</pre> In\u00a0[7]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nud = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, Yb) # loss function\n  \n  # backward pass\n  for layer in layers:\n    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n  with torch.no_grad():\n    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n\n  if i &gt;= 1000:\n    break # AFTER_DEBUG: would take out obviously to run full optimization\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] ud = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   x = emb.view(emb.shape[0], -1) # concatenate the vectors   for layer in layers:     x = layer(x)   loss = F.cross_entropy(x, Yb) # loss function      # backward pass   for layer in layers:     layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())   with torch.no_grad():     ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])    if i &gt;= 1000:     break # AFTER_DEBUG: would take out obviously to run full optimization <pre>      0/ 200000: 3.2870\n</pre> In\u00a0[8]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out\n    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('activation distribution')\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i, layer in enumerate(layers[:-1]): # note: exclude the output layer   if isinstance(layer, Tanh):     t = layer.out     print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'layer {i} ({layer.__class__.__name__}') plt.legend(legends); plt.title('activation distribution') <pre>layer 2 (      Tanh): mean -0.00, std 0.63, saturated: 2.78%\nlayer 5 (      Tanh): mean +0.00, std 0.64, saturated: 2.56%\nlayer 8 (      Tanh): mean -0.00, std 0.65, saturated: 2.25%\nlayer 11 (      Tanh): mean +0.00, std 0.65, saturated: 1.69%\nlayer 14 (      Tanh): mean +0.00, std 0.65, saturated: 1.88%\n</pre> Out[8]: <pre>Text(0.5, 1.0, 'activation distribution')</pre> In\u00a0[9]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out.grad\n    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('gradient distribution')\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i, layer in enumerate(layers[:-1]): # note: exclude the output layer   if isinstance(layer, Tanh):     t = layer.out.grad     print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'layer {i} ({layer.__class__.__name__}') plt.legend(legends); plt.title('gradient distribution') <pre>layer 2 (      Tanh): mean +0.000000, std 2.640702e-03\nlayer 5 (      Tanh): mean +0.000000, std 2.245584e-03\nlayer 8 (      Tanh): mean +0.000000, std 2.045741e-03\nlayer 11 (      Tanh): mean -0.000000, std 1.983133e-03\nlayer 14 (      Tanh): mean -0.000000, std 1.952382e-03\n</pre> Out[9]: <pre>Text(0.5, 1.0, 'gradient distribution')</pre> In\u00a0[10]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i,p in enumerate(parameters):\n  t = p.grad\n  if p.ndim == 2:\n    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'{i} {tuple(p.shape)}')\nplt.legend(legends)\nplt.title('weights gradient distribution');\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i,p in enumerate(parameters):   t = p.grad   if p.ndim == 2:     print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'{i} {tuple(p.shape)}') plt.legend(legends) plt.title('weights gradient distribution'); <pre>weight   (27, 10) | mean +0.000000 | std 8.020532e-03 | grad:data ratio 8.012629e-03\nweight  (30, 100) | mean +0.000246 | std 9.241075e-03 | grad:data ratio 4.881090e-02\nweight (100, 100) | mean +0.000113 | std 7.132878e-03 | grad:data ratio 6.964618e-02\nweight (100, 100) | mean -0.000086 | std 6.234302e-03 | grad:data ratio 6.073738e-02\nweight (100, 100) | mean +0.000052 | std 5.742183e-03 | grad:data ratio 5.631479e-02\nweight (100, 100) | mean +0.000032 | std 5.672205e-03 | grad:data ratio 5.570125e-02\nweight  (100, 27) | mean -0.000082 | std 1.209415e-02 | grad:data ratio 1.160105e-01\n</pre> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(20, 4))\nlegends = []\nfor i,p in enumerate(parameters):\n  if p.ndim == 2:\n    plt.plot([ud[j][i] for j in range(len(ud))])\n    legends.append('param %d' % i)\nplt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\nplt.legend(legends);\n</pre> plt.figure(figsize=(20, 4)) legends = [] for i,p in enumerate(parameters):   if p.ndim == 2:     plt.plot([ud[j][i] for j in range(len(ud))])     legends.append('param %d' % i) plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot plt.legend(legends); In\u00a0[12]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, y)\n  print(split, loss.item())\n\n# put layers into eval mode\nfor layer in layers:\n  layer.training = False\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   for layer in layers:     x = layer(x)   loss = F.cross_entropy(x, y)   print(split, loss.item())  # put layers into eval mode for layer in layers:   layer.training = False split_loss('train') split_loss('val') <pre>train 2.4002976417541504\nval 2.3982467651367188\n</pre> In\u00a0[13]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n      for layer in layers:\n        x = layer(x)\n      logits = x\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       emb = C[torch.tensor([context])] # (1,block_size,n_embd)       x = emb.view(emb.shape[0], -1) # concatenate the vectors       for layer in layers:         x = layer(x)       logits = x       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       # shift the context window and track the samples       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break          print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>mria.\nmmyan.\nseelendhnyal.\nrethrsjendrleg.\nade.\nkdieliin.\nmiloen.\nekeisean.\nxarlelleimhlara.\nnoshdh.\nrgshiries.\nkin.\nreneliqxnthacfiu.\nzayvde.\njymeli.\nehs.\nkay.\nmistoyan.\nhal.\nsalyansuiezajelveu.\n</pre>"},{"location":"ZeroToHero/Makemore-part4/","title":"Makemore Part 4","text":""},{"location":"ZeroToHero/Makemore-part4/#language-model-4-backpropagation-ii","title":"LANGUAGE MODEL - 4 (BACKPROPAGATION II)","text":"<p>Timeline: 15th January - 6th February, 2025</p>"},{"location":"ZeroToHero/Makemore-part4/#introduction-and-overview","title":"Introduction and Overview","text":"<p>Welcome to my documentation for Makemore Part 4 from Andrej Karpathy's Neural Networks: Zero to Hero series. In this section we take the 2-layer MLP (with BatchNorm) from the previous video/lecture and backpropagate through it manually without using PyTorch autograd's loss.backward(). So we will be manually backpropagating through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. </p> <p>Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.</p>"},{"location":"ZeroToHero/Makemore-part4/#key-resources-video-codes-and-lecture-notes","title":"Key Resources: Video, Codes and Lecture notes","text":"<ul> <li>I watched the lecture on YouTube: Building Makemore Part 4</li> <li>The colab notebook initial template - Save a copy of this notebook and start working on it as you follow along in the lecture.</li> <li>My notebooks and code implementations will be available within this documentation itself, feel free to use that as a reference as well or If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Language Model 4</li> <li>Notes have been taken whenever necessary and have been marked with timestamps to the video.</li> </ul> <p>Note from the author</p> <p>The format and structure of this particular section of the project will be different from what I've implemented so far, as Andrej Karpathy himself had quoted- \"I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched.\"</p> <p>So keeping this in mind, we will be focusing more on the notebook itself and only making notes whenever absolutely necessary.</p> <p>You will find my notes/key points as comments in the code cells (Apart from the time stamps with necessary headers which will be in their normal format ofcourse)</p> <p> </p> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Makemore-part4/exercise-1/","title":"Exercise 1","text":"<p>Becoming a backprop ninja - Exercise 1</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() print(len(words)) print(max(len(w) for w in words)) print(words[:8]) <pre>32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n</pre> In\u00a0[3]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[4]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n  X, Y = [], []\n\n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):   X, Y = [], []    for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[5]: Copied! <pre># ok biolerplate done, now we get to the action:\n</pre> # ok biolerplate done, now we get to the action: In\u00a0[5]: Copied! <pre># utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n</pre> # utility function we will use later when comparing manual gradients to PyTorch gradients def cmp(s, dt, t):   ex = torch.all(dt == t.grad).item()   app = torch.allclose(dt, t.grad)   maxdiff = (dt - t.grad).abs().max().item()   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') In\u00a0[26]: Copied! <pre>n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass.  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>4137\n</pre> In\u00a0[27]: Copied! <pre>batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n</pre> batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y In\u00a0[28]: Copied! <pre># forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True) #DONE\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... #DONE\nprobs = counts * counts_sum_inv #DONE\nlogprobs = probs.log()  #DONE\nloss = -logprobs[range(n), Yb].mean() #DONE\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss\n</pre> # forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time  emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) #DONE counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... #DONE probs = counts * counts_sum_inv #DONE logprobs = probs.log()  #DONE loss = -logprobs[range(n), Yb].mean() #DONE  # PyTorch backward pass for p in parameters:   p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way           norm_logits, logit_maxes, logits, h, hpreact, bnraw,          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,          embcat, emb]:   t.retain_grad() loss.backward() loss Out[28]: <pre>tensor(3.3221, grad_fn=&lt;NegBackward0&gt;)</pre> <p>EXERCISE 1</p> <p>13:01 to 19:05 <code>cmp('logprobs', dlogprobs, logprobs)</code></p> In\u00a0[9]: Copied! <pre>print(logprobs.shape)\nlogprobs[range(n), Yb]\n</pre> print(logprobs.shape) logprobs[range(n), Yb] <pre>torch.Size([32, 27])\n</pre> Out[9]: <pre>tensor([-4.0562, -3.0820, -3.6629, -3.2621, -4.1229, -3.4201, -3.2428, -3.9554,\n        -3.1259, -4.2500, -3.1582, -1.6256, -2.8483, -2.9654, -2.9990, -3.1882,\n        -3.9132, -3.0643, -3.5065, -3.5153, -2.8832, -3.0837, -4.2941, -4.0007,\n        -3.4440, -2.9220, -3.1386, -3.8946, -2.6488, -3.5292, -3.3408, -3.1560],\n       grad_fn=&lt;IndexBackward0&gt;)</pre> In\u00a0[10]: Copied! <pre>print(Yb.shape)\nYb\n</pre> print(Yb.shape) Yb <pre>torch.Size([32])\n</pre> Out[10]: <pre>tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])</pre> In\u00a0[\u00a0]: Copied! <pre>#simple breakdown\n#now here we know there are 32 examples, for explaination lets assume we only have 3 in total i.e. a,b,c\n\n#loss = - (a + b + c) / 3 ==&gt; so we are basically doing the mean calculation here\n#loss = - (1/3a + 1/3b + 1/3c) ==&gt; same equation\n#so now, when we take the derivative wrt a\n#dloss/da = -1/3 ==&gt;where 3 is the number of elements we consider, so we can also say that it is -1/n, therefore\n#dloss/dn = -1/n\n</pre> #simple breakdown #now here we know there are 32 examples, for explaination lets assume we only have 3 in total i.e. a,b,c  #loss = - (a + b + c) / 3 ==&gt; so we are basically doing the mean calculation here #loss = - (1/3a + 1/3b + 1/3c) ==&gt; same equation #so now, when we take the derivative wrt a #dloss/da = -1/3 ==&gt;where 3 is the number of elements we consider, so we can also say that it is -1/n, therefore #dloss/dn = -1/n In\u00a0[29]: Copied! <pre>#So based on our calculation above\ndlogprobs = torch.zeros_like(logprobs) #same as torch.zeros((32, 27)) as we need the same shape as logprobs. So instead of hardcoding the values we did this\ndlogprobs[range(n), Yb] = -1.0/n #as we need to do it for each of the elements in the array\n\n#Now, lets check\ncmp('logprobs', dlogprobs, logprobs)\n</pre> #So based on our calculation above dlogprobs = torch.zeros_like(logprobs) #same as torch.zeros((32, 27)) as we need the same shape as logprobs. So instead of hardcoding the values we did this dlogprobs[range(n), Yb] = -1.0/n #as we need to do it for each of the elements in the array  #Now, lets check cmp('logprobs', dlogprobs, logprobs) <pre>logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>19:06 to 20:55 <code>cmp('probs', dprobs, probs)</code></p> In\u00a0[30]: Copied! <pre>dprobs = (1.0/probs) * dlogprobs #we had to take the derivative of logprobs, which was 1/x --&gt; d/dx(log(x)) = 1/x \n#then we multiplied it with dlogprobs (the one we calculated before this for the chainrule)\n\ncmp('probs', dprobs, probs)\n</pre> dprobs = (1.0/probs) * dlogprobs #we had to take the derivative of logprobs, which was 1/x --&gt; d/dx(log(x)) = 1/x  #then we multiplied it with dlogprobs (the one we calculated before this for the chainrule)  cmp('probs', dprobs, probs) <pre>probs           | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>20:56 to 26:21 <code>cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)</code></p> In\u00a0[31]: Copied! <pre># probs = counts * counts_sum_inv, now here before we do the multiplication, take a look at the matrix dimensions using `.shape`\n# You would see that `counts` would have 3x3 and `counts_sum_inv` will have 3x1\n# So before the backpropagation calculation, there is 'broadcasting' happening where the value of b is been replicated/broadcasted multiple time across the matrix\n\n# Example\n# c = a * b\n# a[3x3] * b[3x1] ----&gt;\n# a[1,1]*b1 + a[1,2]*b1 + a[1,3]*b1\n# a[2,1]*b2 + a[2,2]*b2 + a[2,3]*b2\n# a[3,1]*b3 + a[3,2]*b3 + a[2,3]*b3\n# ====&gt; c[3x3]\n\n# The point of this is just to show that there are two things happening internally: The broadcasting and then the backpropagation\n\n# (first case) The derivative of c wrt b will be a\n# So, here just `counts` will remain -&gt; then `dprobs` is multiplied because chain rule.\n# Finally, in order to make `dcounts_sum_inv` the same dimension as `counts_sum_inv` we sum all of them by 1 and also keepdims as true\n\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) # So this is our final manually calcualted equation\n\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n</pre> # probs = counts * counts_sum_inv, now here before we do the multiplication, take a look at the matrix dimensions using `.shape` # You would see that `counts` would have 3x3 and `counts_sum_inv` will have 3x1 # So before the backpropagation calculation, there is 'broadcasting' happening where the value of b is been replicated/broadcasted multiple time across the matrix  # Example # c = a * b # a[3x3] * b[3x1] ----&gt; # a[1,1]*b1 + a[1,2]*b1 + a[1,3]*b1 # a[2,1]*b2 + a[2,2]*b2 + a[2,3]*b2 # a[3,1]*b3 + a[3,2]*b3 + a[2,3]*b3 # ====&gt; c[3x3]  # The point of this is just to show that there are two things happening internally: The broadcasting and then the backpropagation  # (first case) The derivative of c wrt b will be a # So, here just `counts` will remain -&gt; then `dprobs` is multiplied because chain rule. # Finally, in order to make `dcounts_sum_inv` the same dimension as `counts_sum_inv` we sum all of them by 1 and also keepdims as true  dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) # So this is our final manually calcualted equation  cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv) <pre>counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>26:26 to 27:56 first contribution of <code>counts</code></p> In\u00a0[32]: Copied! <pre># Here we have to calculate the second half of `dcounts` i.e. (Second case) The derivative of c wrt a will be b\n\ndcounts = counts_sum_inv * dprobs\n\n#but we cant compare it yet as `counts` is later depended on top again as well, which we will check\n</pre> # Here we have to calculate the second half of `dcounts` i.e. (Second case) The derivative of c wrt a will be b  dcounts = counts_sum_inv * dprobs  #but we cant compare it yet as `counts` is later depended on top again as well, which we will check <p>27:57 to 28:59 <code>cmp('counts_sum', dcounts_sum, counts_sum)</code></p> In\u00a0[33]: Copied! <pre># counts_sum_inv = counts_sum**-1\n\n# Okay so for this, the derivative of x^-1 is -(x^-2)\n\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv #Remember for this its the one before the `26:26 to 27:56 first contribution of counts` section\n\ncmp('counts_sum', dcounts_sum, counts_sum)\n</pre> # counts_sum_inv = counts_sum**-1  # Okay so for this, the derivative of x^-1 is -(x^-2)  dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv #Remember for this its the one before the `26:26 to 27:56 first contribution of counts` section  cmp('counts_sum', dcounts_sum, counts_sum) <pre>counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>29:00 to 32:26 <code>cmp('counts', dcounts, counts)</code></p> In\u00a0[34]: Copied! <pre># counts_sum = counts.sum(1, keepdims=True)\n\n# Now here we know the shape of counts_sum is 32 by 1 and the shape of counts is 32 by 27. So we need to broadcast counts_sum 27 times\n# We are dirctly using a PyTorch function where it keeps adding numbers from `counts`\n\ndcounts += torch.ones_like(counts) * dcounts_sum #Also here we are adding `dcounts` as remember this is the second iteration of it, we had calculated one more value of it at the top\n\ncmp('counts', dcounts, counts)\n</pre> # counts_sum = counts.sum(1, keepdims=True)  # Now here we know the shape of counts_sum is 32 by 1 and the shape of counts is 32 by 27. So we need to broadcast counts_sum 27 times # We are dirctly using a PyTorch function where it keeps adding numbers from `counts`  dcounts += torch.ones_like(counts) * dcounts_sum #Also here we are adding `dcounts` as remember this is the second iteration of it, we had calculated one more value of it at the top  cmp('counts', dcounts, counts) <pre>counts          | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>32:27 to 33:13 <code>cmp('norm_logits', dnorm_logits, norm_logits)</code></p> In\u00a0[35]: Copied! <pre># counts = norm_logits.exp()\n\n# Now here, the derivative of `norm_logits.exp()`, now the derivate of e^x is (famously) e^x, so its just `norm_logits.exp()` itself\n# so we can also just write it as `counts` directly  as it holds that value\n\ndnorm_logits = counts * dcounts\n\ncmp('norm_logits', dnorm_logits, norm_logits)\n</pre> # counts = norm_logits.exp()  # Now here, the derivative of `norm_logits.exp()`, now the derivate of e^x is (famously) e^x, so its just `norm_logits.exp()` itself # so we can also just write it as `counts` directly  as it holds that value  dnorm_logits = counts * dcounts  cmp('norm_logits', dnorm_logits, norm_logits) <pre>norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>33:14 to 36:20 <code>cmp('logit_maxes', dlogit_maxes, logit_maxes)</code></p> In\u00a0[36]: Copied! <pre># norm_logits = logits - logit_maxes\n\n# Now here if you would look at the shape of all these variables, you would notice that there is internal broadcasting happening here (logit_maxes)\n\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) #WILL HAVE TO REWATCH THIS PART AGAIN, DIDN'T COMPLETELY GET IT\n\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\n</pre> # norm_logits = logits - logit_maxes  # Now here if you would look at the shape of all these variables, you would notice that there is internal broadcasting happening here (logit_maxes)  dlogits = dnorm_logits.clone() dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) #WILL HAVE TO REWATCH THIS PART AGAIN, DIDN'T COMPLETELY GET IT  cmp('logit_maxes', dlogit_maxes, logit_maxes) <pre>logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>38:27 to 41:44 <code>cmp('logits', dlogits, logits)</code></p> In\u00a0[37]: Copied! <pre># logit_maxes = logits.max(1, keepdim=True).values\n\n# Here, this step is similar to that of the first one in `dlogprobs` where we used torch.zeros_like() function\n# So we are doing another alternative way of doing that\n\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes #Just remember the += here as we already have one dlogits above\n\ncmp('logits', dlogits, logits)\n</pre> # logit_maxes = logits.max(1, keepdim=True).values  # Here, this step is similar to that of the first one in `dlogprobs` where we used torch.zeros_like() function # So we are doing another alternative way of doing that  dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes #Just remember the += here as we already have one dlogits above  cmp('logits', dlogits, logits) <pre>logits          | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>41:45 to 53:25 <code>cmp('h', dh, h)</code>, <code>cmp('W2', dW2, W2)</code> and <code>cmp('b2', db2, b2)</code> - Bckpropagation through a linear layer</p> <p>( Till 49:56 had theoritical proofs on the matrix multiplication )</p> In\u00a0[\u00a0]: Copied! <pre># # Linear layer 2\n# logits = h @ W2 + b2 # output layer\n\n# in `b2` broadcasting is happening\n</pre> # # Linear layer 2 # logits = h @ W2 + b2 # output layer  # in `b2` broadcasting is happening In\u00a0[19]: Copied! <pre># Need these for understanding the matrix mulitplication why we are multiplying with what\ndlogits.shape, h.shape, W2.shape, b2.shape\n</pre> # Need these for understanding the matrix mulitplication why we are multiplying with what dlogits.shape, h.shape, W2.shape, b2.shape Out[19]: <pre>(torch.Size([32, 27]),\n torch.Size([32, 64]),\n torch.Size([64, 27]),\n torch.Size([27]))</pre> In\u00a0[38]: Copied! <pre># watch the last few minutes, probably from 51 to see how he broke down this based on the matrix sizes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\n\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\n</pre> # watch the last few minutes, probably from 51 to see how he broke down this based on the matrix sizes dh = dlogits @ W2.T dW2 = h.T @ dlogits db2 = dlogits.sum(0)  cmp('h', dh, h) cmp('W2', dW2, W2) cmp('b2', db2, b2) <pre>h               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>53:37 to 55:12 <code>cmp('hpreact', dhpreact, hpreact)</code></p> In\u00a0[39]: Copied! <pre># h = torch.tanh(hpreact) # hidden layer\n\ndhpreact = (1.0 - h**2)*dh #we saw that the derivative of tanh is also (1-a^2) where a was the external variable `a`, not the input `z` to tanh i.e. a = tanh(z)\n\ncmp('hpreact', dhpreact, hpreact)\n</pre> # h = torch.tanh(hpreact) # hidden layer  dhpreact = (1.0 - h**2)*dh #we saw that the derivative of tanh is also (1-a^2) where a was the external variable `a`, not the input `z` to tanh i.e. a = tanh(z)  cmp('hpreact', dhpreact, hpreact) <pre>hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>55:13 to 59:38 <code>cmp('bngain', dbngain, bngain)</code></p> In\u00a0[22]: Copied! <pre>bnraw.shape, bngain.shape, bnbias.shape, dhpreact.shape\n</pre> bnraw.shape, bngain.shape, bnbias.shape, dhpreact.shape Out[22]: <pre>(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]))</pre> In\u00a0[40]: Copied! <pre># hpreact = bngain * bnraw + bnbias\n\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True) #because dbraw and dhpreact are 32by64, but dbngain expects 1by64 (we also keep the dimension)\ndbnraw = (bngain * dhpreact)\ndbnbias = (dhpreact).sum(0, keepdim=True) #because dhpreact is 32by64 but the dbnbias expects 1by64 (we also keep the dimension)\n\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\n</pre> # hpreact = bngain * bnraw + bnbias  dbngain = (bnraw * dhpreact).sum(0, keepdim=True) #because dbraw and dhpreact are 32by64, but dbngain expects 1by64 (we also keep the dimension) dbnraw = (bngain * dhpreact) dbnbias = (dhpreact).sum(0, keepdim=True) #because dhpreact is 32by64 but the dbnbias expects 1by64 (we also keep the dimension)  cmp('bngain', dbngain, bngain) cmp('bnbias', dbnbias, bnbias) cmp('bnraw', dbnraw, bnraw) <pre>bngain          | exact: True  | approximate: True  | maxdiff: 0.0\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>59:40 to 1:04:1 <code>cmp('bnvar_inv', dbnvar_inv, bnvar_inv)</code></p> In\u00a0[\u00a0]: Copied! <pre># From here we are working on the batch norm layer\n# the code has been spread out and broken down to different parts (based on the equations on the \"bottom right corner box\" in the paper for batch norm - See prev lecture) inorder to perform manual backprop more easily\n</pre> # From here we are working on the batch norm layer # the code has been spread out and broken down to different parts (based on the equations on the \"bottom right corner box\" in the paper for batch norm - See prev lecture) inorder to perform manual backprop more easily In\u00a0[21]: Copied! <pre>bnraw.shape, bndiff.shape, bnvar_inv.shape\n</pre> bnraw.shape, bndiff.shape, bnvar_inv.shape Out[21]: <pre>(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))</pre> In\u00a0[41]: Copied! <pre># bnraw = bndiff * bnvar_inv\n\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw #We will come back to this in 1:12:43 - (1)\n\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n</pre> # bnraw = bndiff * bnvar_inv  dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) dbndiff = bnvar_inv * dbnraw #We will come back to this in 1:12:43 - (1)  cmp('bnvar_inv', dbnvar_inv, bnvar_inv) <pre>bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:04:15 to 1:05:16 <code>cmp('bnvar', dbnvar, bnvar)</code></p> In\u00a0[42]: Copied! <pre># bnvar_inv = (bnvar + 1e-5)**-0.5\n#This is a direct equation of derivative of x^n so the output should be n*x^n-1\n\ndbnvar = (-0.5 * ((bnvar + 1e-5) ** (-1.5))) * dbnvar_inv\n\ncmp('bnvar', dbnvar, bnvar)\n</pre> # bnvar_inv = (bnvar + 1e-5)**-0.5 #This is a direct equation of derivative of x^n so the output should be n*x^n-1  dbnvar = (-0.5 * ((bnvar + 1e-5) ** (-1.5))) * dbnvar_inv  cmp('bnvar', dbnvar, bnvar) <pre>bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:05:17 to 1:09:01 - Why he implemented the bessel's correction (as there seem to be some problem/issue in the paper. Using Bias during training time and Unbiased during testing). But we prefer to use Unbiased during both training and testing and that is what we went ahead with.</p> <p>1:09:02 to 1:12:42 <code>cmp('bndiff2', dbndiff2, bndiff2)</code></p> In\u00a0[43]: Copied! <pre># bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n\ndbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n\ncmp('bndiff2', dbndiff2, bndiff2)\n</pre> # bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)  dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar  cmp('bndiff2', dbndiff2, bndiff2) <pre>bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:12:43 to 1:13:58 <code>cmp('bndiff', dbndiff, bndiff)</code></p> In\u00a0[44]: Copied! <pre># bndiff2 = bndiff**2\n\ndbndiff += 2*bndiff * dbndiff2 #This is the (2)nd occurance of dbndiff - 59:40 so, we add it here\n\ncmp('bndiff', dbndiff, bndiff)\n</pre> # bndiff2 = bndiff**2  dbndiff += 2*bndiff * dbndiff2 #This is the (2)nd occurance of dbndiff - 59:40 so, we add it here  cmp('bndiff', dbndiff, bndiff) <pre>bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:13:59 to 1:18:35 <code>cmp('bnmeani', dbnmeani, bnmeani)</code> and <code>cmp('hprebn', dhprebn, hprebn)</code></p> In\u00a0[45]: Copied! <pre>## Please go thorugh this one again, i didnt completely get it\n\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n\ndhprebn = dbndiff.clone() #we are making a copy of it\ndbnmeani = (-dbndiff).sum(0)\n\ndhprebn += (1.0/n)*(torch.ones_like(hprebn) * dbnmeani)\n\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\n</pre> ## Please go thorugh this one again, i didnt completely get it  # bnmeani = 1/n*hprebn.sum(0, keepdim=True) # bndiff = hprebn - bnmeani  dhprebn = dbndiff.clone() #we are making a copy of it dbnmeani = (-dbndiff).sum(0)  dhprebn += (1.0/n)*(torch.ones_like(hprebn) * dbnmeani)  cmp('bnmeani', dbnmeani, bnmeani) cmp('hprebn', dhprebn, hprebn) <pre>bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:18:36 to 1:20:34  <code>cmp('embcat', dembcat, embcat)</code>, <code>cmp('W1', dW1, W1)</code> and <code>cmp('b1', db1, b1)</code></p> In\u00a0[47]: Copied! <pre>hprebn.shape, embcat.shape, W1.shape, b1.shape\n</pre> hprebn.shape, embcat.shape, W1.shape, b1.shape Out[47]: <pre>(torch.Size([32, 64]),\n torch.Size([32, 30]),\n torch.Size([30, 64]),\n torch.Size([64]))</pre> In\u00a0[49]: Copied! <pre># Forward pass: hprebn = embcat @ W1 + b1\n\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\n\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\n</pre> # Forward pass: hprebn = embcat @ W1 + b1  dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn db1 = dhprebn.sum(0)  cmp('embcat', dembcat, embcat) cmp('W1', dW1, W1) cmp('b1', db1, b1) <pre>embcat          | exact: True  | approximate: True  | maxdiff: 0.0\nW1              | exact: True  | approximate: True  | maxdiff: 0.0\nb1              | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:20:35 to 1:21:58 <code>cmp('emb', demb, emb)</code></p> In\u00a0[50]: Copied! <pre>## Please rewatch this as well\n\n# embcat = emb.view(emb.shape[0], -1)\n\ndemb = dembcat.view(emb.shape)\n\ncmp('emb', demb, emb)\n</pre> ## Please rewatch this as well  # embcat = emb.view(emb.shape[0], -1)  demb = dembcat.view(emb.shape)  cmp('emb', demb, emb) <pre>emb             | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>1:21:59 to  <code>cmp('C', dC, C)</code></p> In\u00a0[51]: Copied! <pre>## Please rewatch this as well\n# emb = C[Xb]\n\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n  for j in range(Xb.shape[1]):\n    ix = Xb[k,j]\n    dC[ix] += demb[k,j]\n\ncmp('C', dC, C)\n</pre> ## Please rewatch this as well # emb = C[Xb]  dC = torch.zeros_like(C) for k in range(Xb.shape[0]):   for j in range(Xb.shape[1]):     ix = Xb[k,j]     dC[ix] += demb[k,j]  cmp('C', dC, C) <pre>C               | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> <p>And we are done with the first exercise!!</p> In\u00a0[\u00a0]: Copied! <pre># Exercise 1: backprop through the whole thing manually,\n# backpropagating through exactly all of the variables\n# as they are defined in the forward pass above, one by one\n\n# -----------------\n# YOUR CODE HERE :)\n# -----------------\n\n# cmp('logprobs', dlogprobs, logprobs)\n# cmp('probs', dprobs, probs)\n# cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n# cmp('counts_sum', dcounts_sum, counts_sum)\n# cmp('counts', dcounts, counts)\n# cmp('norm_logits', dnorm_logits, norm_logits)\n# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n# cmp('logits', dlogits, logits)\n# cmp('h', dh, h)\n# cmp('W2', dW2, W2)\n# cmp('b2', db2, b2)\n# cmp('hpreact', dhpreact, hpreact)\n# cmp('bngain', dbngain, bngain)\n# cmp('bnbias', dbnbias, bnbias)\n# cmp('bnraw', dbnraw, bnraw)\n# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n# cmp('bnvar', dbnvar, bnvar)\n# cmp('bndiff2', dbndiff2, bndiff2)\n# cmp('bndiff', dbndiff, bndiff)\n# cmp('bnmeani', dbnmeani, bnmeani)\n# cmp('hprebn', dhprebn, hprebn)\n# cmp('embcat', dembcat, embcat)\n# cmp('W1', dW1, W1)\n# cmp('b1', db1, b1)\n# cmp('emb', demb, emb)\n# cmp('C', dC, C)\n</pre> # Exercise 1: backprop through the whole thing manually, # backpropagating through exactly all of the variables # as they are defined in the forward pass above, one by one  # ----------------- # YOUR CODE HERE :) # -----------------  # cmp('logprobs', dlogprobs, logprobs) # cmp('probs', dprobs, probs) # cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv) # cmp('counts_sum', dcounts_sum, counts_sum) # cmp('counts', dcounts, counts) # cmp('norm_logits', dnorm_logits, norm_logits) # cmp('logit_maxes', dlogit_maxes, logit_maxes) # cmp('logits', dlogits, logits) # cmp('h', dh, h) # cmp('W2', dW2, W2) # cmp('b2', db2, b2) # cmp('hpreact', dhpreact, hpreact) # cmp('bngain', dbngain, bngain) # cmp('bnbias', dbnbias, bnbias) # cmp('bnraw', dbnraw, bnraw) # cmp('bnvar_inv', dbnvar_inv, bnvar_inv) # cmp('bnvar', dbnvar, bnvar) # cmp('bndiff2', dbndiff2, bndiff2) # cmp('bndiff', dbndiff, bndiff) # cmp('bnmeani', dbnmeani, bnmeani) # cmp('hprebn', dhprebn, hprebn) # cmp('embcat', dembcat, embcat) # cmp('W1', dW1, W1) # cmp('b1', db1, b1) # cmp('emb', demb, emb) # cmp('C', dC, C)"},{"location":"ZeroToHero/Makemore-part4/exercise-2/","title":"Exercise 2","text":"In\u00a0[\u00a0]: Copied! <pre># This exercise wasn't exactly smooth sailing for me, but I did try to understand most of it. Will try to come back to this whenever I can\n</pre> # This exercise wasn't exactly smooth sailing for me, but I did try to understand most of it. Will try to come back to this whenever I can In\u00a0[\u00a0]: Copied! <pre># there no change change in the first several cells from last lecture\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> # there no change change in the first several cells from last lecture  import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># download the names.txt file from github\n!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n</pre> # download the names.txt file from github !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt In\u00a0[\u00a0]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n# print(len(words))\n# print(max(len(w) for w in words))\n# print(words[:8])\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\n# print(itos)\n# print(vocab_size)\n\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n  X, Y = [], []\n\n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  # print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() # print(len(words)) # print(max(len(w) for w in words)) # print(words[:8])  # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) # print(itos) # print(vocab_size)  # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):   X, Y = [], []    for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   # print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% In\u00a0[3]: Copied! <pre># utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n</pre> # utility function we will use later when comparing manual gradients to PyTorch gradients def cmp(s, dt, t):   ex = torch.all(dt == t.grad).item()   app = torch.allclose(dt, t.grad)   maxdiff = (dt - t.grad).abs().max().item()   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') In\u00a0[4]: Copied! <pre>n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass.  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>4137\n</pre> In\u00a0[5]: Copied! <pre>batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n</pre> batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y In\u00a0[6]: Copied! <pre># forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss\n</pre> # forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time  emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean()  # PyTorch backward pass for p in parameters:   p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way           norm_logits, logit_maxes, logits, h, hpreact, bnraw,          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,          embcat, emb]:   t.retain_grad() loss.backward() loss Out[6]: <pre>tensor(3.3596, grad_fn=&lt;NegBackward0&gt;)</pre> <p>Similar boiler plate codes as done in the prev exercise and provided in the starter code^</p> <p>EXERCISE 2</p> In\u00a0[\u00a0]: Copied! <pre># Exercise 2: backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# logit_maxes = logits.max(1, keepdim=True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdims=True)\n# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\n# loss_fast = F.cross_entropy(logits, Yb)\n# print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n</pre> # Exercise 2: backprop through cross_entropy but all in one go # to complete this challenge look at the mathematical expression of the loss, # take the derivative, simplify the expression, and just write it out  # forward pass  # before: # logit_maxes = logits.max(1, keepdim=True).values # norm_logits = logits - logit_maxes # subtract max for numerical stability # counts = norm_logits.exp() # counts_sum = counts.sum(1, keepdims=True) # counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... # probs = counts * counts_sum_inv # logprobs = probs.log() # loss = -logprobs[range(n), Yb].mean()  # now: # loss_fast = F.cross_entropy(logits, Yb) # print(loss_fast.item(), 'diff:', (loss_fast - loss).item()) <p>In the above example we are seeing how the forward pass is broken down if we do the manual breakdown of calculation vs just directly using PyTorch</p> <p>1:28:34 to 1:32:48 - Andrej sensei gives us an hint followed with an explaination of solving the equation and convert that to code</p> In\u00a0[7]: Copied! <pre># backward pass\n\ndlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1\ndlogits /= n\n\ncmp('logits', dlogits, logits)\n\n#This wasnt exactly very clear to me, but i will come back to this\n#Also my output came slightly bigger than sensei's though\n</pre> # backward pass  dlogits = F.softmax(logits, 1) dlogits[range(n), Yb] -= 1 dlogits /= n  cmp('logits', dlogits, logits)  #This wasnt exactly very clear to me, but i will come back to this #Also my output came slightly bigger than sensei's though <pre>logits          | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n</pre> <p>1:32:49 to 1:36:36 - Breakdown of what <code>dlogits</code> actually is by taking one row and representing it dynamically</p> In\u00a0[8]: Copied! <pre>logits.shape, Yb.shape\n</pre> logits.shape, Yb.shape Out[8]: <pre>(torch.Size([32, 27]), torch.Size([32]))</pre> In\u00a0[9]: Copied! <pre>F.softmax(logits, 1)[0]\n</pre> F.softmax(logits, 1)[0] Out[9]: <pre>tensor([0.0727, 0.0823, 0.0164, 0.0532, 0.0213, 0.0895, 0.0218, 0.0357, 0.0174,\n        0.0327, 0.0371, 0.0337, 0.0347, 0.0311, 0.0346, 0.0131, 0.0086, 0.0178,\n        0.0161, 0.0499, 0.0532, 0.0226, 0.0259, 0.0712, 0.0607, 0.0274, 0.0192],\n       grad_fn=&lt;SelectBackward0&gt;)</pre> In\u00a0[10]: Copied! <pre>dlogits[0] * n\n</pre> dlogits[0] * n Out[10]: <pre>tensor([ 0.0727,  0.0823,  0.0164,  0.0532,  0.0213,  0.0895,  0.0218,  0.0357,\n        -0.9826,  0.0327,  0.0371,  0.0337,  0.0347,  0.0311,  0.0346,  0.0131,\n         0.0086,  0.0178,  0.0161,  0.0499,  0.0532,  0.0226,  0.0259,  0.0712,\n         0.0607,  0.0274,  0.0192], grad_fn=&lt;MulBackward0&gt;)</pre> In\u00a0[11]: Copied! <pre>dlogits[0].sum()\n</pre> dlogits[0].sum() Out[11]: <pre>tensor(2.0955e-09, grad_fn=&lt;SumBackward0&gt;)</pre> In\u00a0[13]: Copied! <pre>plt.figure(figsize=(8,8))\nplt.imshow(dlogits.detach(), cmap='gray')\n</pre> plt.figure(figsize=(8,8)) plt.imshow(dlogits.detach(), cmap='gray') Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x1b1aabfa7a0&gt;</pre>"},{"location":"ZeroToHero/Makemore-part4/exercise-3/","title":"Exercise 3","text":"In\u00a0[\u00a0]: Copied! <pre># This exercise wasn't exactly smooth sailing for me, but I did try to understand most of it. Will try to come back to this whenever I can\n</pre> # This exercise wasn't exactly smooth sailing for me, but I did try to understand most of it. Will try to come back to this whenever I can In\u00a0[1]: Copied! <pre># there no change change in the first several cells from last lecture\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> # there no change change in the first several cells from last lecture  import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># download the names.txt file from github\n!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n</pre> # download the names.txt file from github !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt In\u00a0[3]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n# print(len(words))\n# print(max(len(w) for w in words))\n# print(words[:8])\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\n# print(itos)\n# print(vocab_size)\n\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n  X, Y = [], []\n\n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  # print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() # print(len(words)) # print(max(len(w) for w in words)) # print(words[:8])  # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) # print(itos) # print(vocab_size)  # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):   X, Y = [], []    for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   # print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% In\u00a0[4]: Copied! <pre># utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n</pre> # utility function we will use later when comparing manual gradients to PyTorch gradients def cmp(s, dt, t):   ex = torch.all(dt == t.grad).item()   app = torch.allclose(dt, t.grad)   maxdiff = (dt - t.grad).abs().max().item()   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') In\u00a0[9]: Copied! <pre>n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass.  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>4137\n</pre> In\u00a0[10]: Copied! <pre>batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n</pre> batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y In\u00a0[11]: Copied! <pre># forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss\n</pre> # forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time  emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean()  # PyTorch backward pass for p in parameters:   p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way           norm_logits, logit_maxes, logits, h, hpreact, bnraw,          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,          embcat, emb]:   t.retain_grad() loss.backward() loss Out[11]: <pre>tensor(3.3479, grad_fn=&lt;NegBackward0&gt;)</pre> In\u00a0[12]: Copied! <pre>#The entire Exercise 1 implementation combined\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ndprobs = (1.0 / probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\ndhpreact = (1.0 - h**2) * dh\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\ndbnmeani = (-dbndiff).sum(0)\ndhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(emb.shape)\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n  for j in range(Xb.shape[1]):\n    ix = Xb[k,j]\n    dC[ix] += demb[k,j]\n</pre> #The entire Exercise 1 implementation combined  dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0/n dprobs = (1.0 / probs) * dlogprobs dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) dcounts = counts_sum_inv * dprobs dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv dcounts += torch.ones_like(counts) * dcounts_sum dnorm_logits = counts * dcounts dlogits = dnorm_logits.clone() dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes dh = dlogits @ W2.T dW2 = h.T @ dlogits db2 = dlogits.sum(0) dhpreact = (1.0 - h**2) * dh dbngain = (bnraw * dhpreact).sum(0, keepdim=True) dbnraw = bngain * dhpreact dbnbias = dhpreact.sum(0, keepdim=True) dbndiff = bnvar_inv * dbnraw dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar dbndiff += (2*bndiff) * dbndiff2 dhprebn = dbndiff.clone() dbnmeani = (-dbndiff).sum(0) dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn db1 = dhprebn.sum(0) demb = dembcat.view(emb.shape) dC = torch.zeros_like(C) for k in range(Xb.shape[0]):   for j in range(Xb.shape[1]):     ix = Xb[k,j]     dC[ix] += demb[k,j] <p>Similar boiler plate codes as done in the prev exercise and provided in the starter code^</p> <p>EXERCISE 3</p> <p>1:36:38 to 1:48:35 - Pen and Paper derivation</p> <p>1:48:36 to  - Implementation of the derivation in code</p> In\u00a0[13]: Copied! <pre># Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n# BatchNorm paper: https://arxiv.org/abs/1502.03167\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())\n</pre> # Exercise 3: backprop through batchnorm but all in one go # to complete this challenge look at the mathematical expression of the output of batchnorm, # take the derivative w.r.t. its input, simplify the expression, and just write it out # BatchNorm paper: https://arxiv.org/abs/1502.03167  # forward pass  # before: # bnmeani = 1/n*hprebn.sum(0, keepdim=True) # bndiff = hprebn - bnmeani # bndiff2 = bndiff**2 # bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) # bnvar_inv = (bnvar + 1e-5)**-0.5 # bnraw = bndiff * bnvar_inv # hpreact = bngain * bnraw + bnbias  # now: hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias print('max diff:', (hpreact_fast - hpreact).abs().max()) <pre>max diff: tensor(7.1526e-07, grad_fn=&lt;MaxBackward1&gt;)\n</pre> In\u00a0[14]: Copied! <pre># backward pass\n\n# before we had:\n# dbnraw = bngain * dhpreact\n# dbndiff = bnvar_inv * dbnraw\n# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n# dbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n\n# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n# (you'll also need to use some of the variables from the forward pass up above)\n\n#This is a direct implementation of what sensei did, as he said in the video this equation itself has a lot of breakdown steps to be considered\n#But this is what we come up with at the end\ndhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n\ncmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10\n</pre> # backward pass  # before we had: # dbnraw = bngain * dhpreact # dbndiff = bnvar_inv * dbnraw # dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) # dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv # dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar # dbndiff += (2*bndiff) * dbndiff2 # dhprebn = dbndiff.clone() # dbnmeani = (-dbndiff).sum(0) # dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)  # calculate dhprebn given dhpreact (i.e. backprop through the batchnorm) # (you'll also need to use some of the variables from the forward pass up above)  #This is a direct implementation of what sensei did, as he said in the video this equation itself has a lot of breakdown steps to be considered #But this is what we come up with at the end dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))  cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10 <pre>hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n</pre>"},{"location":"ZeroToHero/Makemore-part4/final-code/","title":"Final code","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># download the names.txt file from github\n!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n</pre> # download the names.txt file from github !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt In\u00a0[3]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() print(len(words)) print(max(len(w) for w in words)) print(words[:8]) <pre>32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n</pre> In\u00a0[4]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[5]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n  X, Y = [], []\n\n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):   X, Y = [], []    for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[6]: Copied! <pre># utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n</pre> # utility function we will use later when comparing manual gradients to PyTorch gradients def cmp(s, dt, t):   ex = torch.all(dt == t.grad).item()   app = torch.allclose(dt, t.grad)   maxdiff = (dt - t.grad).abs().max().item()   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') <p>FINAL CODE</p> <p>1:50:03 to 1:54:25 - Putting all of the codes together to form a Neural Net, but by commenting out the <code>loss.backward()</code> :)</p> In\u00a0[7]: Copied! <pre># Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\n\n# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n  # kick off optimization\n  for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n    bnvar_inv = (bnvar + 1e-5)**-0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n      p.grad = None\n    #loss.backward() # use this for correctness comparisons, delete it later!\n\n    # manual backprop! #swole_doge_meme\n    # -----------------\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(n), Yb] -= 1\n    dlogits /= n\n    # 2nd layer backprop\n    dh = dlogits @ W2.T\n    dW2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n    # 1st layer\n    dembcat = dhprebn @ W1.T\n    dW1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    for k in range(Xb.shape[0]):\n      for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j]\n    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n    # -----------------\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p, grad in zip(parameters, grads):\n      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n      p.data += -lr * grad # new way of swole doge TODO: enable\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n  #   if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net\n  #     break\n</pre> # Exercise 4: putting it all together! # Train the MLP neural net with your own backward pass  # init n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True  # same optimization as last time max_steps = 200000 batch_size = 32 n = batch_size # convenience lossi = []  # use this context manager for efficiency once your backward pass is written (TODO) with torch.no_grad():    # kick off optimization   for i in range(max_steps):      # minibatch construct     ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)     Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass     emb = C[Xb] # embed the characters into vectors     embcat = emb.view(emb.shape[0], -1) # concatenate the vectors     # Linear layer     hprebn = embcat @ W1 + b1 # hidden layer pre-activation     # BatchNorm layer     # -------------------------------------------------------------     bnmean = hprebn.mean(0, keepdim=True)     bnvar = hprebn.var(0, keepdim=True, unbiased=True)     bnvar_inv = (bnvar + 1e-5)**-0.5     bnraw = (hprebn - bnmean) * bnvar_inv     hpreact = bngain * bnraw + bnbias     # -------------------------------------------------------------     # Non-linearity     h = torch.tanh(hpreact) # hidden layer     logits = h @ W2 + b2 # output layer     loss = F.cross_entropy(logits, Yb) # loss function      # backward pass     for p in parameters:       p.grad = None     #loss.backward() # use this for correctness comparisons, delete it later!      # manual backprop! #swole_doge_meme     # -----------------     dlogits = F.softmax(logits, 1)     dlogits[range(n), Yb] -= 1     dlogits /= n     # 2nd layer backprop     dh = dlogits @ W2.T     dW2 = h.T @ dlogits     db2 = dlogits.sum(0)     # tanh     dhpreact = (1.0 - h**2) * dh     # batchnorm backprop     dbngain = (bnraw * dhpreact).sum(0, keepdim=True)     dbnbias = dhpreact.sum(0, keepdim=True)     dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))     # 1st layer     dembcat = dhprebn @ W1.T     dW1 = embcat.T @ dhprebn     db1 = dhprebn.sum(0)     # embedding     demb = dembcat.view(emb.shape)     dC = torch.zeros_like(C)     for k in range(Xb.shape[0]):       for j in range(Xb.shape[1]):         ix = Xb[k,j]         dC[ix] += demb[k,j]     grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]     # -----------------      # update     lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay     for p, grad in zip(parameters, grads):       #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())       p.data += -lr * grad # new way of swole doge TODO: enable      # track stats     if i % 10000 == 0: # print every once in a while       print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')     lossi.append(loss.log10().item())    #   if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net   #     break <pre>12297\n      0/ 200000: 3.8069\n  10000/ 200000: 2.1598\n  20000/ 200000: 2.4110\n  30000/ 200000: 2.4295\n  40000/ 200000: 2.0158\n  50000/ 200000: 2.4050\n  60000/ 200000: 2.3825\n  70000/ 200000: 2.0596\n  80000/ 200000: 2.3024\n  90000/ 200000: 2.2073\n 100000/ 200000: 2.0443\n 110000/ 200000: 2.2937\n 120000/ 200000: 2.0340\n 130000/ 200000: 2.4557\n 140000/ 200000: 2.2876\n 150000/ 200000: 2.2016\n 160000/ 200000: 1.9720\n 170000/ 200000: 1.8015\n 180000/ 200000: 2.0065\n 190000/ 200000: 1.9932\n</pre> In\u00a0[\u00a0]: Copied! <pre># Looking at this, probably the batch norm layer backward pass was the most complicated one\n# Otherwise, the rest of them were pretty straight forward :)\n</pre> # Looking at this, probably the batch norm layer backward pass was the most complicated one # Otherwise, the rest of them were pretty straight forward :) In\u00a0[\u00a0]: Copied! <pre># useful for checking your gradients\n# for p,g in zip(parameters, grads):\n#   cmp(str(tuple(p.shape)), g, p)\n</pre> # useful for checking your gradients # for p,g in zip(parameters, grads): #   cmp(str(tuple(p.shape)), g, p) In\u00a0[8]: Copied! <pre># calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n  # pass the training set through\n  emb = C[Xtr]\n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1\n  # measure the mean/std over the entire training set\n  bnmean = hpreact.mean(0, keepdim=True)\n  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n</pre> # calibrate the batch norm at the end of training  with torch.no_grad():   # pass the training set through   emb = C[Xtr]   embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 + b1   # measure the mean/std over the entire training set   bnmean = hpreact.mean(0, keepdim=True)   bnvar = hpreact.var(0, keepdim=True, unbiased=True) In\u00a0[9]: Copied! <pre># evaluate train and val loss\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 + b1\n  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> # evaluate train and val loss  @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   hpreact = embcat @ W1 + b1   hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias   h = torch.tanh(hpreact) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.0708959102630615\nval 2.1080715656280518\n</pre> In\u00a0[\u00a0]: Copied! <pre># Okay probably relatively slightly lower but thats cool\n</pre> # Okay probably relatively slightly lower but thats cool In\u00a0[10]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # ------------\n      # forward pass:\n      # Embedding\n      emb = C[torch.tensor([context])] # (1,block_size,d)      \n      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n      hpreact = embcat @ W1 + b1\n      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n      h = torch.tanh(hpreact) # (N, n_hidden)\n      logits = h @ W2 + b2 # (N, vocab_size)\n      # ------------\n      # Sample\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # ------------       # forward pass:       # Embedding       emb = C[torch.tensor([context])] # (1,block_size,d)             embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)       hpreact = embcat @ W1 + b1       hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias       h = torch.tanh(hpreact) # (N, n_hidden)       logits = h @ W2 + b2 # (N, vocab_size)       # ------------       # Sample       probs = F.softmax(logits, dim=1)       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       if ix == 0:         break          print(''.join(itos[i] for i in out)) <pre>mora.\nmayah.\nsee.\nmad.\nryla.\nreisha.\nendraegan.\nchedielin.\nshi.\njen.\neden.\nsana.\narleigh.\nmalaia.\nnoshubergshira.\nsten.\njoselle.\njose.\ncasubenteda.\njamell.\n</pre> In\u00a0[\u00a0]: Copied! <pre># I've definetly got some wayyy better names here through most are gibberish xD\n</pre> # I've definetly got some wayyy better names here through most are gibberish xD <p>And that marks the end of exploring the basic understanding of the 'intuition' of training NN using (traditional) methods. We will be moving on to more complex ones from here on - like RNN etc. So looking forward to that :)</p>"},{"location":"ZeroToHero/Makemore-part5/","title":"Makemore Part 5","text":""},{"location":"ZeroToHero/Makemore-part5/#language-model-5","title":"LANGUAGE MODEL - 5","text":"<p>Timeline: 8th - 9th February, 2025</p>"},{"location":"ZeroToHero/Makemore-part5/#introduction","title":"Introduction","text":"<p>Welcome to my documentation for Makemore Part 5 from Andrej Karpathy's Neural Networks: Zero to Hero series. In this section, we evolve our model from a simple 2-layer MLP into a deeper, tree-like convolutional architecture inspired by WaveNet (2016) from DeepMind. This part not only demonstrates how to scale up the network but also provides insights into the inner workings of torch.nn and the iterative deep learning development process.</p>"},{"location":"ZeroToHero/Makemore-part5/#overview-of-makemore-part-5","title":"Overview of Makemore Part 5","text":"<p>In this installment, the following key topics are explored:</p> <p>Transitioning from MLP to a Deep Convolutional Network: We start by transforming the basic 2-layer MLP into a more complex, hierarchical architecture. This transformation is achieved by adopting a tree-like structure that significantly expands the model\u2019s receptive field and learning capacity.</p> <p>Key Concepts Covered:</p> <ul> <li> <p>Deepening the Architecture:   The video illustrates how to make a neural network deeper by introducing additional layers and branching structures. This deeper model is more adept at capturing complex patterns in the data.</p> </li> <li> <p>Tree-Like Convolutional Structure:   The approach mimics the WaveNet architecture by using a hierarchical layout. Although WaveNet employs causal dilated convolutions for efficient modeling (which are not yet covered in this series), the current implementation lays the groundwork for such advanced techniques.</p> </li> <li> <p>From Fully Connected Layers to Convolutional Layers:   By converting parts of the MLP into convolutional layers, the model is better equipped to handle sequential and spatial data. This step highlights the versatility of convolutional networks in various deep learning tasks.</p> </li> <li> <p>Understanding torch.nn Internals:   A detailed look is provided into how torch.nn modules operate behind the scenes. This deep dive is essential for debugging, optimizing, and extending neural network architectures in PyTorch.</p> </li> <li> <p>Iterative Deep Learning Development Process:   The video emphasizes the iterative nature of designing and refining deep neural networks. From experimenting with network depth to monitoring performance metrics, the development process is showcased as both systematic and exploratory.</p> </li> </ul>"},{"location":"ZeroToHero/Makemore-part5/#key-resources","title":"Key Resources","text":"<p>Video Lecture</p> <ul> <li>I watched the lecture on YouTube: Building Makemore Part 5</li> </ul> <p>Codes:</p> <ul> <li>The Jupyter notebooks and code implementations are available within this documentation itself.</li> <li>If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Language Model 5</li> </ul>"},{"location":"ZeroToHero/Makemore-part5/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li>The lecture documentation has a single file notes and notebook respectively where everything is covered.</li> <li>Notes have been marked with timestamps to the video.</li> </ul> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/","title":"LECTURE NOTES","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#set-a-intro","title":"Set A - Intro","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#introduction","title":"Introduction","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-000000","title":"Timestamp: 00:00:00","text":"<p>We are continuing with the implementation of MLP, we've seen how the NN takes in three inputs and predicts the fourth one with a single hidden layer in the middle (The Diagram from the paper referred in part 3).</p> <p>In this, we would like to complexify that architecture where we feed a sequence of input characters instead of just three. - Now, we won't be passing all of those sequence of characters into a single hidden layer because that will squash the information too much. - Instead, we will be going with an approach which has like a dense number of layers and it progressively implements/processes the sequence of characters (in the form of a tree, bottom up) until it predicts the next character in the sequence.</p> <p>So, as we see this architecture that we are going to complexify, we will notice that we will be arriving at something that will look very much like the architecture of a wavenet.  - Wavenet is also a language model developed by DeepMind in 2016, but it predicts a audio sequences unlike or word level sequences. - Looking at the architecture, they have implemented Auto Regression where they are sequentially predicting the next audio sequence and the diagram flow you would see like a deep tree like architecture.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#starter-code-walkthrough","title":"Starter code walkthrough","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-000140","title":"Timestamp: 00:01:40","text":"<p>The starting section of code is very similar to the ones implemented in part 3, more importantly the API implementations of the PyTorch <code>torch.nn</code> library, where we have made similar implementations of the different Layers that can be added (Linear, BatchNorm1d etc.) - There are some changes done in these, so refer those.</p> <p>What we are going to do is to improve this model now.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#lets-fix-the-learning-rate-plot","title":"Let\u2019s fix the learning rate plot","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-000656","title":"Timestamp: 00:06:56","text":"<p>Okay so the first thing we are doing is to fix the graph that we had plotted (which also supposingly we had improved from that 'hockey stick' appearance). Turns out we are getting that appearance because the number of inputs in our batch norm layer were way too less, so its almost like its bouncing up and down between these two thresholds, so its either too correct (likely) or really wrong (unlikely),  so this creates a very thick loss function.</p> <ul> <li>So, if we look at the values in <code>lossi</code>, they are a list of float values (really long decimal point). What we have to do if to kind of calculate their average values. (I am not exactly sure why we are specifically implementing this next step of converting them into matrix rows and then finding the mean, but maybe i can come back to this in the future and have my 'aha!' moment)</li> <li>So what we are going to do is to convert all those values in <code>lossi</code> first of all into tensors and then arrange them in the form of a matrix <code>torch.tensor(lossi).view(-1, 1000)</code></li> <li>The <code>-1</code> is almost like a dynamic placement, so instead of us specifically mentioning the number of rows, pytorch itself checks the input values and creates them. In our case it turned out to be 200x1000 (so each row has 1000 consecutive elements). Lastly we take the mean on every row <code>torch.tensor(lossi).view(-1, 1000).mean(1)</code>. We plot that and we see a much thinner graph plot.</li> <li>In the plot we see that there is a lot of progress and suddenly a drop for a moment, which is the learning rate decay. Values after that it showed us the local minimum values of that optimization.</li> </ul> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#pytorchifying-our-code-layers-containers-torchnn-fun-bugs","title":"Pytorchifying our code: layers, containers, torch.nn, fun bugs","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-000916","title":"Timestamp: 00:09:16","text":"<p>In this section he converts most of the codes from Part 3 into a more PyTorch implementation mimic version.</p> <p>The important concepts covered here would be: - Embedding, Flattening - These operations were performed in the forward pass of the NN and turns out PyTorch also have implemented such modules. Embedding for Indexing essentially and Flattening for (as the name suggests) flattening the layer. - Containers - Sequential, Dict, List etc. So there is something called PyTorch Containers which have a way of organizing layers into Lists or dicts. So in particular there is something called Sequential which is basically a list of layers.</p> Note <p>As you might have noticed, we are basically implementing all of <code>torch.nn</code> here in these vids to see how they work under the hood.</p> <ul> <li>Lastly, there was a bug which he had encountered (root cause was from batchnorm which was in training mode(?)) which he has explained, but i didn't get it completely. I guess I will have to comeback to it once i understand and implement/practice this a lot more.</li> </ul> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#set-b-implementing-wavenet","title":"Set B - Implementing wavenet","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#overview-wavenet","title":"Overview: WaveNet","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-001711","title":"Timestamp: 00:17:11","text":"<p>Same explaination as provided in the start. The structure of the wavenet being like a tree and instead of stuffing all the inputs into a hidden layer and then passing them to next ones like a sandwich (how the part MLP was implemented) we will be slowly processing them.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#dataset-bump-the-context-size-to-8-and-re-running-baseline-code-on-block_size-8","title":"Dataset bump the context size to 8 and Re-running baseline code on block_size 8","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-001933-to-001955","title":"Timestamp: 00:19:33 to 00:19:55","text":"<p>In the above two timestamps, he changes the input character context from 3 to 8 and we immediately see an improvement in the final loss validation (which improved when we had increased the input context). We could go on and optimize it further but we are moving on to implement the wavenet model.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#implementing-wavenet","title":"Implementing WaveNet","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-002136","title":"Timestamp: 00:21:36","text":"<p>Okay so essentially he spends time designing how the inputs need to be provided to the layers, he pairs them together (like how we see in the diagram) in odd and even numbers <code>(1, 2) (3, 4) (5, 6) (7, 8)</code>. Then he takes each of those two pairs and passes them into a layer.</p> <p>The layer he has created in called <code>FlattenConsequetive</code> which essentially squashes the input dimensions each time it is passed through. We have taken 8 examples to work on and 3 hidden layers (as of now, its been copy pasted three times to show a basic functionality) <pre><code>FlattenConsecutive(2), Linear(n_embed * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh()\n\nFlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh()\n\nFlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh()\n</code></pre></p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#training-the-wavenet-first-pass","title":"Training the WaveNet: first pass","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-003741","title":"Timestamp: 00:37:41","text":"<p>He trains the above implementation, changes the values of the number of neurons in the hidden layer <code>n_hidden</code> and ultimately we get the same performance results.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#fixing-batchnorm1d-bug-and-re-training-wavenet-with-bug-fix","title":"Fixing batchnorm1d bug and Re-training WaveNet with bug fix","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-003850-to-004521","title":"Timestamp: 00:38:50 to 00:45:21","text":"<p>Turns out there is a bug in the batch norm implementation, as it runs but doesn't do the right thing. The first overview that we see is that, in the original implementation of our <code>batchnorm</code> module, it takes in two dimensional inputs, but the reason it doesn't throw any errors is because broadcasting is happening perfectly (i see how powerful it can be now lol).</p> <p>Now the fix is, we make it dynamic i.e. we add a set of conditions where the dimensions passed will be based on the size/input values provided. So rather than passing a fixed value, we do this. Essentially we are treating each dimension as a batch dimension which is what we want. Finally, we run that and we do see a small improvement in the final validation. But again, now we are ensuring that everything is functioning as expected.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#scaling-up-our-wavenet-and-experimental-harness","title":"Scaling up our WaveNet and Experimental harness","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-004607-to-004658","title":"Timestamp: 00:46:07 to 00:46:58","text":"<p>Now since everything was setup we try to increase the scale of this architecture i.e. we increase the parameter values. So we bumped up <code>n_embd</code> and <code>n_hidden</code>, keeping the rest of the architecture essentially the same. The training took quite a bit longer but our validation value became a lot lower! We finally went pass our usual barrier value that we have been seeing all along.</p> <p>But it also turns out that, we could obviously experiment more with these values, for optimizing it i.e. w.r.t the hyperparameters and the learning rates and so on. That is because to perform these experimental runs is taking a lot of time, so that 'experimental harness' where we can run various experiments and tune this architecture really well, is something we are not able to do for now.</p> <p> </p>"},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#set-c-conclusions","title":"Set C - Conclusions","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#conclusion","title":"Conclusion","text":""},{"location":"ZeroToHero/Makemore-part5/Main-makemore-part-5/#timestamp-005134-to-005417","title":"Timestamp: 00:51:34 to 00:54:17","text":"<p>By far my most favorite section of the series, perfectly summarizes everything we have done so far and what to actually expect in the upcoming lectures. We have officially 'unlocked a new skill' to actually implement NN training by completely using PyTorch. Going ahead with RNN, Transformers etc. </p> <p>So if you have made it till here as well, YAY! Now on to the next one :)</p>"},{"location":"ZeroToHero/Makemore-part5/Main-notebook-part-5/","title":"Jupyter Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># download the names.txt file from github\n!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n</pre> # download the names.txt file from github !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt In\u00a0[\u00a0]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() print(len(words)) print(max(len(w) for w in words)) print(words[:8]) <pre>32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n</pre> In\u00a0[\u00a0]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[\u00a0]: Copied! <pre># shuffle up the words\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n</pre> # shuffle up the words import random random.seed(42) random.shuffle(words) In\u00a0[\u00a0]: Copied! <pre># build the dataset\nblock_size = 8 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n  X, Y = [], []\n\n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 8 # context length: how many characters do we take to predict the next one?  def build_dataset(words):   X, Y = [], []    for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  n1 = int(0.8*len(words)) n2 = int(0.9*len(words)) Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 8]) torch.Size([182625])\ntorch.Size([22655, 8]) torch.Size([22655])\ntorch.Size([22866, 8]) torch.Size([22866])\n</pre> In\u00a0[\u00a0]: Copied! <pre>for x,y in zip(Xtr[:20], Ytr[:20]):\n  print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n</pre> for x,y in zip(Xtr[:20], Ytr[:20]):   print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()]) <pre>........ --&gt; y\n.......y --&gt; u\n......yu --&gt; h\n.....yuh --&gt; e\n....yuhe --&gt; n\n...yuhen --&gt; g\n..yuheng --&gt; .\n........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n........ --&gt; x\n.......x --&gt; a\n......xa --&gt; v\n.....xav --&gt; i\n....xavi --&gt; e\n</pre> In\u00a0[\u00a0]: Copied! <pre># Near copy paste of the layers we have developed in Part 3\n\n# -----------------------------------------------------------------------------------------------\nclass Linear:\n\n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n    self.bias = torch.zeros(fan_out) if bias else None\n\n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n\n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n# -----------------------------------------------------------------------------------------------\nclass BatchNorm1d:\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      if x.ndim == 2:\n        dim = 0\n      elif x.ndim == 3:\n        dim = (0,1)\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\n# -----------------------------------------------------------------------------------------------\nclass Tanh:\n  def __call__(self, x):\n    self.out = torch.tanh(x)\n    return self.out\n  def parameters(self):\n    return []\n\n# -----------------------------------------------------------------------------------------------\nclass Embedding:\n\n  def __init__(self, num_embeddings, embedding_dim):\n    self.weight = torch.randn((num_embeddings, embedding_dim))\n\n  def __call__(self, IX):\n    self.out = self.weight[IX]\n    return self.out\n\n  def parameters(self):\n    return [self.weight]\n\n# -----------------------------------------------------------------------------------------------\nclass FlattenConsecutive:\n\n  def __init__(self, n):\n    self.n = n\n\n  def __call__(self, x):\n    B, T, C = x.shape\n    x = x.view(B, T//self.n, C*self.n)\n    if x.shape[1] == 1:\n      x = x.squeeze(1)\n    self.out = x\n    return self.out\n\n  def parameters(self):\n    return []\n\n# -----------------------------------------------------------------------------------------------\nclass Sequential:\n\n  def __init__(self, layers):\n    self.layers = layers\n\n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    self.out = x\n    return self.out\n\n  def parameters(self):\n    # get parameters of all layers and stretch them out into one list\n    return [p for layer in self.layers for p in layer.parameters()]\n</pre> # Near copy paste of the layers we have developed in Part 3  # ----------------------------------------------------------------------------------------------- class Linear:    def __init__(self, fan_in, fan_out, bias=True):     self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init     self.bias = torch.zeros(fan_out) if bias else None    def __call__(self, x):     self.out = x @ self.weight     if self.bias is not None:       self.out += self.bias     return self.out    def parameters(self):     return [self.weight] + ([] if self.bias is None else [self.bias])  # ----------------------------------------------------------------------------------------------- class BatchNorm1d:    def __init__(self, dim, eps=1e-5, momentum=0.1):     self.eps = eps     self.momentum = momentum     self.training = True     # parameters (trained with backprop)     self.gamma = torch.ones(dim)     self.beta = torch.zeros(dim)     # buffers (trained with a running 'momentum update')     self.running_mean = torch.zeros(dim)     self.running_var = torch.ones(dim)    def __call__(self, x):     # calculate the forward pass     if self.training:       if x.ndim == 2:         dim = 0       elif x.ndim == 3:         dim = (0,1)       xmean = x.mean(dim, keepdim=True) # batch mean       xvar = x.var(dim, keepdim=True) # batch variance     else:       xmean = self.running_mean       xvar = self.running_var     xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance     self.out = self.gamma * xhat + self.beta     # update the buffers     if self.training:       with torch.no_grad():         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean         self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar     return self.out    def parameters(self):     return [self.gamma, self.beta]  # ----------------------------------------------------------------------------------------------- class Tanh:   def __call__(self, x):     self.out = torch.tanh(x)     return self.out   def parameters(self):     return []  # ----------------------------------------------------------------------------------------------- class Embedding:    def __init__(self, num_embeddings, embedding_dim):     self.weight = torch.randn((num_embeddings, embedding_dim))    def __call__(self, IX):     self.out = self.weight[IX]     return self.out    def parameters(self):     return [self.weight]  # ----------------------------------------------------------------------------------------------- class FlattenConsecutive:    def __init__(self, n):     self.n = n    def __call__(self, x):     B, T, C = x.shape     x = x.view(B, T//self.n, C*self.n)     if x.shape[1] == 1:       x = x.squeeze(1)     self.out = x     return self.out    def parameters(self):     return []  # ----------------------------------------------------------------------------------------------- class Sequential:    def __init__(self, layers):     self.layers = layers    def __call__(self, x):     for layer in self.layers:       x = layer(x)     self.out = x     return self.out    def parameters(self):     # get parameters of all layers and stretch them out into one list     return [p for layer in self.layers for p in layer.parameters()]  In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42); # seed rng for reproducibility\n</pre> torch.manual_seed(42); # seed rng for reproducibility In\u00a0[\u00a0]: Copied! <pre># original network\n# n_embd = 10 # the dimensionality of the character embedding vectors\n# n_hidden = 300 # the number of neurons in the hidden layer of the MLP\n# model = Sequential([\n#   Embedding(vocab_size, n_embd),\n#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n#   Linear(n_hidden, vocab_size),\n# ])\n\n# hierarchical network\nn_embd = 24 # the dimensionality of the character embedding vectors\nn_hidden = 128 # the number of neurons in the hidden layer of the MLP\nmodel = Sequential([\n  Embedding(vocab_size, n_embd),\n  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n  model.layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # original network # n_embd = 10 # the dimensionality of the character embedding vectors # n_hidden = 300 # the number of neurons in the hidden layer of the MLP # model = Sequential([ #   Embedding(vocab_size, n_embd), #   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), #   Linear(n_hidden, vocab_size), # ])  # hierarchical network n_embd = 24 # the dimensionality of the character embedding vectors n_hidden = 128 # the number of neurons in the hidden layer of the MLP model = Sequential([   Embedding(vocab_size, n_embd),   FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(n_hidden, vocab_size), ])  # parameter init with torch.no_grad():   model.layers[-1].weight *= 0.1 # last layer make less confident  parameters = model.parameters() print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>76579\n</pre> In\u00a0[\u00a0]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n\n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n  # forward pass\n  logits = model(Xb)\n  loss = F.cross_entropy(logits, Yb) # loss function\n\n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n\n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):    # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,))   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y    # forward pass   logits = model(Xb)   loss = F.cross_entropy(logits, Yb) # loss function    # backward pass   for p in parameters:     p.grad = None   loss.backward()    # update: simple SGD   lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())  <pre>      0/ 200000: 3.3167\n  10000/ 200000: 2.0576\n  20000/ 200000: 2.0723\n  30000/ 200000: 2.5134\n  40000/ 200000: 2.1476\n  50000/ 200000: 1.7836\n  60000/ 200000: 2.2592\n  70000/ 200000: 1.9331\n  80000/ 200000: 1.6875\n  90000/ 200000: 2.0395\n 100000/ 200000: 1.7736\n 110000/ 200000: 1.9570\n 120000/ 200000: 1.7465\n 130000/ 200000: 1.8126\n 140000/ 200000: 1.7406\n 150000/ 200000: 1.7466\n 160000/ 200000: 1.8806\n 170000/ 200000: 1.6266\n 180000/ 200000: 1.6476\n 190000/ 200000: 1.8555\n</pre> In\u00a0[\u00a0]: Copied! <pre>plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n</pre> plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1)) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fb5a03e3b50&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre># put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n  layer.training = False\n</pre> # put layers into eval mode (needed for batchnorm especially) for layer in model.layers:   layer.training = False In\u00a0[\u00a0]: Copied! <pre># evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  logits = model(x)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> # evaluate the loss @torch.no_grad() # this decorator disables gradient tracking inside pytorch def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   logits = model(x)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 1.7690284252166748\nval 1.9936515092849731\n</pre> <p>Performance log</p> <ul> <li>original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</li> <li>context: 3 -&gt; 8 (22K params): train 1.918, val 2.027</li> <li>flat -&gt; hierarchical (22K params): train 1.941, val 2.029</li> <li>fix bug in batchnorm: train 1.912, val 2.022</li> <li>scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993</li> </ul> In\u00a0[\u00a0]: Copied! <pre># sample from the model\nfor _ in range(20):\n\n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      logits = model(torch.tensor([context]))\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n\n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model for _ in range(20):      out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       logits = model(torch.tensor([context]))       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1).item()       # shift the context window and track the samples       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break      print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>arlij.\nchetta.\nheago.\nrocklei.\nhendrix.\njamylie.\nbroxin.\ndenish.\nanslibt.\nmarianah.\nastavia.\nannayve.\naniah.\njayce.\nnodiel.\nremita.\nniyelle.\njaylene.\naiyan.\naubreana.\n</pre> <p>Why convolutions? Brief preview/hint</p> In\u00a0[\u00a0]: Copied! <pre>for x,y in zip(Xtr[7:15], Ytr[7:15]):\n  print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n</pre> for x,y in zip(Xtr[7:15], Ytr[7:15]):   print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()]) <pre>........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n</pre> In\u00a0[\u00a0]: Copied! <pre># forward a single example:\nlogits = model(Xtr[[7]])\nlogits.shape\n</pre> # forward a single example: logits = model(Xtr[[7]]) logits.shape Out[\u00a0]: <pre>torch.Size([1, 27])</pre> In\u00a0[\u00a0]: Copied! <pre># forward all of them\nlogits = torch.zeros(8, 27)\nfor i in range(8):\n  logits[i] = model(Xtr[[7+i]])\nlogits.shape\n</pre> # forward all of them logits = torch.zeros(8, 27) for i in range(8):   logits[i] = model(Xtr[[7+i]]) logits.shape Out[\u00a0]: <pre>torch.Size([8, 27])</pre> In\u00a0[\u00a0]: Copied! <pre># convolution is a \"for loop\"\n# allows us to forward Linear layers efficiently over space\n</pre> # convolution is a \"for loop\" # allows us to forward Linear layers efficiently over space"},{"location":"ZeroToHero/Micrograd/","title":"Micrograd","text":""},{"location":"ZeroToHero/Micrograd/#backpropagation-using-an-autograd-engine","title":"BACKPROPAGATION - Using an AutoGrad Engine","text":"<p>Timeline: 2nd - 27th October, 2024</p>"},{"location":"ZeroToHero/Micrograd/#introduction","title":"Introduction","text":"<p>Welcome to my documentation for the Micrograd video from Andrej Karpathy's Neural Networks: Zero to Hero series. This lecture provides a detailed exploration of Micrograd, a tiny autograd engine designed for educational purposes. In this documentation, I\u2019ve compiled my notes and insights from the lecture to serve as a reference for understanding the core concepts of automatic differentiation and backpropagation in neural networks.</p>"},{"location":"ZeroToHero/Micrograd/#overview-of-micrograd","title":"Overview of Micrograd","text":"<p>In this part of the series, I focused on the following key topics:</p> <p>Understanding Micrograd: Micrograd is an autograd engine that simplifies the process of building and training neural networks. It allows users to define mathematical expressions and automatically compute gradients, which are essential for optimizing neural network weights.</p> <p>Key Concepts Covered:</p> <ul> <li> <p>Core Value Object: The foundation of Micrograd is the Value object, which holds both a value and its gradient. This object is crucial for tracking computations and gradients during backpropagation.</p> </li> <li> <p>Forward Pass: The forward pass involves computing the output of a neural network given an input and its weights. This step is essential for evaluating the performance of the model.</p> </li> <li> <p>Backpropagation Process: Backpropagation is explained as a recursive application of the chain rule in calculus, allowing us to calculate gradients efficiently. This process is vital for updating weights during training.</p> </li> <li> <p>Building Mathematical Expressions: The lecture demonstrates how to create expressions using basic operations (addition, multiplication) and how these can be visualized as computation graphs.</p> </li> <li> <p>Implementing Neural Network Structures: The video walks through creating a simple multi-layer perceptron (MLP) using Micrograd, illustrating how minimal code can lead to effective neural network training.</p> </li> </ul>"},{"location":"ZeroToHero/Micrograd/#key-resources","title":"Key Resources","text":"<p>Video Lecture</p> <ul> <li>I watched the lecture on YouTube: Building Micrograd</li> </ul> <p>Codes:</p> <ul> <li>The Jupyter notebooks and code implementations are available within this documentation itself.</li> <li>If you wish to view the repository where I originally worked on, you can view it here: Neural Networks - Micrograd Implementation)</li> <li>The Official full implementation of Micrograd is available on GitHub: Micrograd Repository</li> </ul>"},{"location":"ZeroToHero/Micrograd/#structure-of-contents","title":"Structure of Contents","text":"<ul> <li> <p>There are two main sections: Lecture Notes and Notebooks.</p> </li> <li> <p>The documentation for this lecture has been divided based on the topic, as there were many individual concepts that had to be convered.</p> </li> <li> <p>Everything has been arranged in order, you just have to follow the default navigation path provided.</p> </li> <li> <p>The main notebook with timestamps is under 'Master Notes'.</p> </li> <li> <p>The additional notes can be navigated through the Master notes itself. If you feel like jumping directly to a topic, feel free to do so.</p> </li> <li> <p>Important: It is recommended to start from the 'Master Notes' page, this is your main page for the lecture, you'll be guided to the rest of the topics in the necessary sections within this page itself.</p> </li> <li> <p>The jupyter notebooks have also been divided and arranged based on the respective topic for simpler navigation and understanding.</p> </li> </ul> <p> </p> <p>Have fun, Happy Learning!</p>"},{"location":"ZeroToHero/Micrograd/notebooks/1-derivative-simple-function/","title":"Derivative Simple Function","text":"In\u00a0[1]: Copied! <pre>import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre># Now define a function, a scaler value function f(x)\ndef f(x):\n    return 3*x**2 - 4*x +5\n</pre> # Now define a function, a scaler value function f(x) def f(x):     return 3*x**2 - 4*x +5 In\u00a0[4]: Copied! <pre># Now we can just pass in some value to check\nf(3.0)\n</pre> # Now we can just pass in some value to check f(3.0) Out[4]: <pre>20.0</pre> In\u00a0[7]: Copied! <pre># The f(x) equation as you can see is a Quadratic equation, precisely a Parabola\n# So now, we try to plot it\n\n# Now, we'll just add like a range of values to feed in\n\n# We'll start with x-axis values so, from -5 to 5 (Not including 5) in the steps of 0.25\n# Therefore creating a numpy array\nxs = np.arange(-5,5,0.25)\nxs\n</pre> # The f(x) equation as you can see is a Quadratic equation, precisely a Parabola # So now, we try to plot it  # Now, we'll just add like a range of values to feed in  # We'll start with x-axis values so, from -5 to 5 (Not including 5) in the steps of 0.25 # Therefore creating a numpy array xs = np.arange(-5,5,0.25) xs Out[7]: <pre>array([-5.  , -4.75, -4.5 , -4.25, -4.  , -3.75, -3.5 , -3.25, -3.  ,\n       -2.75, -2.5 , -2.25, -2.  , -1.75, -1.5 , -1.25, -1.  , -0.75,\n       -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ,  1.25,  1.5 ,\n        1.75,  2.  ,  2.25,  2.5 ,  2.75,  3.  ,  3.25,  3.5 ,  3.75,\n        4.  ,  4.25,  4.5 ,  4.75])</pre> In\u00a0[8]: Copied! <pre># Now for the y-axis values, we call each of those elements in the numpy array to the function f(x)\n\n# Therefore we create an another numpy array which containes the values after applying the function to each of the elements in xs\nys = f(xs)\nys\n</pre> # Now for the y-axis values, we call each of those elements in the numpy array to the function f(x)  # Therefore we create an another numpy array which containes the values after applying the function to each of the elements in xs ys = f(xs) ys Out[8]: <pre>array([100.    ,  91.6875,  83.75  ,  76.1875,  69.    ,  62.1875,\n        55.75  ,  49.6875,  44.    ,  38.6875,  33.75  ,  29.1875,\n        25.    ,  21.1875,  17.75  ,  14.6875,  12.    ,   9.6875,\n         7.75  ,   6.1875,   5.    ,   4.1875,   3.75  ,   3.6875,\n         4.    ,   4.6875,   5.75  ,   7.1875,   9.    ,  11.1875,\n        13.75  ,  16.6875,  20.    ,  23.6875,  27.75  ,  32.1875,\n        37.    ,  42.1875,  47.75  ,  53.6875])</pre> In\u00a0[9]: Copied! <pre># And now we plot this using matplotlib\nplt.plot(xs, ys)\n</pre> # And now we plot this using matplotlib plt.plot(xs, ys) Out[9]: <pre>[&lt;matplotlib.lines.Line2D at 0x2274508baf0&gt;]</pre> <p>Now we need to see what is the derivative of this function f(x) at any single input point x   So what is the derivative at different point in the x-axis to the function f(x)</p> <p></p> <p>Now, we can check the derivative of the value by considering a very small value of h (almost close to zero)</p> In\u00a0[10]: Copied! <pre>h = 0.001\nx = 3.0\n</pre> h = 0.001 x = 3.0 In\u00a0[12]: Copied! <pre># Now, if we take the f(x) value directly, we know we get 20.0 (Already done in above cells)\n\n# Lets say what if we add the value of h to it, so we are nudging it to a slighly more positive direction\n# So the value must be slightly more than 20\n\nf(x+h)\n</pre> # Now, if we take the f(x) value directly, we know we get 20.0 (Already done in above cells)  # Lets say what if we add the value of h to it, so we are nudging it to a slighly more positive direction # So the value must be slightly more than 20  f(x+h) Out[12]: <pre>20.014003000000002</pre> In\u00a0[14]: Copied! <pre># Now, by how much that above value has increased shows us the strength or the size of that slope\n\n# Therefore, Next we see how much the function has responded\n\nf(x+h) - f(x)\n</pre> # Now, by how much that above value has increased shows us the strength or the size of that slope  # Therefore, Next we see how much the function has responded  f(x+h) - f(x) Out[14]: <pre>0.01400300000000243</pre> In\u00a0[15]: Copied! <pre># Next we have to normalise it by adding the value rise of the run i.e. h, to get the value of the slope\n\n(f(x+h) - f(x))/h\n</pre> # Next we have to normalise it by adding the value rise of the run i.e. h, to get the value of the slope  (f(x+h) - f(x))/h Out[15]: <pre>14.00300000000243</pre> <p>Therefore, at 3 i.e. when x=3, the slope is 14.   You can see the same value if you calculate it manually using the derivative formula:  =&gt; Derivative of 3x^2 - 4x +5  =&gt; 6x-4  =&gt; 6(3) - 4  =&gt; 18 -4  =&gt; 14</p> In\u00a0[18]: Copied! <pre># Now what if we add a negative value for x?\n# Then even the function will become negative. Therefore, we will be getting a negative sign slope\n\nh = 0.00000001  # Cant make this too small, as unlike in theory, computer can handle only a finite amount. Therefore make it too small and it will directly return 0 :)\nx = -3.0\n(f(x+h) - f(x))/h\n</pre> # Now what if we add a negative value for x? # Then even the function will become negative. Therefore, we will be getting a negative sign slope  h = 0.00000001  # Cant make this too small, as unlike in theory, computer can handle only a finite amount. Therefore make it too small and it will directly return 0 :) x = -3.0 (f(x+h) - f(x))/h  Out[18]: <pre>-22.00000039920269</pre> In\u00a0[19]: Copied! <pre># Now at some point the slop must be 0, therefore nudging a small value either way from that point, it still remains 0\n# In this case, for the function it is at around x = 2/3\n\nh = 0.00000001\nx = 2/3\n(f(x+h) - f(x))/h\n</pre> # Now at some point the slop must be 0, therefore nudging a small value either way from that point, it still remains 0 # In this case, for the function it is at around x = 2/3  h = 0.00000001 x = 2/3 (f(x+h) - f(x))/h Out[19]: <pre>0.0</pre>"},{"location":"ZeroToHero/Micrograd/notebooks/2-derivative-function-with-multiple-inputs/","title":"Derivative Function with Multiple Inputs","text":"In\u00a0[1]: Copied! <pre># Now we get slighly more complex from the previous one\n\n# We are adding 3 scalar inputs\na = 2.0\nb = -3.0\nc = 10.0\n\n#Single ouput\nd = a*b + c\n</pre> # Now we get slighly more complex from the previous one  # We are adding 3 scalar inputs a = 2.0 b = -3.0 c = 10.0  #Single ouput d = a*b + c In\u00a0[2]: Copied! <pre>print(d)\n</pre> print(d) <pre>4.0\n</pre> <p>Now we are gonna look at the derivitive of d wrt to a,b &amp; c</p> In\u00a0[3]: Copied! <pre># We'll set the value of h\nh = 0.0001\n</pre> # We'll set the value of h h = 0.0001 In\u00a0[11]: Copied! <pre>#inputs\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\n</pre> #inputs a = 2.0 b = -3.0 c = 10.0  d1 = a*b + c <p>(i) wrt a</p> In\u00a0[6]: Copied! <pre>a += h\n\nd2 = a*b + c\n</pre> a += h  d2 = a*b + c In\u00a0[7]: Copied! <pre>print('d1', d1)\nprint('d2', d2)\n\n# Here (d2 - d1) is the amount of bump that was actually increased, after we had bump the value of the specific value that we are interested in by a tiny amount\n# And that is normalised by dividing that value by h, to get the slope\nprint('Slope: ', (d2 - d1)/h)\n</pre> print('d1', d1) print('d2', d2)  # Here (d2 - d1) is the amount of bump that was actually increased, after we had bump the value of the specific value that we are interested in by a tiny amount # And that is normalised by dividing that value by h, to get the slope print('Slope: ', (d2 - d1)/h) <pre>d1 4.0\nd2 3.999699999999999\nSlope:  -3.000000000010772\n</pre> <p>Now here with the ouput d2, it becomes slightly less than 4 after we had added a small value to a   As the negative value formed after the product is bigger   Now, since we know the overall value had reduced, the sign of the slope should also naturally be negative (as seen)</p> <p>Even Mathematically, if we take the derivative of the expression d2 wrt a  then only b is left  which is a negative value, hence the negative sign</p> <p>(ii) wrt b</p> In\u00a0[10]: Copied! <pre>b += h\n\nd2 = a*b + c\n\nprint('d1', d1)\nprint('d2', d2)\n\nprint('Slope: ', (d2 - d1)/h)\n</pre> b += h  d2 = a*b + c  print('d1', d1) print('d2', d2)  print('Slope: ', (d2 - d1)/h) <pre>d1 4.0\nd2 4.0002\nSlope:  2.0000000000042206\n</pre> <p>Similar explaination here   the value of b becomes slightly less after adding the small increament  Therefore increasing the value of the final expression d2   Here, since its the derivative of d2 wrt b  only a remains, therefore a positive slope value</p> <p>(iii) wrt c</p> In\u00a0[12]: Copied! <pre>c += h\n\nd2 = a*b + c\n\nprint('d1', d1)\nprint('d2', d2)\n\nprint('Slope: ', (d2 - d1)/h)\n</pre> c += h  d2 = a*b + c  print('d1', d1) print('d2', d2)  print('Slope: ', (d2 - d1)/h) <pre>d1 4.0\nd2 4.0001\nSlope:  0.9999999999976694\n</pre> <p>Here, we add a slightly bit higher value to c   For this derivative, a and b are not considered.  The function or d2 becomes slightly bit higher because of c  It becomes slightly bit higher by the exact amount that was added to c   Therefore cancelling out and the value of the slope is hence 1</p> <p>Therefore, this will be the derivative at which 'd' will increase, as we scale 'c'</p>"},{"location":"ZeroToHero/Micrograd/notebooks/3-value-object/","title":"Value Object","text":"<p>Expression: d = a * b + c</p> In\u00a0[6]: Copied! <pre>class Value:\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data)\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data * other.data)\n        return out\n</pre> class Value:      def __init__(self, data):         self.data = data      def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"          def __add__(self, other):         out = Value(self.data + other.data)         return out          def __mul__(self, other):         out = Value(self.data * other.data)         return out In\u00a0[12]: Copied! <pre>a = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\n</pre> a = Value(2.0) b = Value(-3.0) c = Value(10.0) In\u00a0[9]: Copied! <pre>a*b + c\n</pre> a*b + c Out[9]: <pre>Value(data=4.0)</pre> <p>SAME THING!! (Up and Down cells)</p> In\u00a0[10]: Copied! <pre>(a.__mul__(b)).__add__(c)\n</pre> (a.__mul__(b)).__add__(c) Out[10]: <pre>Value(data=4.0)</pre> <p>Visualization of the expression</p> In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op=''):         self.data = data         self._prev = set(_children)         self._op = _op      def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"          def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out          def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[13]: Copied! <pre>d = a*b + c\n</pre> d = a*b + c In\u00a0[14]: Copied! <pre>d._prev\n</pre> d._prev Out[14]: <pre>{Value(data=-6.0), Value(data=10.0)}</pre> In\u00a0[15]: Copied! <pre>d._op\n</pre> d._op Out[15]: <pre>'+'</pre> In\u00a0[4]: Copied! <pre>a = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\n\nd= a*b + c\nd\n</pre> a = Value(2.0) b = Value(-3.0) c = Value(10.0)  d= a*b + c d Out[4]: <pre>Value(data=4.0)</pre> In\u00a0[1]: Copied! <pre>%pip install graphviz\n</pre> %pip install graphviz <pre>Collecting graphviz\n  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\nInstalling collected packages: graphviz\nSuccessfully installed graphviz-0.20.3\nNote: you may need to restart the kernel to use updated packages.\n</pre> <pre>WARNING: You are using pip version 21.2.3; however, version 24.2 is available.\nYou should consider upgrading via the 'c:\\Users\\imdiv\\OneDrive\\Desktop\\GitHub Portal\\Micrograd\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n</pre> In\u00a0[9]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ data %.4f }\" % ( n.data, ), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node (in a circle/oval shape to distinguish) for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ data %.4f }\" % ( n.data, ), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node (in a circle/oval shape to distinguish) for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(d)\n</pre> draw_dot(d) <p>NOTE: The graph will only get generated if you have graphviz installed locally on your machine and the env variable path needs to be set.</p> <p>If you want to avoid downloading it locally, you can directly run this on Google Colab to view the output, as graphviz is already preinstalled for you</p> <p>Check 3_1-graph-visualisation.ipynb for viewing the outputs</p>"},{"location":"ZeroToHero/Micrograd/notebooks/3_1-graph-visualisation/","title":"Graph Visualisation","text":"<p>Note: Start from 3-value-object.ipynb and then come back to this notebook</p> In\u00a0[1]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op=''):         self.data = data         self._prev = set(_children)         self._op = _op      def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[2]: Copied! <pre>a = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\n\nd= a*b + c\nd\n</pre> a = Value(2.0) b = Value(-3.0) c = Value(10.0)  d= a*b + c d Out[2]: <pre>Value(data=4.0)</pre> In\u00a0[3]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ data %.4f }\" % ( n.data, ), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ data %.4f }\" % ( n.data, ), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[4]: Copied! <pre>draw_dot(d)\n</pre> draw_dot(d) Out[4]: <p>Same graph, but we are just adding labels</p> In\u00a0[5]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self._prev = set(_children)         self._op = _op         self.label = label      def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[6]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nd\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' d Out[6]: <pre>Value(data=4.0)</pre> In\u00a0[7]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f }\" % ( n.label, n.data), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f }\" % ( n.label, n.data), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[8]: Copied! <pre>draw_dot(d)\n</pre> draw_dot(d) Out[8]: <p>Adding another layer to this NN</p> In\u00a0[9]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[9]: <pre>Value(data=-8.0)</pre> In\u00a0[10]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f }\" % ( n.label, n.data), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f }\" % ( n.label, n.data), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[11]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[11]:"},{"location":"ZeroToHero/Micrograd/notebooks/4_0_manual_backpropagation_simpleExpression/","title":"Manual Backpropagation-0","text":"<p>INITIAL TEMPLATE</p> <p>Will be useful for the remaining lectures or calculations within this section</p> In\u00a0[\u00a0]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[\u00a0]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[\u00a0]: <pre>Value(data=-8.0)</pre> In\u00a0[\u00a0]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]:"},{"location":"ZeroToHero/Micrograd/notebooks/4_1_manual_backpropagation_simpleExpression/","title":"Manual Backpropagation-1","text":"In\u00a0[2]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[17]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[17]: <pre>Value(data=-8.0)</pre> In\u00a0[7]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Now, let's start to fill those grad values</p> <p>Let's first find the derivative of L w.r.t L</p> In\u00a0[6]: Copied! <pre>#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.001\n\n  #Here we are basically making them as local variables, to not affect the global variables on top\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data + h\n\n  print((L2-L1)/h)\n\nlol()\n</pre> #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.001    #Here we are basically making them as local variables, to not affect the global variables on top   a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data + h    print((L2-L1)/h)  lol() <pre>1.000000000000334\n</pre> <p>This was theoritically obvious as well. The derivitive of L wrt L will be one.</p> <p> </p> <p>So, lets add that value manually. (Remember to run the global variables for this)</p> In\u00a0[10]: Copied! <pre>L.grad = 1.0\n</pre> L.grad = 1.0 In\u00a0[11]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[11]: <p>Now, we find the derivative of L wrt to f and d</p> <p>So, mathematically:</p> <p>dL/dd = ?</p> <p>L = d * f</p> <p>Therefore, dL/dd = f</p> <p>If we do manual calculation to verify,           \\</p> <p>=&gt; f(x+h) - f(x) / h                                \\</p> <p>(Remember the f(x) is basically L here)           =&gt; (d+h)f - df / h                                 =&gt; df + hf - df / h                                  =&gt; hf/h                                              = f</p> <p>So here if you see,</p> <p>The derivative of L wrt f is the value in d  &amp;  The derivative of L wrt d is the value in f</p> <p>So, grad f is 4.0  and grad d is -2.0</p> <p> </p> <p>Lets check this in code!</p> In\u00a0[12]: Copied! <pre># STARTING WITH d\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  d.data = d.data + h\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # STARTING WITH d  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   d.data = d.data + h   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>-2.000000000000668\n</pre> In\u00a0[15]: Copied! <pre># NOW WITH f\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0 + h, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH f  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0 + h, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>4.000000000026205\n</pre> <p>So, now that we have verified that mathematically and on our code. Lets manually add those variables to the graph</p> In\u00a0[19]: Copied! <pre>f.grad = 4.0\nd.grad = -2.0\n</pre> f.grad = 4.0 d.grad = -2.0 In\u00a0[20]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[20]:"},{"location":"ZeroToHero/Micrograd/notebooks/4_2_manual_backpropagation_simpleExpression/","title":"Manual Backpropagation-2","text":"In\u00a0[20]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[21]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[21]: <pre>Value(data=-8.0)</pre> In\u00a0[22]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Now, let's start to fill those grad values</p> <p>Let's first find the derivative of L w.r.t L</p> In\u00a0[\u00a0]: Copied! <pre>#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.001\n\n  #Here we are basically making them as local variables, to not affect the global variables on top\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data + h\n\n  print((L2-L1)/h)\n\nlol()\n</pre> #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.001    #Here we are basically making them as local variables, to not affect the global variables on top   a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data + h    print((L2-L1)/h)  lol() <pre>1.000000000000334\n</pre> <p>This was theoritically obvious as well. The derivitive of L wrt L will be one.</p> <p> </p> <p>So, lets add that value manually. (Remember to run the global variables for this)</p> In\u00a0[23]: Copied! <pre>L.grad = 1.0\n</pre> L.grad = 1.0 In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Now, we find the derivative of L wrt to f and d</p> <p>So, mathematically:</p> <p>dL/dd = ?</p> <p>L = d * f</p> <p>Therefore, dL/dd = f</p> <p>If we do manual calculation to verify,           \\</p> <p>=&gt; f(x+h) - f(x) / h                                \\</p> <p>(Remember the f(x) is basically L here)           =&gt; (d+h)f - df / h                                 =&gt; df + hf - df / h                                  =&gt; hf/h                                              = f</p> <p>So here if you see,</p> <p>The derivative of L wrt f is the value in d  &amp;  The derivative of L wrt d is the value in f</p> <p>So, grad f is 4.0  and grad d is -2.0</p> <p> </p> <p>Lets check this in code!</p> In\u00a0[\u00a0]: Copied! <pre># STARTING WITH d\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  d.data = d.data + h\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # STARTING WITH d  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   d.data = d.data + h   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>-2.000000000000668\n</pre> In\u00a0[\u00a0]: Copied! <pre># NOW WITH f\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0 + h, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH f  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0 + h, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>4.000000000026205\n</pre> <p>So, now that we have verified that mathematically and on our code. Lets manually add those variables to the graph</p> In\u00a0[24]: Copied! <pre>f.grad = 4.0\nd.grad = -2.0\n</pre> f.grad = 4.0 d.grad = -2.0 In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>VERY IMPORTANT PART</p> <p>Now we'll be calculating the derivatives of the middle nodes</p> <p>Starting with c &amp; e</p> <p>dL/dd had already been calculated (Check end of 4_1-manual-backpropagation notebook)</p> <p>d = c + e</p> <p>now, Derivative of d wrt c, will be 1  Derivative of d wrt e, will be 1 \\</p> <p> </p> <p>Because the derivative of '+' operation variables will lead to 1 (Calculus basics, it leads to constant, so 1)</p> <p> </p> <p>If we try to prove this mathematically:</p> <p> </p> <pre><code>d = c + e\nf(x+h) - f(x) / h\nNow, we'll calculate wrt c\n=&gt; ( ((c+h)+e) - (c+e) ) / h\n=&gt; c + h + e - c - e / h\n=&gt; h / h\n=&gt; 1\nTherefore, dd/dc = 1</code></pre> <p> </p> <p>Therefore, we can just substitute the value respectively.</p> <p>For node c: dL/dc = dL/dd . dd/dc  So here, the values should be -&gt; dL/dc = -2.0 * 1 = -2.0</p> <p>For node e: dL/de = dL/dd . dd/de  So here, the values should be -&gt; dL/de = -2.0 * 1 = -2.0</p> In\u00a0[\u00a0]: Copied! <pre># NOW WITH c\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0 + h, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH c  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0 + h, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>-1.999999987845058\n</pre> In\u00a0[\u00a0]: Copied! <pre># NOW WITH e\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  e.data += h\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH e  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   e.data += h   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>-1.9999999999242843\n</pre> In\u00a0[25]: Copied! <pre># Therefore, we now add those values manually\nc.grad = -2.0\ne.grad = -2.0\n</pre> # Therefore, we now add those values manually c.grad = -2.0 e.grad = -2.0 In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Continuing with a &amp; b</p> <p>Same principle as above, but a different kind of equation here.</p> <p> </p> <p>Also remember here, derivative of L wrt e was just calculated above^ (dL/de)</p> <p>e = a * b</p> <p>Therefore, Derivative of e wrt a, will be b Derivative of e wrt b, will be a</p> <p> </p> <p>Because the derivative of the same variable at the denominator gets out, so the other variable in the product remains (Calculus derivative theory itself) d/da(a * b) = b</p> <p> </p> <p>If we try to prove this mathematically,</p> <p> </p> <pre><code>e = a * b\nf(x+h) - f(x) / h\nRemember, f(x) is equation here. So, finding wrt a, substituting the values\n=&gt; ( ((a + h) * b) - (a * b) ) / h\n=&gt; ab + hb - ab / h\n=&gt; hb / h\n=&gt; b\nTherefore, de/da = b</code></pre> <p> </p> <p>Therefore, we can just substitute the value respectively.</p> <p>For node a: dL/da = dL/de . dd/da  So here, the values should be -&gt; dL/da = -2.0 * -3.0 = 6.0</p> <p>For node b: dL/db = dL/de . dd/db  So here, the values should be -&gt; dL/db = -2.0 * 2.0 = -4.0</p> In\u00a0[\u00a0]: Copied! <pre># NOW WITH a\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0 + h, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH a  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0 + h, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>6.000000000128124\n</pre> In\u00a0[\u00a0]: Copied! <pre># NOW WITH b\n\n#This is just a staging function to show how the calculation of each of the derivative is taking place\ndef lol():\n\n  h = 0.00001\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L1 = L.data #L is basically a node, so we need its data\n\n  a = Value(2.0, label='a')\n  b = Value(-3.0 + h, label='b')\n  c = Value(10.0, label='c')\n  e = a*b; e.label='e'\n  d= e + c; d.label='d'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label='L'\n  L2 = L.data\n\n  print((L2-L1)/h)\n\nlol()\n</pre> # NOW WITH b  #This is just a staging function to show how the calculation of each of the derivative is taking place def lol():    h = 0.00001    a = Value(2.0, label='a')   b = Value(-3.0, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L1 = L.data #L is basically a node, so we need its data    a = Value(2.0, label='a')   b = Value(-3.0 + h, label='b')   c = Value(10.0, label='c')   e = a*b; e.label='e'   d= e + c; d.label='d'   f = Value(-2.0, label='f')   L = d*f; L.label='L'   L2 = L.data    print((L2-L1)/h)  lol() <pre>-4.000000000026205\n</pre> In\u00a0[26]: Copied! <pre>#Now, we add those values manually\na.grad = 6.0\nb.grad = -4.0\n</pre> #Now, we add those values manually a.grad = 6.0 b.grad = -4.0 In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Hence the FINAL GENERATED GRAPH AFTER MANUAL BACKPROPAGATION!!</p> In\u00a0[27]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[27]:"},{"location":"ZeroToHero/Micrograd/notebooks/5_optimization_single_step_preview/","title":"Optimization Single Step Preview","text":"In\u00a0[1]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[2]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[2]: <pre>Value(data=-8.0)</pre> In\u00a0[3]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[4]: Copied! <pre>L.grad = 1.0\nf.grad = 4.0\nd.grad = -2.0\nc.grad = -2.0\ne.grad = -2.0\na.grad = 6.0\nb.grad = -4.0\n</pre> L.grad = 1.0 f.grad = 4.0 d.grad = -2.0 c.grad = -2.0 e.grad = -2.0 a.grad = 6.0 b.grad = -4.0 In\u00a0[5]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[5]: <p>Now, we are going to try and nudge the leaf nodes (because those are usually what we have control over. In our example: a,b,c,f) slightly towards the gradient value, to nudge L towards a more positive direction.</p> In\u00a0[7]: Copied! <pre>a.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nf.data += 0.01 * f.grad\n\ne = a*b;\nd= e + c;\nL = d*f;\nL\n\nprint(L.data)\n</pre> a.data += 0.01 * a.grad b.data += 0.01 * b.grad c.data += 0.01 * c.grad f.data += 0.01 * f.grad  e = a*b; d= e + c; L = d*f; L  print(L.data) <pre>-6.723584000000001\n</pre> <p>Therefore the value of L was pushed to a more positive direction from -8.0 to -6.0</p>"},{"location":"ZeroToHero/Micrograd/notebooks/6_0_backpropagation_neuron/","title":"Backpropagation Neuron","text":"In\u00a0[1]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[\u00a0]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[\u00a0]: <pre>Value(data=-8.0)</pre> In\u00a0[2]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Implementing the Neuron Mathematical Mode</p> <p></p> In\u00a0[3]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.7, label='b')\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.7, label='b') In\u00a0[4]: Copied! <pre>x1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n</pre> x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2' In\u00a0[5]: Copied! <pre>#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n</pre> #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n' In\u00a0[6]: Copied! <pre>draw_dot(n)\n</pre> draw_dot(n) Out[6]: <p>Now, we have to get the output i.e. by having the dot product with the activation function.</p> <p>So we have to implement the tanh function</p> <p>Now, tanh is a hyperbolic expression. So it doesnt just contain +, -, it also has exponetials. So we have to create that function first in our Value object.</p> <p> </p> <p></p> <p>So now, lets update our Value object</p> In\u00a0[7]: Copied! <pre>import math\n</pre> import math In\u00a0[8]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')         return out In\u00a0[9]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.7, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.7, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2' In\u00a0[10]: Copied! <pre>#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\n\no = n.tanh(); o.label = 'o'\n</pre> #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function  o = n.tanh(); o.label = 'o' In\u00a0[11]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[11]: <p>We have recieved that output. So now, tanh is our little 'micrograd supported' node here, as an operation :)</p>"},{"location":"ZeroToHero/Micrograd/notebooks/6_1_backpropagation_neuron_manual_calculation/","title":"Backpropagation Neuron - Manual Calculation","text":"In\u00a0[\u00a0]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out In\u00a0[\u00a0]: Copied! <pre>a = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd= e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L'\nL\n</pre> a = Value(2.0, label='a') b = Value(-3.0, label='b') c = Value(10.0, label='c') e = a*b; e.label='e' d= e + c; d.label='d' f = Value(-2.0, label='f') L = d*f; L.label='L' L Out[\u00a0]: <pre>Value(data=-8.0)</pre> In\u00a0[8]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[\u00a0]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[\u00a0]: <p>Implementing the Neuron Mathematical Mode</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.7, label='b')\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.7, label='b') In\u00a0[\u00a0]: Copied! <pre>x1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n</pre> x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2' In\u00a0[\u00a0]: Copied! <pre>#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n</pre> #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n' In\u00a0[\u00a0]: Copied! <pre>draw_dot(n)\n</pre> draw_dot(n) Out[\u00a0]: <p>Now, we have to get the output i.e. by having the dot product with the activation function.</p> <p>So we have to implement the tanh function</p> <p>Now, tanh is a hyperbolic expression. So it doesnt just contain +, -, it also has exponetials. So we have to create that function first in our Value object.</p> <p> </p> <p></p> <p>So now, lets update our Value object</p> In\u00a0[4]: Copied! <pre>import math\n</pre> import math In\u00a0[1]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')         return out In\u00a0[2]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.7, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.7, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2' In\u00a0[\u00a0]: Copied! <pre>#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\n\no = n.tanh(); o.label = 'o'\n</pre> #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function  o = n.tanh(); o.label = 'o' In\u00a0[\u00a0]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[\u00a0]: <p>We have recieved that output. So now, tanh is our little 'micrograd supported' node here, as an operation :)</p> <p>We'll be doing the manual backpropagation calculation now</p> In\u00a0[30]: Copied! <pre>x1 = Value(0.0, label='x1')\nx2 = Value(2.0, label='x2')\n\nw1 = Value(1.0, label='w1')\nw2 = Value(-3.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')  #We've set this specific value for calculation purposes. Normally, if you increase this value, the final ouput will close to one.\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\nn = x1w1x2w2 + b; n.label = 'n'\no = n.tanh(); o.label = 'o'\n</pre> x1 = Value(0.0, label='x1') x2 = Value(2.0, label='x2')  w1 = Value(1.0, label='w1') w2 = Value(-3.0, label='w2')  b = Value(6.8813735870195432, label='b')  #We've set this specific value for calculation purposes. Normally, if you increase this value, the final ouput will close to one.  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  n = x1w1x2w2 + b; n.label = 'n' o = n.tanh(); o.label = 'o' <p>First we know the derivative of o wrt o will be 1, so</p> In\u00a0[31]: Copied! <pre>o.grad = 1.0\n</pre> o.grad = 1.0 In\u00a0[32]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[32]: <p>Now, for do/dn we need to find the derivative through tanh  we know, 0 = tanh(n)  so what will be do/dn</p> <p> </p> <p>We'll refer the derivatives of tanh (3rd equation)  </p> <p> </p> <p>We will use the first one in that, i.e. 1 - tan^2 x  So basically,  =&gt; o = tanh(n)  =&gt; do/dn = 1 - tanh(n) ** 2  =&gt; do/dn = 1 - o ** 2</p> <p> </p> <p>Now this is broken down to a simpler equation which our Value object can perform.</p> In\u00a0[33]: Copied! <pre>1 - (o.data**2)\n</pre> 1 - (o.data**2) Out[33]: <pre>0.4999999999999999</pre> <p>So, it is coming to 0.5 approx</p> In\u00a0[34]: Copied! <pre>n.grad = 0.5\n</pre> n.grad = 0.5 In\u00a0[35]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[35]: <p>Here, we know from previous example that, if it is an addition operation, then the derivative just comes as 1. So here the gradient value of n will itself channel out to its child nodes.</p> In\u00a0[36]: Copied! <pre>b.grad = 0.5\nx1w1x2w2.grad = 0.5\n</pre> b.grad = 0.5 x1w1x2w2.grad = 0.5 In\u00a0[37]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[37]: <p>Continuing, same we have the '+' operation again. So the grad value of x1w1x2w2 flows onto its child nodes</p> In\u00a0[38]: Copied! <pre>x1w1.grad = 0.5\nx2w2.grad = 0.5\n</pre> x1w1.grad = 0.5 x2w2.grad = 0.5 In\u00a0[39]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[39]: <p>Now, its the product '*' (We have already seen how it interchanges the values)  So x1 = w1 * x1w1.grad =&gt; 1.0 * 0.5 = 0.5  w1 = x1 * x1w1.grad =&gt; 0.0 * 0.5 = 0.0</p> <p> </p> <p>x2 = w2 * x2w2.grad =&gt; -3.0 * 0.5 = -1.5 w2 = x2 * x2w2.grad =&gt; 2.0 * 0.5 = 1.0</p> In\u00a0[40]: Copied! <pre>x1.grad = 0.5\nw1.grad = 0.0\nx2.grad = -1.5\nw2.grad = 1.0\n</pre> x1.grad = 0.5 w1.grad = 0.0 x2.grad = -1.5 w2.grad = 1.0 In\u00a0[41]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[41]: <p>P.S. I have edited this file from my local system, I have modified the above cells to the correct value (x1.grad), so in the above node x1, the grad value should display 0.5 not 1.0</p> <p>Now, here the weights play a huge rule in reducing the loss function value in the end. Therefore, he we can get to know by changing which 'w' we can affect the final output. In this case, w1 has no effect on this neuron's output, as it's gradient is 0 So, in this example, only 'w2' has an effect.  Therefore, my modifying the value of 'w2' and increasing the bias, we can squash the tanh function and get the final output to flat out to 1.</p>"},{"location":"ZeroToHero/Micrograd/notebooks/7_0_backward_func_each_operation/","title":"Backward Function - Each operation","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[\u00a0]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          return out In\u00a0[\u00a0]: Copied! <pre>#What the graph looks like right now, the grad values are set to 0\ndraw_dot(o)\n</pre> #What the graph looks like right now, the grad values are set to 0 draw_dot(o) Out[\u00a0]: <p>We'll be adding a 'backward' function to our Value object and then implement its functionality in each of the operation function.   We are basically converting everything we did manually to calculate the gradients in each operation to code :)</p> In\u00a0[29]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad = 1.0 * out.grad  #Remember we are doing chain rule here, hence the product with out.grad\n          other.grad = 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad = other.data * out.grad #Remember we are doing chain rule here, hence the product with out.grad\n          other.grad = self.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad = 1 - (t**2) * out.grad #Remember we are doing chain rule here, hence the product with out.grad\n\n        out._backward = backward\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad = 1.0 * out.grad  #Remember we are doing chain rule here, hence the product with out.grad           other.grad = 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad = other.data * out.grad #Remember we are doing chain rule here, hence the product with out.grad           other.grad = self.data * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad = 1 - (t**2) * out.grad #Remember we are doing chain rule here, hence the product with out.grad          out._backward = backward         return out In\u00a0[32]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\n#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\no = n.tanh(); o.label = 'o'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function o = n.tanh(); o.label = 'o' <p>Now, we call the '_backward' function that we have made one by one in order (backwards through the equation/graph)   But before we could do that, we have to first set the value of o.grad to 1.0  As notice in the Value object code that, we have initialised it to 0   Therefore, we'll start by adding o.grad to 1.0 and then we'll call the '_backward' function rest of them one by one</p> In\u00a0[33]: Copied! <pre>o.grad = 1.0\n</pre> o.grad = 1.0 In\u00a0[34]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[34]: <p>Now. we go with o</p> In\u00a0[35]: Copied! <pre>o._backward()\n</pre> o._backward() In\u00a0[36]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[36]: <p>Now, from n</p> In\u00a0[37]: Copied! <pre>n._backward()\n</pre> n._backward() In\u00a0[38]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[38]: <p>Now, b is a leaf node so we just leave it there (Another reason why _backward was initiallised to None function, it's because it won't be called for leaf nodes. Therefore we set it to None for them)   We'll continue with x1w1x1w2</p> In\u00a0[39]: Copied! <pre>x1w1x2w2._backward()\n</pre> x1w1x2w2._backward() In\u00a0[40]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[40]: <p>Now finally, to finding the gradient of the intial values</p> In\u00a0[41]: Copied! <pre>x1w1._backward()\nx2w2._backward()\n</pre> x1w1._backward() x2w2._backward() In\u00a0[42]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[42]: <p>Final output^</p> <p>Hence, we have not only verified the manual backpropagation calculation that we did, but also created funtions directly for each of them!</p>"},{"location":"ZeroToHero/Micrograd/notebooks/7_1_backward_func_entire_graph/","title":"Backward Function - Entire Graph","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad = 1.0 * out.grad  #Remember we are doing chain rule here, hence the product with out.grad\n          other.grad = 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad = other.data * out.grad #Remember we are doing chain rule here, hence the product with out.grad\n          other.grad = self.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad = 1 - (t**2) * out.grad #Remember we are doing chain rule here, hence the product with out.grad\n\n        out._backward = backward\n        return out\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):   # This basically allows us to print nicer looking expressions for the final output         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad = 1.0 * out.grad  #Remember we are doing chain rule here, hence the product with out.grad           other.grad = 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad = other.data * out.grad #Remember we are doing chain rule here, hence the product with out.grad           other.grad = self.data * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad = 1 - (t**2) * out.grad #Remember we are doing chain rule here, hence the product with out.grad          out._backward = backward         return out In\u00a0[14]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\n#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\no = n.tanh(); o.label = 'o'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function o = n.tanh(); o.label = 'o' In\u00a0[5]: Copied! <pre>o.grad = 1.0 #This is the base case, we set this\n</pre> o.grad = 1.0 #This is the base case, we set this In\u00a0[6]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[6]: <p>Instead of calling the '_ backward' function each time, we are creating it as a function in the Value object itself.</p> <p>Now, we also need to make sure that all the nodes have been accessed and forward pass through. So, we have used a concept called 'topological sort' where all the nodes are accessed/traversed in the same/one direction (either left-to-right or vice-versa) </p> <p>Therefore, we are adding the code for it, where it ensures that all the nodes are accessed at least once (and only stored once) and the node is only stored after/when all of it's child nodes are accessed and stored. This way we know we have traversed through the entire graph.</p> In\u00a0[7]: Copied! <pre>topo = []\nvisited = set() #We are maintaining a set of visited nodes\n\ndef build_topo(v):\n\n  if v not in visited:\n    visited.add(v)\n\n    for child in v._prev:\n      build_topo(child)\n\n    topo.append(v)  #Adds itself, only after all its children have been added\n\nbuild_topo(o) #We are starting from the root node, for us 'o'\ntopo\n</pre> topo = [] visited = set() #We are maintaining a set of visited nodes  def build_topo(v):    if v not in visited:     visited.add(v)      for child in v._prev:       build_topo(child)      topo.append(v)  #Adds itself, only after all its children have been added  build_topo(o) #We are starting from the root node, for us 'o' topo Out[7]: <pre>[Value(data=6.881373587019543),\n Value(data=1.0),\n Value(data=0.0),\n Value(data=0.0),\n Value(data=-3.0),\n Value(data=2.0),\n Value(data=-6.0),\n Value(data=-6.0),\n Value(data=0.8813735870195432),\n Value(data=0.7071067811865476)]</pre> <p>Once all the nodes have been topologically sorted, we then reverse the nodes order (Since we are traversing it from left to right i.e. input to output, we are reversing it, so that the gradients are calculated. As we have done previously in our examples) call the '_ backward' function to perform backpropagation from the output.</p> In\u00a0[8]: Copied! <pre>for node in reversed(topo):\n  node._backward()\n</pre> for node in reversed(topo):   node._backward() <p>And now, we will add all the calculated gradients to the graph!</p> In\u00a0[9]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[9]: <p>Now, we add this functionality to our value object</p> In\u00a0[\u00a0]: Copied! <pre>import math\n</pre> import math In\u00a0[16]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad = 1.0 * out.grad\n          other.grad = 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad = other.data * out.grad\n          other.grad = self.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad = 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad = 1.0 * out.grad           other.grad = 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad = other.data * out.grad           other.grad = self.data * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad = 1 - (t**2) * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() <p>This is the updated value object^</p> <p>Cross-verifying it once</p> In\u00a0[17]: Copied! <pre>#I re-run the cells containing the node values as well\ndraw_dot(o)\n</pre> #I re-run the cells containing the node values as well draw_dot(o) Out[17]: In\u00a0[18]: Copied! <pre>o.backward()\n</pre> o.backward() In\u00a0[19]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[19]:"},{"location":"ZeroToHero/Micrograd/notebooks/8_handling_onenode_used_multiple_times/","title":"Handing one node used multiple times","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad = 1.0 * out.grad\n          other.grad = 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad = other.data * out.grad\n          other.grad = self.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad = 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad = 1.0 * out.grad           other.grad = 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad = other.data * out.grad           other.grad = self.data * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad = 1 - (t**2) * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[13]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\n#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\no = n.tanh(); o.label = 'o'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function o = n.tanh(); o.label = 'o' In\u00a0[5]: Copied! <pre>o.grad = 1.0 #This is the base case, we set this\n</pre> o.grad = 1.0 #This is the base case, we set this In\u00a0[8]: Copied! <pre>o.backward()\n</pre> o.backward() In\u00a0[9]: Copied! <pre>draw_dot(o)\n</pre> draw_dot(o) Out[9]: <p>Now, here we have noticed a bug. Which we can see in these following examples</p> <p>Example 1</p> In\u00a0[10]: Copied! <pre>a = Value(3.0, label='a')\nb = a + a ; b.label = 'b'\nb.backward()\ndraw_dot(b)\n</pre> a = Value(3.0, label='a') b = a + a ; b.label = 'b' b.backward() draw_dot(b) Out[10]: <p>So here, the data value is correct, but the grad value is wrong.   The derivative of b wrt a, in this equation b = a + a, is 2. Because it's 1+1, therefore 2   In the graph the second arrow is right on top of the first one, so we only see one arrow.   So it seems like one value is stored and when another same one comes, the first one gets over-written, and therefore only one of those values is considered.</p> <p>Example 2</p> In\u00a0[11]: Copied! <pre>a = Value(-2.0, label='a')\nb = Value(3.0, label='b')\nd = a * b ; d.label = 'd'\ne = a + b ; e.label = 'e'\nf = d * e ; f.label = 'f'\n\nf.backward()\ndraw_dot(f)\n</pre> a = Value(-2.0, label='a') b = Value(3.0, label='b') d = a * b ; d.label = 'd' e = a + b ; e.label = 'e' f = d * e ; f.label = 'f'  f.backward() draw_dot(f) Out[11]: <p>Okay so here, the derivative of f wrt a, b have two more cases: through e and d.   So now through e:  a and b grads are -6 respectively   And through d:  a is 3 and b is -2   So if you notice, through e was calculated first and then overwirtten by the calculations through d.</p> <p>So, the solution is simple. We just need to ensure that the gradient values get accumelated if the same variable is used again.   (Mathematical explaination or proof will be to use the Multivariable case rule of the chain rule.)  So basically we'll add the new value with the prev added one</p> <p>So, let's update the Value object</p> In\u00a0[12]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad   #Adding it on\n          other.grad += 1.0 * out.grad  #Adding it on\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad  #Adding it on\n          other.grad += self.data * out.grad  #Adding it on\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad    #Adding it on\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad   #Adding it on           other.grad += 1.0 * out.grad  #Adding it on          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad  #Adding it on           other.grad += self.data * out.grad  #Adding it on          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad    #Adding it on          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() <p>Now lets check those examples again</p> In\u00a0[14]: Copied! <pre>a = Value(3.0, label='a')\nb = a + a ; b.label = 'b'\nb.backward()\ndraw_dot(b)\n</pre> a = Value(3.0, label='a') b = a + a ; b.label = 'b' b.backward() draw_dot(b) Out[14]: In\u00a0[15]: Copied! <pre>a = Value(-2.0, label='a')\nb = Value(3.0, label='b')\nd = a * b ; d.label = 'd'\ne = a + b ; e.label = 'e'\nf = d * e ; f.label = 'f'\n\nf.backward()\ndraw_dot(f)\n</pre> a = Value(-2.0, label='a') b = Value(3.0, label='b') d = a * b ; d.label = 'd' e = a + b ; e.label = 'e' f = d * e ; f.label = 'f'  f.backward() draw_dot(f) Out[15]: <p>And now we've got the correct gradient values!   So after the values have been deposited the first time, if it is called again, then it deposites it again on top it next.   Example 1:  1 + 1 = 2  Example 2:  a -&gt; -6 + 3 = -3  b -&gt; -6 + (- 2) = -8</p> <p>Hence, resolving the issue :)</p>"},{"location":"ZeroToHero/Micrograd/notebooks/9_expanding_tanh_into_more_operations/","title":"Expanding tanh into more operations","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad   #Adding it on\n          other.grad += 1.0 * out.grad  #Adding it on\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad  #Adding it on\n          other.grad += self.data * out.grad  #Adding it on\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad    #Adding it on\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad   #Adding it on           other.grad += 1.0 * out.grad  #Adding it on          out._backward = backward         return out      def __mul__(self, other):         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad  #Adding it on           other.grad += self.data * out.grad  #Adding it on          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad    #Adding it on          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[8]: Copied! <pre>#Inputs x1, x2 of the neuron\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n#Weights w1, w2 of the neuron - The synaptic values\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n#The bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\n#The summation\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\n#n is basically the cell body, but without the activation function\nn = x1w1x2w2 + b; n.label = 'n'\n\n#Now we pass n to the activation function\no = n.tanh(); o.label = 'o'\n</pre> #Inputs x1, x2 of the neuron x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  #Weights w1, w2 of the neuron - The synaptic values w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  #The bias of the neuron b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  #The summation x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  #n is basically the cell body, but without the activation function n = x1w1x2w2 + b; n.label = 'n'  #Now we pass n to the activation function o = n.tanh(); o.label = 'o' In\u00a0[9]: Copied! <pre>#o.grad = 1.0\no.backward()\n\ndraw_dot(o)\n</pre> #o.grad = 1.0 o.backward()  draw_dot(o) Out[9]: <p>Updating the Value object</p> In\u00a0[10]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[11]: Copied! <pre>x1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\nn = x1w1x2w2 + b; n.label = 'n'\n\n#o = n.tanh(); o.label = 'o'\n\n#Spliting up of the tanh function\n\ne = (2*n).exp()\no = (e - 1) / (e + 1)\no.label = 'o'\n</pre> x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  n = x1w1x2w2 + b; n.label = 'n'  #o = n.tanh(); o.label = 'o'  #Spliting up of the tanh function  e = (2*n).exp() o = (e - 1) / (e + 1) o.label = 'o' In\u00a0[13]: Copied! <pre>o.backward()\n\ndraw_dot(o)\n</pre> o.backward()  draw_dot(o) Out[13]: <p>Hence, we have now split the tanh function and the additional operations have also been added! Plus, we are also getting the same output as before: o and the inputs &amp; weights :)</p>"},{"location":"ZeroToHero/Micrograd/notebooks/x10_implementing_in_pytorch/","title":"Implementing in PyTorch","text":"In\u00a0[1]: Copied! <pre>import math\n</pre> import math In\u00a0[2]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[\u00a0]: Copied! <pre>x1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\nn = x1w1x2w2 + b; n.label = 'n'\n\n#o = n.tanh(); o.label = 'o'\n\n#Spliting up of the tanh function\n\ne = (2*n).exp()\no = (e - 1) / (e + 1)\no.label = 'o'\n</pre> x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  b = Value(6.8813735870195432, label='b')  x1w1 = x1*w1; x1w1.label = 'x1*w1' x2w2 = x2*w2; x2w2.label = 'x2*w2'  x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'  n = x1w1x2w2 + b; n.label = 'n'  #o = n.tanh(); o.label = 'o'  #Spliting up of the tanh function  e = (2*n).exp() o = (e - 1) / (e + 1) o.label = 'o' <p>Now we'll how the same thing can be written in PyTorch syntax</p> In\u00a0[3]: Copied! <pre>import torch\n</pre> import torch In\u00a0[4]: Copied! <pre>x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n</pre> x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True  n = x1*w1 + x2*w2 + b o = torch.tanh(n)  print(o.data.item()) o.backward()  print('x2', x2.grad.item()) print('w2', w2.grad.item()) print('x1', x1.grad.item()) print('w1', w1.grad.item()) <pre>0.7071066904050358\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n</pre> <p>Some PyTorch syntax review</p> In\u00a0[5]: Copied! <pre>torch.Tensor([[1,2,3], [4,5,6]])\n</pre> torch.Tensor([[1,2,3], [4,5,6]]) Out[5]: <pre>tensor([[1., 2., 3.],\n        [4., 5., 6.]])</pre> In\u00a0[6]: Copied! <pre>torch.Tensor([[1,2,3], [4,5,6]]).shape\n</pre> torch.Tensor([[1,2,3], [4,5,6]]).shape Out[6]: <pre>torch.Size([2, 3])</pre> In\u00a0[7]: Copied! <pre>torch.Tensor([2.0]).dtype\n</pre> torch.Tensor([2.0]).dtype Out[7]: <pre>torch.float32</pre> In\u00a0[8]: Copied! <pre>torch.Tensor([2.0]).double().dtype\n</pre> torch.Tensor([2.0]).double().dtype Out[8]: <pre>torch.float64</pre> In\u00a0[9]: Copied! <pre>x2.data\n</pre> x2.data Out[9]: <pre>tensor([0.], dtype=torch.float64)</pre> In\u00a0[10]: Copied! <pre>x2.data.item()\n</pre> x2.data.item() Out[10]: <pre>0.0</pre>"},{"location":"ZeroToHero/Micrograd/notebooks/x11_creating_a_multi_layer_perceptron/","title":"Creating a MLP","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[5]: Copied! <pre>import random\n</pre> import random In\u00a0[8]: Copied! <pre>class Neuron:\n\tdef __init__(self, nin):\n\t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n\t\tself.b = Value(random.uniform(-1,1))\n\n\tdef __call__(self, x):\n\t\t# (w*x)+b\n\t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n\t\tout = act.tanh()\n\t\treturn out\n\nclass Layer:\n\tdef __init__(self, nin, nout):\n\t\tself.neurons = [Neuron(nin) for _ in range(nout)]\n\n\tdef __call__(self, x):\n\t\touts = [n(x) for n in self.neurons]\n\t\treturn outs\n\nclass MLP:\n\tdef __init__(self, nin, nouts):\n\t\tsz = [nin] + nouts\n\t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]\n\n\tdef __call__(self, x):\n\t\tfor layer in self.layers:\n\t\t\tx = layer(x)\n\t\treturn x\n</pre> class Neuron: \tdef __init__(self, nin): \t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ] \t\tself.b = Value(random.uniform(-1,1))  \tdef __call__(self, x): \t\t# (w*x)+b \t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b ) \t\tout = act.tanh() \t\treturn out  class Layer: \tdef __init__(self, nin, nout): \t\tself.neurons = [Neuron(nin) for _ in range(nout)]  \tdef __call__(self, x): \t\touts = [n(x) for n in self.neurons] \t\treturn outs  class MLP: \tdef __init__(self, nin, nouts): \t\tsz = [nin] + nouts \t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]  \tdef __call__(self, x): \t\tfor layer in self.layers: \t\t\tx = layer(x) \t\treturn x In\u00a0[9]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[9]: <pre>[Value(data=0.6731685486278488)]</pre> <p>Now in the final output, we see it in the form of a list right.   So just for making this look better, we are adding this one line in the return statement of the layer method where if it is just one value left i.e. if it is the last value, then just return that value directly instead of putting it inside a list.</p> In\u00a0[10]: Copied! <pre>class Neuron:\n\tdef __init__(self, nin):\n\t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n\t\tself.b = Value(random.uniform(-1,1))\n\n\tdef __call__(self, x):\n\t\t# (w*x)+b\n\t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n\t\tout = act.tanh()\n\t\treturn out\n\nclass Layer:\n\tdef __init__(self, nin, nout):\n\t\tself.neurons = [Neuron(nin) for _ in range(nout)]\n\n\tdef __call__(self, x):\n\t\touts = [n(x) for n in self.neurons]\n\t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better\n\nclass MLP:\n\tdef __init__(self, nin, nouts):\n\t\tsz = [nin] + nouts\n\t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]\n\n\tdef __call__(self, x):\n\t\tfor layer in self.layers:\n\t\t\tx = layer(x)\n\t\treturn x\n</pre> class Neuron: \tdef __init__(self, nin): \t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ] \t\tself.b = Value(random.uniform(-1,1))  \tdef __call__(self, x): \t\t# (w*x)+b \t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b ) \t\tout = act.tanh() \t\treturn out  class Layer: \tdef __init__(self, nin, nout): \t\tself.neurons = [Neuron(nin) for _ in range(nout)]  \tdef __call__(self, x): \t\touts = [n(x) for n in self.neurons] \t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better  class MLP: \tdef __init__(self, nin, nouts): \t\tsz = [nin] + nouts \t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]  \tdef __call__(self, x): \t\tfor layer in self.layers: \t\t\tx = layer(x) \t\treturn x In\u00a0[11]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[11]: <pre>Value(data=-0.597277746687069)</pre> <p>Like that!  Now finally, lets make the graph of this</p> In\u00a0[12]: Copied! <pre>draw_dot(n(x))\n</pre> draw_dot(n(x)) Out[12]: <p>THAT'S AN ENTIRE MLP THAT WE'VE DEFINED NOW!</p>"},{"location":"ZeroToHero/Micrograd/notebooks/x12_creating_a_loss_function/","title":"Creating a Loss function","text":"<p>One small change as been done in our Value object. Please note that   The radd() functionality has been added.</p> In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[20]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __radd__(self, other): #here\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __radd__(self, other): #here         return self + other      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[4]: Copied! <pre>import random\n</pre> import random In\u00a0[21]: Copied! <pre>class Neuron:\n\tdef __init__(self, nin):\n\t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n\t\tself.b = Value(random.uniform(-1,1))\n\n\tdef __call__(self, x):\n\t\t# (w*x)+b\n\t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n\t\tout = act.tanh()\n\t\treturn out\n\nclass Layer:\n\tdef __init__(self, nin, nout):\n\t\tself.neurons = [Neuron(nin) for _ in range(nout)]\n\n\tdef __call__(self, x):\n\t\touts = [n(x) for n in self.neurons]\n\t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better\n\nclass MLP:\n\tdef __init__(self, nin, nouts):\n\t\tsz = [nin] + nouts\n\t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]\n\n\tdef __call__(self, x):\n\t\tfor layer in self.layers:\n\t\t\tx = layer(x)\n\t\treturn x\n</pre> class Neuron: \tdef __init__(self, nin): \t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ] \t\tself.b = Value(random.uniform(-1,1))  \tdef __call__(self, x): \t\t# (w*x)+b \t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b ) \t\tout = act.tanh() \t\treturn out  class Layer: \tdef __init__(self, nin, nout): \t\tself.neurons = [Neuron(nin) for _ in range(nout)]  \tdef __call__(self, x): \t\touts = [n(x) for n in self.neurons] \t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better  class MLP: \tdef __init__(self, nin, nouts): \t\tsz = [nin] + nouts \t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]  \tdef __call__(self, x): \t\tfor layer in self.layers: \t\t\tx = layer(x) \t\treturn x In\u00a0[23]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[23]: <pre>Value(data=-0.33393070997191954)</pre> <p>Now, we are creating a very simple dataset. Where we feed a list of values which we would like the NN to take as input.  Along with the list of desired output targets.  So whenever we give the values xs into the NN, we want the output values to be those in ys respectively. \\</p> <p> </p> <p>It's almost like a simple Binary Classification. It needs to be either 1.0 or -1.0 in our example.</p> In\u00a0[24]: Copied! <pre>xs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0] #desired targets\n</pre> xs = [     [2.0, 3.0, -1.0],     [3.0, -1.0, 0.5],     [0.5, 1.0, 1.0],     [1.0, 1.0, -1.0] ]  ys = [1.0, -1.0, -1.0, 1.0] #desired targets <p>Now, ys is what WE WANT.   Lets see what our NN thinks of those inputs</p> In\u00a0[25]: Copied! <pre>ypred = [n(x) for x in xs]\nypred\n</pre> ypred = [n(x) for x in xs] ypred Out[25]: <pre>[Value(data=-0.33393070997191954),\n Value(data=-0.7996605801165794),\n Value(data=-0.053910703703307694),\n Value(data=-0.5691658715750736)]</pre> <p>So we can see how the outputs we have received are different from what we need   first and fourth one needs to be increased slighly to reach our desired value.   And the second, third one needs to be decreased.</p> <p>So now how do train/tune the NN or how do tune the weights in order to get our desired output.</p> <p> </p> <p>The trick in NN is to calculate a single value that contains the sum of the total performance of your NN. And that will be the loss value.</p> <p>So this loss, will give us an intuition on how well the NN is performing.   Right now in our example, it is not performing very well, as the values are way off. Therefore the loss will be high and we need to look to minimize the loss.</p> <p>So in this particular situation/example, we are going to do/calculate the Mean Squared Error Loss.</p> <p>So first we take the 'y ground truth (ygt)' and then 'y output (yout)'. We will be pairing them together in the loop</p> In\u00a0[\u00a0]: Copied! <pre>for ygt, yout in zip(ys, ypred)\n</pre> for ygt, yout in zip(ys, ypred) <p>And then we will be subtracting each of those values and then squaring them</p> <p>Now that will give us the loss for each of those individual values</p> In\u00a0[11]: Copied! <pre>[(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]\n</pre> [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)] Out[11]: <pre>[Value(data=0.2551293266642757),\n Value(data=1.5689343597801393),\n Value(data=1.942823557477381),\n Value(data=0.2264329556998548)]</pre> <p>So depending on how off your pred value is from the expected one, the higher th value will be   That expression is written in such a way that, only when the yout and ygt are close to each other, it will become 0. Therefore no loss in that case   So the aim is to make the loss as small as possible</p> <p>Now, as mentioned before. The final loss will just be the sum of all those numbers.</p> In\u00a0[26]: Copied! <pre>loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n</pre> loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) loss Out[26]: <pre>Value(data=5.176873511248545)</pre> <p>Note: There was an error here where int could not be added to a Value. So I have made the modification in the Value object to handle that :)</p> In\u00a0[27]: Copied! <pre>loss.backward()\n</pre> loss.backward() <p>Something cool really happended when you ran that last cell</p> In\u00a0[28]: Copied! <pre>n.layers[0].neurons[0].w[0]\n</pre> n.layers[0].neurons[0].w[0] Out[28]: <pre>Value(data=-0.22585628440403194)</pre> In\u00a0[29]: Copied! <pre>n.layers[0].neurons[0].w[0].grad\n</pre> n.layers[0].neurons[0].w[0].grad Out[29]: <pre>3.080379043409595</pre> <p>So now we have extracted the value and its grad value for one particular neuron!</p> <p>Lets look at the graph of the loss. That will give us a bigger graph, as if you would notice in the mean squared error expression that we wrote, we are passing each of the neuron to it.</p> In\u00a0[30]: Copied! <pre>draw_dot(loss)\n</pre> draw_dot(loss) Out[30]: <p>PHEEWWWW THAT WAS AWESOME LMAO</p> <p>We have like 4 different foward pass and finally calculating the loss   Not only that, we have also backpropagated throughout the entire graph!</p>"},{"location":"ZeroToHero/Micrograd/notebooks/x13_collecting_all_parameters_in_NN/","title":"Collecting all parameters in Neural Net","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __radd__(self, other): #here\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __radd__(self, other): #here         return self + other      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[4]: Copied! <pre>import random\n</pre> import random In\u00a0[\u00a0]: Copied! <pre>class Neuron:\n\tdef __init__(self, nin):\n\t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n\t\tself.b = Value(random.uniform(-1,1))\n\n\tdef __call__(self, x):\n\t\t# (w*x)+b\n\t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n\t\tout = act.tanh()\n\t\treturn out\n\nclass Layer:\n\tdef __init__(self, nin, nout):\n\t\tself.neurons = [Neuron(nin) for _ in range(nout)]\n\n\tdef __call__(self, x):\n\t\touts = [n(x) for n in self.neurons]\n\t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better\n\nclass MLP:\n\tdef __init__(self, nin, nouts):\n\t\tsz = [nin] + nouts\n\t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]\n\n\tdef __call__(self, x):\n\t\tfor layer in self.layers:\n\t\t\tx = layer(x)\n\t\treturn x\n</pre> class Neuron: \tdef __init__(self, nin): \t\tself.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ] \t\tself.b = Value(random.uniform(-1,1))  \tdef __call__(self, x): \t\t# (w*x)+b \t\tact = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b ) \t\tout = act.tanh() \t\treturn out  class Layer: \tdef __init__(self, nin, nout): \t\tself.neurons = [Neuron(nin) for _ in range(nout)]  \tdef __call__(self, x): \t\touts = [n(x) for n in self.neurons] \t\treturn outs[0] if len(outs)==1 else outs  #The New added line for making the output better  class MLP: \tdef __init__(self, nin, nouts): \t\tsz = [nin] + nouts \t\tself.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]  \tdef __call__(self, x): \t\tfor layer in self.layers: \t\t\tx = layer(x) \t\treturn x In\u00a0[\u00a0]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[\u00a0]: <pre>Value(data=-0.33393070997191954)</pre> <p>Now, we'll be returning the parameters from the MLP. So that will be from Neuron -&gt; Layer -&gt; MLP</p> In\u00a0[5]: Copied! <pre>class Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b]\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n        # Alternative way of writing the above return function:\n        # parameters = []\n        # for n in self.neurons:\n        #   p = n.parameters()\n        #   parameters.extend(p)\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for l in self.layers for p in l.parameters()]\n</pre> class Neuron:     def __init__(self, nin):         self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]         self.b = Value(random.uniform(-1, 1))      def __call__(self, x):         act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)         out = act.tanh()         return out      def parameters(self):         return self.w + [self.b]  class Layer:     def __init__(self, nin, nout):         self.neurons = [Neuron(nin) for _ in range(nout)]      def __call__(self, x):         outs = [n(x) for n in self.neurons]         return outs[0] if len(outs) == 1 else outs      def parameters(self):         return [p for n in self.neurons for p in n.parameters()]          # Alternative way of writing the above return function:         # parameters = []         # for n in self.neurons:         #   p = n.parameters()         #   parameters.extend(p)  class MLP:     def __init__(self, nin, nouts):         sz = [nin] + nouts         self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]      def __call__(self, x):         for layer in self.layers:             x = layer(x)         return x      def parameters(self):         return [p for l in self.layers for p in l.parameters()] In\u00a0[6]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[6]: <pre>Value(data=0.7625252102576119)</pre> In\u00a0[7]: Copied! <pre>n.parameters()\n</pre> n.parameters() Out[7]: <pre>[Value(data=0.31785584973173164),\n Value(data=0.2998372553774835),\n Value(data=-0.8029008199517247),\n Value(data=-0.39340060142531286),\n Value(data=0.23322412084873956),\n Value(data=0.29891813550514534),\n Value(data=-0.5314862907700675),\n Value(data=0.19661072911432642),\n Value(data=0.9142418954398666),\n Value(data=0.041208786424172805),\n Value(data=-0.23983634992214187),\n Value(data=-0.593538786941121),\n Value(data=0.39482399486723296),\n Value(data=-0.9880306400643504),\n Value(data=-0.8097855189886964),\n Value(data=0.4629484174790124),\n Value(data=0.31168805444961634),\n Value(data=-0.9828138115624934),\n Value(data=0.5221437252554255),\n Value(data=-0.19703997468926882),\n Value(data=-0.5504279057638468),\n Value(data=-0.8365261779265616),\n Value(data=-0.22783861276612227),\n Value(data=0.5666981389300718),\n Value(data=-0.06415010714317604),\n Value(data=0.845414529622897),\n Value(data=0.4793425135418725),\n Value(data=-0.38321354069020086),\n Value(data=-0.10963021731006206),\n Value(data=0.14485994942129898),\n Value(data=-0.19028270981146433),\n Value(data=0.5148204886483112),\n Value(data=-0.8559156650791364),\n Value(data=0.3778416962066449),\n Value(data=0.09608787032156774),\n Value(data=-0.8288362456839788),\n Value(data=0.5641592956285757),\n Value(data=0.13764114112689052),\n Value(data=-0.19625087652731277),\n Value(data=-0.6117936229921406),\n Value(data=0.7546009612155813)]</pre> <p>So these are all our parameters provided as inputs. The weights, inputs and biases</p> In\u00a0[8]: Copied! <pre>len(n.parameters())\n</pre> len(n.parameters()) Out[8]: <pre>41</pre>"},{"location":"ZeroToHero/Micrograd/notebooks/x14_manual_gradient_descent_optimization/","title":"Manual Gradient Descent Optimization","text":"In\u00a0[1]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    #Builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        #For any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')\n        if n._op:\n            #If this value is a result of some operation, then create an op node for it\n            dot.node(name = uid + n._op, label=n._op)\n            #and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        #Connect n1 to the node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     #Builds a set of all nodes and edges in a graph     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR == Left to Right      nodes, edges = trace(root)     for n in nodes:         uid = str(id(n))         #For any value in the graph, create a rectangular ('record') node for it         dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % ( n.label, n.data, n.grad), shape='record')         if n._op:             #If this value is a result of some operation, then create an op node for it             dot.node(name = uid + n._op, label=n._op)             #and connect this node to it             dot.edge(uid + n._op, uid)      for n1, n2 in edges:         #Connect n1 to the node of n2         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre>import math\n</pre> import math In\u00a0[3]: Copied! <pre>class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def backward():\n          self.grad += 1.0 * out.grad\n          other.grad += 1.0 * out.grad\n\n        out._backward = backward\n        return out\n\n    def __radd__(self, other): #here\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def backward():\n          self.grad += other.data * out.grad\n          other.grad += self.data * out.grad\n        out._backward = backward\n        return out\n\n    def __rmul__(self, other):   #other * self\n        return self * other\n\n    def __truediv__(self, other):  #self/other\n        return self * other**-1\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):  #self - other\n        return self + (-other)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f\"**{other}\")\n\n        def backward():\n          self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n        out._backward = backward\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def backward():\n          self.grad += 1 - (t**2) * out.grad\n\n        out._backward = backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n        def backward():\n            self.grad += out.data * out.grad\n\n        out._backward = backward\n        return out\n\n    def backward(self):\n\n      topo = []\n      visited = set()\n      def build_topo(v):\n        if v not in visited:\n          visited.add(v)\n          for child in v._prev:\n            build_topo(child)\n          topo.append(v)\n\n      build_topo(self)\n\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):         self.data = data         self.grad = 0.0         self._backward = lambda: None #Its an empty function by default. This is what will do that gradient calculation at each of the operations.         self._prev = set(_children)         self._op = _op         self.label = label       def __repr__(self):         return f\"Value(data={self.data})\"      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def backward():           self.grad += 1.0 * out.grad           other.grad += 1.0 * out.grad          out._backward = backward         return out      def __radd__(self, other): #here         return self + other      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def backward():           self.grad += other.data * out.grad           other.grad += self.data * out.grad         out._backward = backward         return out      def __rmul__(self, other):   #other * self         return self * other      def __truediv__(self, other):  #self/other         return self * other**-1      def __neg__(self):         return self * -1      def __sub__(self, other):  #self - other         return self + (-other)      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data ** other, (self, ), f\"**{other}\")          def backward():           self.grad += (other * (self.data ** (other - 1))) * out.grad          out._backward = backward         return out      def tanh(self):         x = self.data         t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)         out = Value(t, (self, ), 'tanh')          def backward():           self.grad += 1 - (t**2) * out.grad          out._backward = backward         return out      def exp(self):         x = self.data         out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out          def backward():             self.grad += out.data * out.grad          out._backward = backward         return out      def backward(self):        topo = []       visited = set()       def build_topo(v):         if v not in visited:           visited.add(v)           for child in v._prev:             build_topo(child)           topo.append(v)        build_topo(self)        self.grad = 1.0       for node in reversed(topo):         node._backward() In\u00a0[4]: Copied! <pre>import random\n</pre> import random In\u00a0[5]: Copied! <pre>class Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b]\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n        # Alternative way of writing the above return function:\n        # parameters = []\n        # for n in self.neurons:\n        #   p = n.parameters()\n        #   parameters.extend(p)\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for l in self.layers for p in l.parameters()]\n</pre> class Neuron:     def __init__(self, nin):         self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]         self.b = Value(random.uniform(-1, 1))      def __call__(self, x):         act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)         out = act.tanh()         return out      def parameters(self):         return self.w + [self.b]  class Layer:     def __init__(self, nin, nout):         self.neurons = [Neuron(nin) for _ in range(nout)]      def __call__(self, x):         outs = [n(x) for n in self.neurons]         return outs[0] if len(outs) == 1 else outs      def parameters(self):         return [p for n in self.neurons for p in n.parameters()]          # Alternative way of writing the above return function:         # parameters = []         # for n in self.neurons:         #   p = n.parameters()         #   parameters.extend(p)  class MLP:     def __init__(self, nin, nouts):         sz = [nin] + nouts         self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]      def __call__(self, x):         for layer in self.layers:             x = layer(x)         return x      def parameters(self):         return [p for l in self.layers for p in l.parameters()] <p>Now we are trying to slighly nudge the value in order to reduce the loss</p> <p>So this essentially adds as an update function</p> In\u00a0[12]: Copied! <pre>for p in n.parameters():\n  p.data += -0.01 * p.grad #The negative sign is to convert any negative value to positive. Therefore increasing the value of the data, therefore decresing the loss\n</pre> for p in n.parameters():   p.data += -0.01 * p.grad #The negative sign is to convert any negative value to positive. Therefore increasing the value of the data, therefore decresing the loss <p>Now we follow three steps: Forward pass -&gt; Backward pass -&gt; Update</p> In\u00a0[35]: Copied! <pre>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[35]: <pre>Value(data=0.33215137965743546)</pre> In\u00a0[36]: Copied! <pre>xs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0] #output we want\n</pre> xs = [     [2.0, 3.0, -1.0],     [3.0, -1.0, 0.5],     [0.5, 1.0, 1.0],     [1.0, 1.0, -1.0] ]  ys = [1.0, -1.0, -1.0, 1.0] #output we want In\u00a0[54]: Copied! <pre>#forward pass\nypred = [n(x) for x in xs]\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n</pre> #forward pass ypred = [n(x) for x in xs] loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) loss Out[54]: <pre>Value(data=5.767047506521353)</pre> In\u00a0[55]: Copied! <pre>#backward pass\nloss.backward()\n</pre> #backward pass loss.backward() In\u00a0[56]: Copied! <pre>#update\nfor p in n.parameters():\n  p.data += -0.01 * p.grad\n</pre> #update for p in n.parameters():   p.data += -0.01 * p.grad In\u00a0[57]: Copied! <pre>#check the prediction\nypred\n</pre> #check the prediction ypred Out[57]: <pre>[Value(data=-0.25151630590655727),\n Value(data=0.42164884655021817),\n Value(data=-0.09631033350969018),\n Value(data=-0.16748189979649136)]</pre> <p>Putting the entire process together in a single function</p> In\u00a0[58]: Copied! <pre>#Initialize the neural net\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</pre> #Initialize the neural net x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) Out[58]: <pre>Value(data=0.9135198339971514)</pre> In\u00a0[59]: Copied! <pre>#Data definition\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0] #output we want\n</pre> #Data definition xs = [     [2.0, 3.0, -1.0],     [3.0, -1.0, 0.5],     [0.5, 1.0, 1.0],     [1.0, 1.0, -1.0] ]  ys = [1.0, -1.0, -1.0, 1.0] #output we want In\u00a0[92]: Copied! <pre>for k in range(10):\n\n  #forward pass\n  ypred = [n(x) for x in xs]\n  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n\n  #backward pass\n  for p in n.parameters():\n    p.grad = 0.0 #This is because after one round of update, we need to reset the value of the grads so that it can calculate and store the grad value of the updated loss function (i.e. The loss value that was improved after gradient descent). If we don't do this, the previous value of grad gets increamented with the new value during each back propagation (each time backward is called)\n  loss.backward()\n\n  #update\n  #THIS HERE, WHAT WE ARE DOING IS 'GRADIENT DESCENT'. WE ARE NUDGING THE INPUT VALUES BY A LITTLE BIT\n  for p in n.parameters():\n    p.data += -0.04 * p.grad\n\n  print(k, loss.data) #Printing the current number/iteration number plus how much loss\n</pre>  for k in range(10):    #forward pass   ypred = [n(x) for x in xs]   loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))    #backward pass   for p in n.parameters():     p.grad = 0.0 #This is because after one round of update, we need to reset the value of the grads so that it can calculate and store the grad value of the updated loss function (i.e. The loss value that was improved after gradient descent). If we don't do this, the previous value of grad gets increamented with the new value during each back propagation (each time backward is called)   loss.backward()    #update   #THIS HERE, WHAT WE ARE DOING IS 'GRADIENT DESCENT'. WE ARE NUDGING THE INPUT VALUES BY A LITTLE BIT   for p in n.parameters():     p.data += -0.04 * p.grad    print(k, loss.data) #Printing the current number/iteration number plus how much loss <pre>0 7.6021312440956095\n1 8.0\n2 6.398187062451399\n3 7.999999999997639\n4 8.0\n5 7.999964084143684\n6 8.0\n7 8.0\n8 7.999999961266539\n9 8.0\n</pre> In\u00a0[93]: Copied! <pre>ypred\n</pre> ypred Out[93]: <pre>[Value(data=-1.0), Value(data=-1.0), Value(data=-1.0), Value(data=-1.0)]</pre> In\u00a0[94]: Copied! <pre>loss\n</pre> loss Out[94]: <pre>Value(data=8.0)</pre> <p>If the loss was reduced, then you can <code>n.parameters</code> to see what were the values into the NN that caused to get the desired target outputs</p> <p>Okay so the predicted output didn't exactly come as expected \ud83e\udd72 (The first and last value weren't supposed to be negative lol)   But that was the idea of how we train a neural net!</p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/","title":"MASTER NOTES - NEURAL NETWORKS AND BACKPROPAGATION","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#introduction","title":"Introduction","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-000000","title":"Timestamp: 00:00:00","text":"<p>What training a Neural Network looks like under the hood. By the end of this, we will be able to define and train a Neural Net, to see how it works in an intuitive level.</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#micrograd-overview","title":"Micrograd overview","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-000025","title":"Timestamp: 00:00:25","text":"<p>Micrograd - is basically a tiny Autograd engine.</p> <p>Autograd engine - short for Automatic Gradient, it implements backpropagation.</p> <p>Backpropagation - is an algorithm which allows you to efficiently evaluate a gradient of some kind of a loss function with respect to the weights of a neural network. What that allows us to do is that we can iteratively tune the weights of that neural network to -&gt; minimize the loss function, and therefore -&gt; improve the accuracy of the neural network.</p> <p>So, Backpropagation would be at the Mathematical core of any modern deep neural network library like PyTorch or Jax.</p> <p>Functionality of Micrograd explained - micrograd-functionality</p> <p>So ultimately, Micrograd is all you need to train a Neural Network. Everything else is just for efficiency.</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#derivative-of-a-simple-function-with-one-input","title":"Derivative of a simple function with one input","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-000808","title":"Timestamp: 00:08:08","text":"<p>A Simple working example was done to explain the working/calculation of a Derivative of a function. Added to Jupyter notebook</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#derivative-of-a-function-with-multiple-inputs","title":"Derivative of a function with multiple inputs","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-001412","title":"Timestamp: 00:14:12","text":"<p>Added three more scalar inputs a, b, c. To the function d = a*b + c</p> <p>The derivative of the final function d was seen wrt to each of them, and the behavior was observed.</p> <p>Added to Jupyter notebook</p> <p>(Although I understood how the math of how this worked, I'm still not fully aware of how this explains the \"behavior of the derivative\" as he mentions in the end \ud83e\udd14)</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#starting-the-core-value-object-of-micrograd-and-its-visualization","title":"Starting the core Value object of micrograd and its visualization","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-001909","title":"Timestamp: 00:19:09","text":"<p>Now, Neural Networks are these massive mathematical expressions. So, we will be needing some data structures to maintain these expressions. Which is what we will be building.</p> <ul> <li>Initial explanation from value-object-creation</li> <li>From 22:45 to 24:54 - Visualization: Explained in value-object-creation (Visualization of the expression)</li> <li>From 24:55 to 29:01 - Generating the visual graphs in value-object-creation (Visualization of the expression continued)</li> <li>value-object-creation (SUMMARY &amp; WHAT TO DO NEXT)</li> </ul> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#manual-backpropagation-example-1-simple-expression","title":"Manual backpropagation example #1: simple expression","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-003210","title":"Timestamp: 00:32:10","text":"<p>A grad variable has been declared in the Value object. Which will contain the derivative of L w.r.t each of those leaf nodes. </p> <p>Manual Backpropagation for L, f and g in the expression: Notebook We have basically done 'gradient checks' here. Gradient check is essentially where we are deriving the backpropagation of an expression, by checking all of it's intermediate nodes.</p> <p>(VERY IMPORTANT PART) From 38:06 - This step will be the crux of backpropagation. This will be THE MOST IMPORTANT NODE TO UNDERSTAND. If we understand the gradient of this node, then we understand all of backpropagation and all of training of NN!! -&gt; crux-node-backpropagation</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#preview-of-a-single-optimization-step","title":"Preview of a single optimization step","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-005110","title":"Timestamp: 00:51:10","text":"<p>Here, we are just trying to nudge the inputs to make our L value go up.</p> <p>Now, we modify the weights of the leaf nodes (because that is what we usually have control over) slightly towards more positive direction and see how that affects L (in a more positive direction). </p> <p>So if we want L to increase, we should nudge the nodes slightly towards the gradient (eg, a should increase in the direction of the gradient, in step size) Notebook</p> <p>This is basically one step of the optimization that we ended up running. And really these gradient values calculated, give us some power, because we know how to influence the final outcome. And this will be extremely useful in training neural nets (which we will see soon :) )</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#manual-backpropagation-example-2-a-neuron","title":"Manual backpropagation example #2: a neuron","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-005252","title":"Timestamp: 00:52:52","text":"<p>Here, we are going to backpropagate through a neuron.</p> <p>So now, we want to eventually build out neural networks. In the simpler stage, these are multi-level perceptrons. For example: We have a two layer neural net, which contains two hidden (inner) layers which is made up of neurons which are interconnected to each other.</p> <p>Now, biologically neurons are obviously complicated devices, but there are simple mathematical models of them. neurons-explaination Notebook</p> <p>From 1:00:38, we'll see the tanh function in action and then the implementation of backpropagation (Manual backpropagation method)</p> <p>If you want to influence the final output, then you should increase the bias. Only then the tanh will squash the final output and flat out to the value 1 (As seen in the graph. Notebook</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#implementing-the-backward-function-for-each-operation","title":"Implementing the backward function for each operation","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-010902","title":"Timestamp: 01:09:02","text":"<p>We will be creating functions which would calculate the backpropagation i.e. the gradient values by itself! As the name of the chapter suggests, we'll be implementing it in each of the operations, like for '+', ' * ', 'tanh'</p> <p>Notebook</p> <p>Note on the '_ backward' function created:</p> <ul> <li>In the operation functions, we had created 'out' values which are an addition to/combination of the 'self' and 'other' values.</li> <li>Therefore we set the 'out._ backward' to be the function that backpropagates the gradient.</li> <li>Therefore we define what should happen when that particular operation function (Eg, add, mul) is called, inside the 'def backward()'</li> </ul> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#implementing-the-backward-function-for-a-whole-expression-graph","title":"Implementing the backward function for a whole expression graph","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-011732","title":"Timestamp: 01:17:32","text":"<p>Instead of calling the '_ backward' function each time, we are creating it as a function in the Value object itself.    </p> <p>Now, we also need to make sure that all the nodes have been accessed and forward pass through. So, we have used a concept called 'topological sort' where all the nodes are accessed/traversed in the same/one direction (either left-to-right or vice-versa) See the image here </p> <p>Therefore, we are adding the code for it, where it ensures that all the nodes are accessed at least once (and only stored once) and the node is only stored after/when all of it's child nodes are accessed and stored. This way we know we have traversed through the entire graph.    </p> <p>Once all the nodes have been topologically sorted, we then reverse the nodes order (Since we are traversing it from left to right i.e. input to output, we are reversing it, so that the gradients are calculated. As we have done previously in our examples) call the '_ backward' function to perform backpropagation from the output. Notebook </p> <p>And that was it, that was backpropagation! (Atleast for one neuron :P)</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#fixing-a-backprop-bug-when-one-node-is-used-multiple-times","title":"Fixing a backprop bug when one node is used multiple times","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-012228","title":"Timestamp: 01:22:28","text":"<p>Resolving a bug, where if there are multiple same nodes, then the calculation of the gradient isn't happening correctly as it considers both those separate nodes as a single node.  Notebook </p> <p>Solution was to append the gradient values.</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#breaking-up-a-tanh-exercising-with-more-operations","title":"Breaking up a tanh, exercising with more operations","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-012705","title":"Timestamp: 01:27:05","text":"<p>So far we had directly made a function for tanh, because we knew what it's derivative be.   </p> <p>So now we'll be trying to expand it into its other derivative form which contains exponents.    </p> <p>Therefore we are able to perform more operations such as exponents, division and subtraction (Therefore making it a good exercise)  </p> <p>Entire detailed explanation in: expanding-tanh-and-adding-more-operations</p> <p>Apart from showing that we can do different operations. We also want to show that the level up to which we want to implement the operations is up to us.    </p> <p>To explain, in our example- It can directly be 'tanh' or break it down into the expressions of exp, divide and subtract.    </p> <p>As long as the know the backward pass of that operation, it can be implemented in anyway.</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#doing-the-same-thing-but-in-pytorch-comparison","title":"Doing the same thing but in PyTorch: comparison","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-013931","title":"Timestamp: 01:39:31","text":"<p>Now we are going to see how we can convert our code into PyTorch (syntax?). Normally PyTorch is used during production. Comparison and Explanation: pytorch-comparision</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd","title":"Building out a neural net library (multi-layer perceptron) in micrograd","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-014355","title":"Timestamp: 01:43:55","text":"<p>We'll be building everything we have learnt till now, into a proper Neural Network type in code, using PyTorch. Everything will be broken down properly in - multi-layer-perceptron So, forward pass implementation has been done. Next we will also implement the backpropagation part.</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#creating-a-tiny-dataset-writing-the-loss-function","title":"Creating a tiny dataset, writing the loss function","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-015104","title":"Timestamp: 01:51:04","text":"<p>We made like a list of input values and then a list of values we want as an output. Saw how the pred values turn out. Calculated the loss function for individual as well as entire, to see how it is affecting.</p> <p>Also note: I had added <code>__radd__()</code> to the Value object to handle the case of int adding with a Value. Detailed step by step process is in the notebook</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#collecting-all-of-the-parameters-of-the-neural-net","title":"Collecting all of the parameters of the neural net","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-015756","title":"Timestamp: 01:57:56","text":"<p>Here we are going to write some convenience code, so that we can gather all the parameters of the neural net and work on all of them simultaneously.  We'll be nudging them by a small amount based on the gradient differentiation.</p> <p>That's where we are adding parameters functions. Now another reason why we are doing this, is that even the n function (Neuron function) in PyTorch also provides us with parameters which we can use. Therefore, we are declaring one for our MLP too. So there is the parameters of tensors and for us it's parameters of scalars. Notebook</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#doing-gradient-descent-optimization-manually-training-the-network","title":"Doing gradient descent optimization manually, training the network","text":""},{"location":"ZeroToHero/Micrograd/notes/A-main-video-lecture-notes/#timestamp-020112","title":"Timestamp: 02:01:12","text":"<p>We are basically performing 'Gradient Descent' here, by slightly nudging the values of the inputs to see how it can help to reduce the loss function. Notebook </p> <p>(I didn't exactly get the predicted output as excepted, but hey this is just the beginning. We'll see where we get from here :) )</p> <p> </p> <p>And that was the end of the first lecture! See you in the next one ;)</p> <p> </p>"},{"location":"ZeroToHero/Micrograd/notes/crux-node-backpropagation/","title":"Crux Node - Backpropagation","text":"<p>Keeping that simple expression graph in mind, we will now perform backpropagation to the c &amp; e nodes and then to the a &amp; b. </p> <p>d &amp; f have already been calculated as they are the direct child nodes of L.</p> <p>Now, we cannot find the derivative of L wrt c &amp; derivative of L wrt e directly. Since there is another node 'd' that is in the middle of them.</p> <p>Therefore, we use the chain rule - </p> <p>dz / dx = dz/dy . dy/dx</p> <p>Can also check the link here  Explanation example: \"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 \u00d7 4 = 8 times as fast as the man.\"</p> <p>So, keeping that in mind, lets go and calculate the derivative of L wrt nodes c, e, a, b.</p>"},{"location":"ZeroToHero/Micrograd/notes/crux-node-backpropagation/#starting-with-c-e","title":"Starting with c &amp; e","text":"<p>dL/dd had already been calculated (Check end of 4_1-manual-backpropagation notebook)</p> <p>d = c + e</p> <p>now,  Derivative of d wrt c, will be 1 Derivative of d wrt e, will be 1</p> <p>Because the derivative of '+' operation variables will lead to 1 (Calculus basics, it leads to constant, so 1)</p> <p>If we try to prove this mathematically:     d = c + e     f(x+h) - f(x) / h     Now, we'll calculate wrt c     =&gt; ( ((c+h)+e) - (c+e) ) / h     =&gt; c + h + e - c - e / h     =&gt; h / h     =&gt; 1     Therefore, dd/dc = 1</p> <p>Therefore, we can just substitute the value respectively.</p> <p>For node c:     dL/dc = dL/dd . dd/dc</p> <p>For node e:     dL/de = dL/dd . dd/de</p>"},{"location":"ZeroToHero/Micrograd/notes/crux-node-backpropagation/#continuing-with-a-b","title":"Continuing with a &amp; b","text":"<p>Same principle as above, but a different kind of equation here.</p> <p>Also remember here, derivative of L wrt e was just calculated above^ (dL/de)</p> <p>e = a * b</p> <p>Therefore,  Derivative of e wrt a, will be b Derivative of e wrt b, will be a</p> <p>Because the derivative of the same variable at the denominator gets out, so the other variable in the product remains (Calculus derivative theory itself)     d/da(a * b) = b</p> <p>If we try to prove this mathematically,     e = a * b     f(x+h) - f(x) / h     Remember, f(x) is equation here. So, finding wrt a, substituting the values     =&gt; ( ((a + h) * b) - (a * b) ) / h     =&gt; ab + hb - ab / h     =&gt; hb / h     =&gt; b     Therefore, de/da = b</p> <p>Therefore, we can just substitute the value respectively.</p> <p>For node a:     dL/da = dL/de . dd/da</p> <p>For node b:     dL/db = dL/de . dd/db</p> <p>And that was it! So we basically iterated through each node one by one, and locally applied the chain rule on each of the operations. Therefore we see backwards from L, as to how that output was produced.</p> <p>And THAT IS WHAT BACKPROPAGATION IS - It is just a recursive application of chain rule, backwards through the computational graph. :)</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/","title":"Expanding tanh and adding more operations","text":""},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#addressing-some-conditions","title":"Addressing some conditions:","text":""},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#condition-1-","title":"Condition 1-","text":"<p>First, we cannot multiply a number/constant directly with a Value variable, as in our __ add __ () functions, it takes the data of the object, i.e. self.data or other.data</p> <p>Therefore, when we do: <pre><code>a = Value(2.0)\nb = a + 2\n</code></pre></p> <p>This won't work as Value object cannot be added to a constant or this integer variable (Similarly for a * 2)</p> <p>Therefore, we add this condition: <pre><code>other = other if isinstance(other, Value) else Value(other)\n</code></pre> Note: we add this for both add and mul functions</p> <p>So here, we first check if 'other' (Which is the second parameter passed in the arguments of for eg: def __ add __ (self, other): ), we check if it is a Value object or not, else we turn it into a Value by wrapping the object around it.</p>"},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#condition-2-","title":"Condition 2-","text":"<p>Now in the above condition, we multiplied a Value variable with a constant.  If we do it the other way round, i.e. 2 * a, it won't work, as python thinks it is the same thing. </p> <p>So, what we do is that we add this kind of a 'fallback function', where if there is a condition where 2 can't be multiplied by a, then python checks if a can be multiplied by 2, i.e. in reverse.</p> <p>Therefore, <pre><code>def __rmul__(self, other):   #other * self\n    return self * other\n</code></pre></p>"},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#expanding-tanh","title":"Expanding Tanh:","text":""},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#first-we-start-with-the-exponential-part-","title":"First we start with the exponential part-","text":"<p>So, we take what we wrote for tanh and first start with writing the function just for the exponential part</p> <pre><code>def exp(self):\n\n\u00a0 \u00a0 \u00a0 \u00a0 x = self.data\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(math.exp(x), (self, ), 'exp')   #We merged t and out, into just out\n\n\u00a0 \u00a0 \u00a0 \u00a0 def backward():\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.grad += out.data * out.grad\n\n\u00a0 \u00a0 \u00a0 \u00a0 out._backward = backward\n\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n</code></pre> <p>Now here, the derivative of e^x is ex. Which is what we are getting in out, therefore we add the 'out.data' value there itself.</p>"},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#division-part","title":"Division part:","text":"<p>Now, instead of doing just for division operation, we are taking this kind of a special case which could act as a common ground.</p> <p>To explain: <pre><code>a / b    #This is the normal a divided by b\na * (1/b)    #Which can also be written this\na * (b**-1)    #Which again can be written like this\n\n#So we are trying the achive the second half of the third alternative, i.e.\n b ** -1\n#so we are trying to get an equation for \n b ** k #Where k is a constant\n</code></pre></p> <p>So now we can even do calculations where equations have 'to the power of' values.</p> <p>And if it is just divide, when we change the k value to -1 (We will have a kind of a fallback function for this as well, called __ truediv __() )</p> <p>Therefore writing the division function as (power - pow): <pre><code>def __pow__(self, other):\n\n    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n    out = Value(self.data ** other, (self, ), f\"**{other})\n\n\n    def backward():\n        self.grad += (other * (self.data ** (other - 1))) * out.grad\n\n    out._backward = backward\n    return out\n</code></pre></p> <p>Here we are using the power rule in derivatives:</p> <p>d/dx (x^n) = nx^(n-1)</p> <p>Lastly, we add this function for divide: <pre><code>def __truediv__(self, other):  #self/other\n    return self * other**-1\n</code></pre></p>"},{"location":"ZeroToHero/Micrograd/notes/expanding-tanh-and-adding-more-operations/#finally-subtraction-part","title":"Finally, Subtraction part:","text":"<p>First we add our kind of fallback functions to handle subtraction, i.e. the second value will have a negative number <pre><code>def __neg__(self):\n    return self * -1\n\ndef __sub__(self, other):  #self - other\n    return self + (-other)\n</code></pre></p> <p>So, when subtraction is called, we add self with the negation of other. To calculate or find what the negation of a number is, we call __ neg __ ()</p> <p>Therefore, no backward function here, as we are ultimately performing the addition operation itself.</p> <p>Now that all of this is done, we update our Value object.</p> <p>Then we change the way we want 'o' in our examples. Therefore we will convert the 'tanh' into it's various expression (One of its derivative expression in fact =&gt; (e^2x -1) / (e^2x +1) ) Notebook</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/micrograd-functionality/","title":"Micrograd Functionality","text":"<p>Example <pre><code>from micrograd.engine import Value\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\ng.backward()\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n</code></pre></p> <p>Here what we are doing is we have an expression that we are building, so we have two inputs 'a' and 'b'.</p> <p>They are -4 and 2. </p> <p>They are wrapped in this 'Value' object, that we are going to build as part of this Micrograd library.</p> <p>Now, those Value objects are going to wrap up those numbers themselves and we are going to build up the mathematical expressions that you see after that. So, a &amp; b eventually get transformed to c, d, e, f and then g.</p> <p>So, we are showing the functionality of Micrograd in some of the operations that it supports like: Adding, Multiplying, Raising them to a constant power etc.</p> <p>Therefore, we are building an expression graph {Hidden Layer} -&gt; with these two values 'a' and 'b' as an input {Input Layer} -&gt; producing an output of 'g' {Output Layer}</p> <p>So micrograd in the background will create that entire mathematical expression (from c to f) {Therefore acts as that Hidden Layer}</p> <p>Apart from just doing a Forward Pass, i.e. finding g.</p> <p>We are also going to perform Backpropagation where we find the derivative of g by going backwards from output -&gt; hidden -&gt; input by recursively applying chain of rule (from calculus).</p> <p>So what that allows us to do, is to find the derivative of g w.r.t to all the internal nodes i.e. f, d, c and the input nodes i.e. a &amp; b.</p> <p>Then we can query the derivative of g w.r.t a <pre><code>a.grad\n</code></pre></p> <p>query the derivative of g w.r.t a <pre><code>b.grad\n</code></pre></p> <p>Now this query is important as it tells us: How this a &amp; b is affecting -&gt; g -&gt; through this mathematical expression.</p> <p>So lets say, when we nudge the value of a little bit, then the value of g grows a little bit, therefore adjusting the value of that graph; vice versa.</p> <p>Neural Network are just a Mathematical Expression.</p> <p>They take the Input data as well as the weights of the nodes -&gt; MATHEMATICAL EXPRESSION -&gt; Output, the predictions (basically a loss function is generated)</p> <p>Now, Backpropagation is just a general term. It doesn't care about NN at all. It only cares about arbitral mathematical equations/expressions.</p> <p>We just happen to use this machinery/mechanism to train a NN.</p> <p>This is such a very basic, atom level expression. So a and b are in scaler level, so only takes one values each.</p> <p>(IMP) See this timestamp once- 5:48 to 6:48**</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/multi-layer-perceptron/","title":"Multi-Layer Perceptron","text":"<p>Now we will be building a single neural net. It is basically the diagram we've been seeing for NN, where there is an Input layer, Hidden layers and Output layers.</p> <p>Note: Also remember that simple diagram with a circle in the middle where they show the summation of product of weights &amp; inputs with the bias along with an activation function added to produce an output (That is essentially what we are creating here)</p> <p>This process/lecture is divided into 4 stages, each interconnected to each other but broken down for understanding purpose.</p>"},{"location":"ZeroToHero/Micrograd/notes/multi-layer-perceptron/#stage-1-defining-a-simple-neuron","title":"STAGE 1: Defining a simple Neuron","text":"<pre><code>class Neuron:\n\n    def __init__(self, nin):\n        self.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n        self.b = Value(random.uniform(-1,1))\n\n    def __call__(self, x):\n        # (w*x)+b\n        act = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n        out = act.tanh()\n        return out\n\nx = [2.0, 3.0]\nn = Neuron(2)\nn(x)\n</code></pre> <p>This Python code is defining a very simple \"neuron\" using OOPS. We've first made a class which will make/define a single NEURON.</p> <p>'__ init __ method':</p> <p>This is the constructor of the class, and it's called when we create a new neuron object. There are two things happening inside it:</p> <ol> <li> <p>Inputs and Weights (self.w): When you create a neuron, you specify how many inputs (nin) it will receive.</p> <p>In our case, we create a neuron with 2 inputs: <code>[2.0, 3.0]</code> (the last part of the code). The ones in the list are the x values, but we explicitly mention the number of inputs in the Neuron(2) which is the name of the class itself, hence the init method will be called then.</p> <p>Each of these inputs will be multiplied by a corresponding weight. A weight is like the importance of an input. If one input is more important, its weight will be larger.</p> <p>So first, in self.w we initialize the weight as random values between -1 and 1.</p> </li> <li> <p>Bias (self.b): Besides the inputs and their weights, neurons also have something called a bias. This bias is like an extra nudge the neuron gets to help it make better decisions.</p> <p>Here too in the code we randomly initialize the bias between -1 and 1.</p> </li> </ol> <p>'__ call __ method':</p> <p>This is where the actual calculation happens when you pass inputs through the neuron. We had made the structure of the Neuron in the init method, and now we perform our mathematical expression calculations here.</p> <ol> <li> <p>Inputs Multiplying Weights (<code>wi * xi</code>):</p> <p>Here each input (<code>x</code>) multiplied by its corresponding weight (<code>self.w</code>).</p> <p>The line <code>zip(self.w, x)</code> simply pairs the weights with the inputs.     So basically, if <code>self.w = [0.5, -0.2]</code> and <code>x = [2.0, 3.0]</code>, then it will be calculated as =&gt; (0.5 * 2.0) + (-0.2 * 3.0)</p> </li> <li> <p>Summing the Results (<code>sum(wi * xi)</code>):</p> <p>After multiplying the inputs by their respective weights, we add them up to get a combined score.</p> </li> <li> <p>Adding the Bias (<code>+ self.b</code>):</p> <p>To the combined score from the inputs and weights, we add the bias. This bias can help shift the score slightly in one direction or the other.</p> </li> </ol> <p>So all these 3 steps we have merged into a single line of code <pre><code>act = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n</code></pre></p> <ol> <li> <p>Activation Function (tanh):</p> <p>Finally, we apply the activation function. Here, the function used is <code>tanh</code>, which squeezes the output between -1 and 1.</p> <p>Explanation on Activation function- Imagine the result going through a \"gate\" that controls how strong the neuron\u2019s final decision is, making sure it's not too big or too small.</p> <p><code>tanh</code> has a nice property of pushing values that are too large or too small closer to the boundaries (-1, 1). If you imagine a spring, the more you pull it in one direction, the harder it is to pull further, which stops it from becoming too extreme.</p> </li> </ol> <p>Finally, in this last part of the code (So that we can understand how this class runs individually) - Inputs: You are giving the neuron two inputs: <code>2.0</code> and <code>3.0</code>. - Neuron: You create a neuron that expects 2 inputs (<code>Neuron(2)</code>). - Calling the Neuron: Finally, you pass the inputs to the neuron using <code>n(x)</code>, which will use the weights, bias, and <code>tanh</code> to return an output.</p>"},{"location":"ZeroToHero/Micrograd/notes/multi-layer-perceptron/#stage-2-defining-hidden-layers","title":"STAGE 2: Defining Hidden Layer(s)","text":"<p>Now we are moving from a single neuron to a hidden layer of neurons. A hidden layer is just a collection of neurons working together, where each neuron processes inputs and produces outputs.</p> <pre><code>class Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs\n</code></pre> <p>'__ init __ method':</p> <p>Here again, this is a constructor which initializes the layer.</p> <ol> <li> <p>Inputs (<code>nin</code>) and Outputs (<code>nout</code>):</p> <p>The <code>nin</code> is the number of inputs coming into the layer, and <code>nout</code> is the number of neurons in the layer.</p> <p><code>self.neurons = [Neuron(nin) for _ in range(nout)]</code>: This line is creating a list of neurons, one for each output. If <code>nout = 4</code>, it creates 4 neurons, each expecting 3 inputs (or however many <code>nin</code> represents).</p> </li> </ol> <p>'__ call __ method':</p> <p>Now, this Layer contains many neurons. So we are performing the actions on each of the neuron one by one. Therefore we are calling the Neuron class we had defined in stage 1. </p> <ol> <li> <p><code>outs = [n(x) for n in self.neurons]</code>: This line says: \"Take each neuron (<code>n</code>) in the layer and pass the inputs (<code>x</code>) to it.\" It loops over every neuron and gets the output from each one.</p> <p>The result, <code>outs</code>, is a list of outputs\u2014one from each neuron. So, if you have 4 neurons in this layer, you\u2019ll get a list of 4 outputs.</p> </li> </ol> <p>So each neuron in the layer is receiving the same inputs, but each making its own unique decision based on its weights and bias and providing the respective output.</p>"},{"location":"ZeroToHero/Micrograd/notes/multi-layer-perceptron/#stage-3-creating-a-multi-level-perceptron","title":"STAGE 3: Creating a Multi-Level Perceptron","text":"<p>This is how the staging process has gone: Single Neuron \u2192 Layer of Neurons \u2192 Multiple Layers of Neurons Now we'll be connecting all of them together.</p> <pre><code>class MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [ Layer(sz[i], sz[i+1]) for i in rangle(len(nouts)) ]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n</code></pre> <p>'__ init __ method' (Constructor): This method is responsible for setting up the structure of the MLP (i.e., the different layers).</p> <ol> <li> <p>Input Size (<code>nin</code>) and Layer Sizes (<code>nouts</code>):</p> <ul> <li> <p><code>nin</code> is the number of inputs going into the network (like the input size).</p> </li> <li> <p><code>nouts</code> is a list that defines the number of neurons in each layer of the network.     For example, <code>[4, 3, 2]</code> means the first hidden layer has 4 neurons, the second hidden layer has 3 neurons, and the final output layer has 2 neurons.</p> </li> </ul> </li> <li> <p>Building the Layers:</p> <ul> <li> <p><code>sz = [nin] + nouts</code>: This line simply creates a list that contains the input size followed by the sizes of all the layers.     If <code>nin = 2</code> and <code>nouts = [4, 3]</code>, then <code>sz = [2, 4, 3]</code>, where 2 is the number of inputs, 4 is the number of neurons in the first hidden layer, and 3 is the number of neurons in the next layer.</p> </li> <li> <p><code>self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]</code>: This line loops through the <code>sz</code> list and creates each layer. Each layer is constructed by taking the number of inputs from <code>sz[i]</code> and the number of neurons in that layer from <code>sz[i+1]</code>.</p> <p>For example, if <code>sz = [2, 4, 3]</code>, this means: - The first layer takes 2 inputs and has 4 neurons. - The second layer takes 4 inputs (which are the outputs from the first layer) and has 3 neurons.</p> </li> </ul> <p>Similarly, when more layers need to be added, the similar process is followed</p> </li> </ol> <p>'__ call __ method': This method runs the forward pass through the network, where data passes through each layer, one after the other.</p> <ol> <li> <p>Processing the Input (<code>x</code>) Through Each Layer:</p> <p><code>for layer in self.layers:</code>x = layer(x)`</p> <ul> <li>The input <code>x</code> is passed to the first layer, which processes it using its neurons.</li> <li>Then, the output of that layer becomes the input to the next layer.</li> <li>This continues for each layer, passing the outputs from one layer as inputs to the next.</li> <li>After the input has passed through all the layers, the final output is returned.</li> </ul> <p>Note: This loop processes one entire layer at a time, not neuron by neuron. When it\u2019s processing a layer:</p> <ul> <li>All the neurons in that layer receive the same input (which is the output of the previous layer).</li> <li>All neurons in the layer independently calculate their outputs (in parallel).</li> <li>The outputs of all neurons in the layer are collected and passed as a single new input to the next layer.</li> </ul> <p>Now that is passed here in STAGE 2 function: <code>outs = [n(x) for n in self.neurons]</code></p> <p>This means: - Each neuron (<code>n</code>) in the layer gets the same input (<code>x</code>). - Each neuron processes the input at the same time and independently. - The outputs from all neurons in this layer (first hidden layer) are collected into a list (<code>outs</code>).</p> </li> </ol>"},{"location":"ZeroToHero/Micrograd/notes/multi-layer-perceptron/#stage-4-combining-all-of-them-together","title":"STAGE 4: COMBINING ALL OF THEM TOGETHER","text":"<p>Final code will look like this:</p> <p><pre><code>class Neuron:\n\n    def __init__(self, nin):\n        self.w = [ Value(random.uniform(-1,1)) for _ in range(nin) ]\n        self.b = Value(random.uniform(-1,1))\n\n    def __call__(self, x):\n        # (w*x)+b\n        act = sum( (wi*xi for wi,xi in zip(self.w, x)), self.b )\n        out = act.tanh()\n        return out\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts)) ]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n</code></pre> (Also import random)</p> <p>Now, lets pass the values to create the entire neural network of this and make it work!</p> <p><pre><code>x = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n</code></pre> x is the input values n -&gt; - The number of input values provided is mentioned (so its 3 in our case, as provided in x) - Next we mention how many layers we want and in each of those layers how many neurons must be there.      So the number of layers needed, will be the length of the list given (If you count, there are 3 values there in total, so 3 layers).     We also mention how many neurons should be there in each of those layers (So in this case we want 4 - 4 - 1)</p> <p>This diagram is essentially what we have recreated here :)</p> <p>Now, this is just the forward pass implementation (You will notice that the grad values are still 0.0) So next that is what we will be working on! :))</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/neurons-explaination/","title":"Neurons Explaination","text":"<p>Take a look at these images: Simple Neural Net Neuron Mathematical Model</p>"},{"location":"ZeroToHero/Micrograd/notes/neurons-explaination/#neuron-mathematical-model","title":"Neuron Mathematical Model:","text":"<p>INPUTS: You have these neurons as inputs which are the 'x'</p> <p>Then there are the synapse, which are the weights as 'w'</p> <p>So the synapse interacts with the neurons multiplicatively.</p> <p>So, what flows into the 'cell body' is w times x</p> <p>But there are multiple of it, so many w times x  Eg in the image: w0x0, w1x1, w2x2</p> <p>CELL BODY: It takes in the product of those inputs. It also contains some bias 'b' - This is like a innate trigger happiness for the neuron. So the bias can make it a bit more trigger happy or a bit less trigger happy, regardless of the input.</p> <p>So now if you see,</p> <p>We have the summation of all the inputs (w.x) added with the bias 'b' and we take it through an activation function 'f'</p> <p>Now this activation function 'f' is usually like a squashing function, like sigmoid or tanh.</p> <p>We'll be using tanh for our example. numpy has np.tanh which we can call in code.</p> <p><pre><code>plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();\n</code></pre> Output graph</p> <p>So, you will see that the inputs we are adding gets squashed at the y-coordinate (at the middle) Therefore at 0, it becomes 0 When it's more positive in the input, it is pushed upto 1 only and then smoothens out there. When it is negative, its is flattened out at -1 itself</p> <p>OUTPUTS:</p> <p>So what comes out as an output is the activation function 'f' applied as a dot product the sum of the weight of the inputs.</p> <p>Now, we'll be implementing this in code.</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/pytorch-comparision/","title":"PyTorch Comparison","text":"<p>In PyTorch, everything is defined in Tensors. Tensors are just n-dimensional array of scalars.</p> <pre><code>import torch\n\nx1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n</code></pre> <p>Now, in PyTorch these values are stored as Tensor arrays</p> <p>For example:  <pre><code>torch.Tensor([[1,2,3], [4,5,6]])\n</code></pre> <pre><code>torch.Tensor([[1,2,3], [4,5,6]]).shape\n</code></pre></p> <p>Now for the datatype, python by default gives double to its decimal numbers.</p> <p>But PyTorch gives it only a single precision float i.e. Float32: <pre><code>torch.Tensor([2.0]).dtype\n</code></pre></p> <p>Therefore we double it by adding: <pre><code>torch.Tensor([2.0]).double().dtype\n</code></pre> And we get Float64</p> <p>Also, PyTorch assumes that we do not need the gradient values of the leaf nodes, so we manually assign them there - x1.requires_grad = True. By default they are set to False (For efficiency reasons)</p> <p>Now, we have made the variable declarations based on PyTorch.</p> <p>When we try to access the values individually, they will be stored inside tensor objects <pre><code>x2.data\n</code></pre></p> <p>In order to remove the tensor wrap, we just add .item() to it <pre><code>x2.data.item()\n</code></pre></p> <p>So the idea was to prove that everything we've build till now very much agrees with the PyTorch API syntax.  But to also realize that everything will become just for efficient with PyTorch. Notebook</p> <p> </p> <p>Back to Master Notes</p>"},{"location":"ZeroToHero/Micrograd/notes/value-object-creation/","title":"Value Object Creation","text":"<p>First we will be building the 'Value' object which we had seen in the micrograd-functionality explanation example.</p> <pre><code>class Value:\n\n    # So Value object takes a single scalar value that it stores and keeps track of\n\u00a0 \u00a0 \n\u00a0 \u00a0 def __init__(self, data):\n\u00a0 \u00a0 \u00a0 \u00a0 self.data = data\n\n    # Python uses this repr function internally, to return the below string\n\u00a0 \u00a0 \n\u00a0 \u00a0 def __repr__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 return f\"Value(data={self.data})\"\n</code></pre> <pre><code>a = Value(2.0) \u00a0# So now this is a Value object whose data equals 2\n\na\n\n\n\nb = Value(-3.0) #Another Value object\n\nb\n</code></pre> <pre><code>a + b\n\n# Here when we try to add, it throws an error as right now python doesn't know how to add our two Value object\n\n\"\"\"\nTypeError: unsupported operand type(s) for +: 'Value' and 'Value'\n\"\"\"\n</code></pre> <p>Therefore, in order to resolve that, <pre><code># Python will call these internal __add__ operator to perform the addition\ndef __add__(self, other):\n\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(self.data + other.data)\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n</code></pre></p> <p>So, when we call 'a+b' -&gt; python internally calls -&gt; a.__ add __ (b) Therefore now, the addition in (self.data + other.data) happens as its usual floating point values addition.</p> <p>Note: The repr function basically allows us to print nicer looking expressions. If that was not there, in the output we'll get some random gibberish output instead of what we actually want to be printed like: Value(data=-1.0)</p>"},{"location":"ZeroToHero/Micrograd/notes/value-object-creation/#visualization-of-the-expression","title":"Visualization of the expression","text":"<p>That is we will be producing graphs i.e. to keep points of what values produce these other values. (That will act as the connecting tissue of these expressions)</p> <p>Therefore in the final expression code that we had made: <pre><code>class Value:\n\n\u00a0 \u00a0 def __init__(self, data, _children=()):\n\u00a0 \u00a0 \u00a0 \u00a0 self.data = data\n\u00a0 \u00a0 \u00a0 \u00a0 self._prev = set(_children)\n\n\u00a0 \u00a0 def __repr__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 return f\"Value(data={self.data})\"\n\n\u00a0 \u00a0 def __add__(self, other):\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(self.data + other.data, (self, other))\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n\n\u00a0 \u00a0 def __mul__(self, other):\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(self.data * other.data, (self, other))\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n</code></pre></p> <p><pre><code>a = Value(2.0)\nb = Value(-.3.0)\nc = Value(10.0)\n\nd = a*b + c\nd\n</code></pre> Here, _ children was used for efficiency and by default it stores an empty tuple (This is just for convenience) but when we use it in the class it will be stored as a set.</p> <p>Inside however we call the different function _ prev  and store that children set. (So, in the above expression, it basically stores the result of the sub expressions, like a mul b and value of c.)</p> <p><pre><code>d._prev\n</code></pre> The above cell would return:</p> <p>Now, we know the children that were created as a value.</p> <p>But we still don't know what operation created those values, therefore</p> <pre><code>class Value:\n\n\u00a0 \u00a0 def __init__(self, data, _children=(), _op=''):\n\u00a0 \u00a0 \u00a0 \u00a0 self.data = data\n\u00a0 \u00a0 \u00a0 \u00a0 self._prev = set(_children)\n\u00a0 \u00a0 \u00a0 \u00a0 self._op = op\n\n\u00a0 \u00a0 def __repr__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 return f\"Value(data={self.data})\"\n\n\u00a0 \u00a0 def __add__(self, other):\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(self.data + other.data, (self, other), '+')\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n\n\u00a0 \u00a0 def __mul__(self, other):\n\u00a0 \u00a0 \u00a0 \u00a0 out = Value(self.data * other.data, (self, other), '*')\n\u00a0 \u00a0 \u00a0 \u00a0 return out\n</code></pre> <p><pre><code>d._op\n</code></pre> The above cell will show the final operation that was done to produce 'd' which was '+'</p> <p>Now we have: - Full Mathematical expression - Build the all the data structures to hold it - We know how all of these values came to be, in the form of what expression and from what other values</p>"},{"location":"ZeroToHero/Micrograd/notes/value-object-creation/#visualization-of-the-expression-continued","title":"Visualization of the expression continued","text":"<p>Made the visual graphs for the nodes of the NN. I have split it into two notebooks as I had to use Google Colab for graphviz. Notebook</p>"},{"location":"ZeroToHero/Micrograd/notes/value-object-creation/#summary-and-what-to-do-next","title":"SUMMARY AND WHAT TO DO NEXT","text":"<ul> <li> <p>We have given various inputs like a, b, f that going into a mathematical expression and generate a single output L. The entire graph generated visualizes a Forward Pass. So, the output of the forward pass is -8 (the final value obtained in L)</p> </li> <li> <p>Next we would like to run Back propagation.</p> </li> <li> <p>We are going to calculate the derivative of every single value/node w.r.t L</p> </li> <li> <p>In Neural Networks of course we will be interested in finding the derivative of the Loss Function 'L' w.r.t to the weights of the neural networks e.g. in the above graph the weights of each of those nodes.</p> </li> <li> <p>Usually we do not consider the initial values to find the derivate of the loss function with, as they were fixed. E.g. a &amp; b</p> </li> <li> <p>So it's the weights that will be iterated on E.g. c, e, d, f</p> </li> <li> <p>Therefore during initialization, the gradient values will be set to 0 as we believe that those initial values do not affect the value of the output.</p> </li> </ul> <p> </p> <p>Back to Master Notes</p>"}]}